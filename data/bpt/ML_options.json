{"model": {"binary": {"dt": {"docs_name": "sklearn.tree.DecisionTreeClassifier", "preset_params": ["default", "dt classifier dist"], "param_names": ["ccp_alpha", "class_weight", "criterion", "max_depth", "max_features", "max_leaf_nodes", "min_impurity_decrease", "min_impurity_split", "min_samples_leaf", "min_samples_split", "min_weight_fraction_leaf", "presort", "random_state", "splitter"], "default_param_types": ["float", "NoneType", "str", "NoneType", "NoneType", "NoneType", "float", "NoneType", "int", "int", "float", "str", "NoneType", "str"], "default_params": ["0.0", "None", "'gini'", "None", "None", "None", "0.0", "None", "1", "2", "0.0", "'deprecated'", "None", "'best'"], "options": ["", ["None", "balanced"], ["\"gini\"", "\"entropy\""], "", ["\"auto\"", "\"sqrt\"", "\"log2\""], "", "", "", "", "", "", "", "", ["\"best\"", "\"random\""]], "descrs": ["    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n    .. versionadded:: 0.22\n", "    Weights associated with classes in the form ``{class_label: weight}``.\n    If None, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n    For multi-output, the weights of each column of y will be multiplied.\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n", "    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n", "    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n", "    The number of features to consider when looking for the best split:\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `int(max_features * n_features)` features are considered at each\n          split.\n        - If \"auto\", then `max_features=sqrt(n_features)`.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n", "    Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n", "    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n    The weighted impurity decrease equation is the following::\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n    .. versionadded:: 0.19\n", "    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n", "    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum number of samples required to split an internal node:\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n", "    This parameter is deprecated and will be removed in v0.24.\n    .. deprecated:: 0.22\n", "    Controls the randomness of the estimator. The features are always\n    randomly permuted at each split, even if ``splitter`` is set to\n    ``\"best\"``. When ``max_features < n_features``, the algorithm will\n    select ``max_features`` at random at each split before finding the best\n    split among them. But the best found split may vary across different\n    runs, even if ``max_features=n_features``. That is the case, if the\n    improvement of the criterion is identical for several splits and one\n    split has to be selected at random. To obtain a deterministic behaviour\n    during fitting, ``random_state`` has to be fixed to an integer.\n    See :term:`Glossary <random_state>` for details.\n", "    The strategy used to choose the split at each node. Supported\n    strategies are \"best\" to choose the best split and \"random\" to choose\n    the best random split.\n"]}, "elastic net": {"docs_name": "sklearn.linear_model.LogisticRegression", "preset_params": ["base elastic", "elastic classifier", "elastic clf v2", "elastic classifier extra"], "param_names": ["C", "class_weight", "dual", "fit_intercept", "intercept_scaling", "l1_ratio", "max_iter", "multi_class", "n_jobs", "penalty", "random_state", "solver", "tol", "verbose", "warm_start"], "default_param_types": ["float", "NoneType", "bool", "bool", "int", "NoneType", "int", "str", "NoneType", "str", "NoneType", "str", "float", "int", "bool"], "default_params": ["1.0", "None", "False", "True", "1", "None", "100", "'auto'", "None", "'l2'", "None", "'lbfgs'", "0.0001", "0", "False"], "options": ["", ["None", "balanced"], ["True", "False"], ["True", "False"], "", "", "", ["'auto'", "'ovr'", "'multinomial'"], "", ["'l1'", "'l2'", "'elasticnet'", "'none'"], "", ["'newton-cg'", "'lbfgs'", "'liblinear'", "'sag'", "'saga'"], "", "", ["True", "False"]], "descrs": ["    Inverse of regularization strength; must be a positive float.\n    Like in support vector machines, smaller values specify stronger\n    regularization.\n", "    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one.\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n    .. versionadded:: 0.17\n       *class_weight='balanced'*\n", "    Dual or primal formulation. Dual formulation is only implemented for\n    l2 penalty with liblinear solver. Prefer dual=False when\n    n_samples > n_features.\n", "    Specifies if a constant (a.k.a. bias or intercept) should be\n    added to the decision function.\n", "    Useful only when the solver 'liblinear' is used\n    and self.fit_intercept is set to True. In this case, x becomes\n    [x, self.intercept_scaling],\n    i.e. a \"synthetic\" feature with constant value equal to\n    intercept_scaling is appended to the instance vector.\n    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n    Note! the synthetic feature weight is subject to l1/l2 regularization\n    as all other features.\n    To lessen the effect of regularization on synthetic feature weight\n    (and therefore on the intercept) intercept_scaling has to be increased.\n", "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n    combination of L1 and L2.\n", "    Maximum number of iterations taken for the solvers to converge.\n", "    If the option chosen is 'ovr', then a binary problem is fit for each\n    label. For 'multinomial' the loss minimised is the multinomial loss fit\n    across the entire probability distribution, *even when the data is\n    binary*. 'multinomial' is unavailable when solver='liblinear'.\n    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n    and otherwise selects 'multinomial'.\n    .. versionadded:: 0.18\n       Stochastic Average Gradient descent solver for 'multinomial' case.\n    .. versionchanged:: 0.22\n        Default changed from 'ovr' to 'auto' in 0.22.\n", "    Number of CPU cores used when parallelizing over classes if\n    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n    set to 'liblinear' regardless of whether 'multi_class' is specified or\n    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors.\n    See :term:`Glossary <n_jobs>` for more details.\n", "    Used to specify the norm used in the penalization. The 'newton-cg',\n    'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n    only supported by the 'saga' solver. If 'none' (not supported by the\n    liblinear solver), no regularization is applied.\n    .. versionadded:: 0.19\n       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n", "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n    data. See :term:`Glossary <random_state>` for details.\n", "    Algorithm to use in the optimization problem.\n    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n      'saga' are faster for large ones.\n    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n      schemes.\n    - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n    - 'liblinear' and 'saga' also handle L1 penalty\n    - 'saga' also supports 'elasticnet' penalty\n    - 'liblinear' does not support setting ``penalty='none'``\n    Note that 'sag' and 'saga' fast convergence is only guaranteed on\n    features with approximately the same scale. You can\n    preprocess the data with a scaler from sklearn.preprocessing.\n    .. versionadded:: 0.17\n       Stochastic Average Gradient descent solver.\n    .. versionadded:: 0.19\n       SAGA solver.\n    .. versionchanged:: 0.22\n        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n", "    Tolerance for stopping criteria.\n", "    For the liblinear and lbfgs solvers set verbose to any positive\n    number for verbosity.\n", "    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n    .. versionadded:: 0.17\n       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n"]}, "et": {"docs_name": "sklearn.ensemble.ExtraTreesClassifier", "preset_params": ["default"], "param_names": ["bootstrap", "ccp_alpha", "class_weight", "criterion", "max_depth", "max_features", "max_leaf_nodes", "max_samples", "min_impurity_decrease", "min_impurity_split", "min_samples_leaf", "min_samples_split", "min_weight_fraction_leaf", "n_estimators", "n_jobs", "oob_score", "random_state", "verbose", "warm_start"], "default_param_types": ["bool", "float", "NoneType", "str", "NoneType", "str", "NoneType", "NoneType", "float", "NoneType", "int", "int", "float", "int", "NoneType", "bool", "NoneType", "int", "bool"], "default_params": ["False", "0.0", "None", "'gini'", "None", "'auto'", "None", "None", "0.0", "None", "1", "2", "0.0", "100", "None", "False", "None", "0", "False"], "options": [["True", "False"], "", ["None", "balanced"], ["\"gini\"", "\"entropy\""], "", ["\"auto\"", "\"sqrt\"", "\"log2\""], "", "", "", "", "", "", "", "", "", ["True", "False"], "", "", ["True", "False"]], "descrs": ["    Whether bootstrap samples are used when building trees. If False, the\n    whole dataset is used to build each tree.\n", "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n    .. versionadded:: 0.22\n", "    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n    The \"balanced_subsample\" mode is the same as \"balanced\" except that\n    weights are computed based on the bootstrap sample for every tree\n    grown.\n    For multi-output, the weights of each column of y will be multiplied.\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n", "    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n", "    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n", "    The number of features to consider when looking for the best split:\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `int(max_features * n_features)` features are considered at each\n      split.\n    - If \"auto\", then `max_features=sqrt(n_features)`.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n", "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n", "    If bootstrap is True, the number of samples to draw from X\n    to train each base estimator.\n    - If None (default), then draw `X.shape[0]` samples.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n      `max_samples` should be in the interval `(0, 1)`.\n    .. versionadded:: 0.22\n", "    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n    The weighted impurity decrease equation is the following::\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n    .. versionadded:: 0.19\n", "    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n", "    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum number of samples required to split an internal node:\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n", "    The number of trees in the forest.\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n", "    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    <n_jobs>` for more details.\n", "    Whether to use out-of-bag samples to estimate\n    the generalization accuracy.\n", "    Controls 3 sources of randomness:\n    - the bootstrapping of the samples used when building trees\n      (if ``bootstrap=True``)\n    - the sampling of the features to consider when looking for the best\n      split at each node (if ``max_features < n_features``)\n    - the draw of the splits for each of the `max_features`\n    See :term:`Glossary <random_state>` for details.\n", "    Controls the verbosity when fitting and predicting.\n", "    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`the Glossary <warm_start>`.\n"]}, "gaussian nb": {"docs_name": "sklearn.naive_bayes.GaussianNB", "preset_params": ["base gnb"], "param_names": ["priors", "var_smoothing"], "default_param_types": ["NoneType", "float"], "default_params": ["None", "1e-09"], "options": ["", ""], "descrs": ["    Prior probabilities of the classes. If specified the priors are not\n    adjusted according to the data.\n", "    Portion of the largest variance of all features that is added to\n    variances for calculation stability.\n    .. versionadded:: 0.20\n"]}, "gb": {"docs_name": "sklearn.ensemble.GradientBoostingClassifier", "preset_params": ["default"], "param_names": ["ccp_alpha", "criterion", "init", "learning_rate", "loss", "max_depth", "max_features", "max_leaf_nodes", "min_impurity_decrease", "min_impurity_split", "min_samples_leaf", "min_samples_split", "min_weight_fraction_leaf", "n_estimators", "n_iter_no_change", "presort", "random_state", "subsample", "tol", "validation_fraction", "verbose", "warm_start"], "default_param_types": ["float", "str", "NoneType", "float", "str", "int", "NoneType", "NoneType", "float", "NoneType", "int", "int", "float", "int", "NoneType", "str", "NoneType", "float", "float", "float", "int", "bool"], "default_params": ["0.0", "'friedman_mse'", "None", "0.1", "'deviance'", "3", "None", "None", "0.0", "None", "1", "2", "0.0", "100", "None", "'deprecated'", "None", "1.0", "0.0001", "0.1", "0", "False"], "options": ["", ["'friedman_mse'", "'mse'", "'mae'"], "", "", ["'deviance'", "'exponential'"], "", ["'auto'", "'sqrt'", "'log2'"], "", "", "", "", "", "", "", "", "", "", "", "", "", "", ["True", "False"]], "descrs": ["    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n    .. versionadded:: 0.22\n", "    The function to measure the quality of a split. Supported criteria\n    are 'friedman_mse' for the mean squared error with improvement\n    score by Friedman, 'mse' for mean squared error, and 'mae' for\n    the mean absolute error. The default value of 'friedman_mse' is\n    generally the best as it can provide a better approximation in\n    some cases.\n    .. versionadded:: 0.18\n", "    An estimator object that is used to compute the initial predictions.\n    ``init`` has to provide :meth:`fit` and :meth:`predict_proba`. If\n    'zero', the initial raw predictions are set to zero. By default, a\n    ``DummyEstimator`` predicting the classes priors is used.\n", "    learning rate shrinks the contribution of each tree by `learning_rate`.\n    There is a trade-off between learning_rate and n_estimators.\n", "    loss function to be optimized. 'deviance' refers to\n    deviance (= logistic regression) for classification\n    with probabilistic outputs. For loss 'exponential' gradient\n    boosting recovers the AdaBoost algorithm.\n", "    maximum depth of the individual regression estimators. The maximum\n    depth limits the number of nodes in the tree. Tune this parameter\n    for best performance; the best value depends on the interaction\n    of the input variables.\n", "    The number of features to consider when looking for the best split:\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `int(max_features * n_features)` features are considered at each\n      split.\n    - If 'auto', then `max_features=sqrt(n_features)`.\n    - If 'sqrt', then `max_features=sqrt(n_features)`.\n    - If 'log2', then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n    Choosing `max_features < n_features` leads to a reduction of variance\n    and an increase in bias.\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n", "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n", "    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n    The weighted impurity decrease equation is the following::\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n    .. versionadded:: 0.19\n", "    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n", "    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum number of samples required to split an internal node:\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n", "    The number of boosting stages to perform. Gradient boosting\n    is fairly robust to over-fitting so a large number usually\n    results in better performance.\n", "    ``n_iter_no_change`` is used to decide if early stopping will be used\n    to terminate training when validation score is not improving. By\n    default it is set to None to disable early stopping. If set to a\n    number, it will set aside ``validation_fraction`` size of the training\n    data as validation and terminate training when validation score is not\n    improving in all of the previous ``n_iter_no_change`` numbers of\n    iterations. The split is stratified.\n    .. versionadded:: 0.20\n", "    This parameter is deprecated and will be removed in v0.24.\n    .. deprecated :: 0.22\n", "    Controls the random seed given to each Tree estimator at each\n    boosting iteration.\n    In addition, it controls the random permutation of the features at\n    each split (see Notes for more details).\n    It also controls the random spliting of the training data to obtain a\n    validation set if `n_iter_no_change` is not None.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    The fraction of samples to be used for fitting the individual base\n    learners. If smaller than 1.0 this results in Stochastic Gradient\n    Boosting. `subsample` interacts with the parameter `n_estimators`.\n    Choosing `subsample < 1.0` leads to a reduction of variance\n    and an increase in bias.\n", "    Tolerance for the early stopping. When the loss is not improving\n    by at least tol for ``n_iter_no_change`` iterations (if set to a\n    number), the training stops.\n    .. versionadded:: 0.20\n", "    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if ``n_iter_no_change`` is set to an integer.\n    .. versionadded:: 0.20\n", "    Enable verbose output. If 1 then it prints progress and performance\n    once in a while (the more trees the lower the frequency). If greater\n    than 1 then it prints progress and performance for every tree.\n", "    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just erase the\n    previous solution. See :term:`the Glossary <warm_start>`.\n"]}, "gp": {"docs_name": "sklearn.gaussian_process.GaussianProcessClassifier", "preset_params": ["base gp classifier"], "param_names": ["copy_X_train", "kernel", "max_iter_predict", "multi_class", "n_jobs", "n_restarts_optimizer", "optimizer", "random_state", "warm_start"], "default_param_types": ["bool", "NoneType", "int", "str", "NoneType", "int", "str", "NoneType", "bool"], "default_params": ["True", "None", "100", "'one_vs_rest'", "None", "0", "'fmin_l_bfgs_b'", "None", "False"], "options": [["True", "False"], "", "", ["'one_vs_rest'", "'one_vs_one'"], "", "", "", "", ["True", "False"]], "descrs": ["    If True, a persistent copy of the training data is stored in the\n    object. Otherwise, just a reference to the training data is stored,\n    which might cause predictions to change if the data is modified\n    externally.\n", "    The kernel specifying the covariance function of the GP. If None is\n    passed, the kernel \"1.0 * RBF(1.0)\" is used as default. Note that\n    the kernel's hyperparameters are optimized during fitting.\n", "    The maximum number of iterations in Newton's method for approximating\n    the posterior during predict. Smaller values will reduce computation\n    time at the cost of worse results.\n", "    Specifies how multi-class classification problems are handled.\n    Supported are 'one_vs_rest' and 'one_vs_one'. In 'one_vs_rest',\n    one binary Gaussian process classifier is fitted for each class, which\n    is trained to separate this class from the rest. In 'one_vs_one', one\n    binary Gaussian process classifier is fitted for each pair of classes,\n    which is trained to separate these two classes. The predictions of\n    these binary predictors are combined into multi-class predictions.\n    Note that 'one_vs_one' does not support predicting probability\n    estimates.\n", "    The number of jobs to use for the computation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n", "    The number of restarts of the optimizer for finding the kernel's\n    parameters which maximize the log-marginal likelihood. The first run\n    of the optimizer is performed from the kernel's initial parameters,\n    the remaining ones (if any) from thetas sampled log-uniform randomly\n    from the space of allowed theta-values. If greater than 0, all bounds\n    must be finite. Note that n_restarts_optimizer=0 implies that one\n    run is performed.\n", "    Can either be one of the internally supported optimizers for optimizing\n    the kernel's parameters, specified by a string, or an externally\n    defined optimizer passed as a callable. If a callable is passed, it\n    must have the  signature::\n        def optimizer(obj_func, initial_theta, bounds):\n            # * 'obj_func' is the objective function to be maximized, which\n            #   takes the hyperparameters theta as parameter and an\n            #   optional flag eval_gradient, which determines if the\n            #   gradient is returned additionally to the function value\n            # * 'initial_theta': the initial value for theta, which can be\n            #   used by local optimizers\n            # * 'bounds': the bounds on the values of theta\n            ....\n            # Returned are the best found hyperparameters theta and\n            # the corresponding value of the target function.\n            return theta_opt, func_min\n    Per default, the 'L-BFGS-B' algorithm from scipy.optimize.minimize\n    is used. If None is passed, the kernel's parameters are kept fixed.\n    Available internal optimizers are::\n        'fmin_l_bfgs_b'\n", "    Determines random number generation used to initialize the centers.\n    Pass an int for reproducible results across multiple function calls.\n    See :term: `Glossary <random_state>`.\n", "    If warm-starts are enabled, the solution of the last Newton iteration\n    on the Laplace approximation of the posterior mode is used as\n    initialization for the next call of _posterior_mode(). This can speed\n    up convergence when _posterior_mode is called several times on similar\n    problems as in hyperparameter optimization. See :term:`the Glossary\n    <warm_start>`.\n"]}, "hgb": {"docs_name": "sklearn.ensemble.gradient_boosting.HistGradientBoostingClassifier", "preset_params": ["default"], "param_names": ["early_stopping", "l2_regularization", "learning_rate", "loss", "max_bins", "max_depth", "max_iter", "max_leaf_nodes", "min_samples_leaf", "monotonic_cst", "n_iter_no_change", "random_state", "scoring", "tol", "validation_fraction", "verbose", "warm_start"], "default_param_types": ["str", "float", "float", "str", "int", "NoneType", "int", "int", "int", "NoneType", "int", "NoneType", "str", "float", "float", "int", "bool"], "default_params": ["'auto'", "0.0", "0.1", "'auto'", "255", "None", "100", "31", "20", "None", "10", "None", "'loss'", "1e-07", "0.1", "0", "False"], "options": ["", "", "", ["'auto'", "'binary_crossentropy'", "'categorical_crossentropy'"], "", "", "", "", "", "", "", "", "", "", "", "", ["True", "False"]], "descrs": ["    If 'auto', early stopping is enabled if the sample size is larger than\n    10000. If True, early stopping is enabled, otherwise early stopping is\n    disabled.\n", "    The L2 regularization parameter. Use 0 for no regularization.\n", "    The learning rate, also known as *shrinkage*. This is used as a\n    multiplicative factor for the leaves values. Use ``1`` for no\n    shrinkage.\n", "    The loss function to use in the boosting process. 'binary_crossentropy'\n    (also known as logistic loss) is used for binary classification and\n    generalizes to 'categorical_crossentropy' for multiclass\n    classification. 'auto' will automatically choose either loss depending\n    on the nature of the problem.\n", "    The maximum number of bins to use for non-missing values. Before\n    training, each feature of the input array `X` is binned into\n    integer-valued bins, which allows for a much faster training stage.\n    Features with a small number of unique values may use less than\n    ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\n    is always reserved for missing values. Must be no larger than 255.\n", "    The maximum depth of each tree. The depth of a tree is the number of\n    edges to go from the root to the deepest leaf.\n    Depth isn't constrained by default.\n", "    The maximum number of iterations of the boosting process, i.e. the\n    maximum number of trees for binary classification. For multiclass\n    classification, `n_classes` trees per iteration are built.\n", "    The maximum number of leaves for each tree. Must be strictly greater\n    than 1. If None, there is no maximum limit.\n", "    The minimum number of samples per leaf. For small datasets with less\n    than a few hundred samples, it is recommended to lower this value\n    since only very shallow trees would be built.\n", "    Indicates the monotonic constraint to enforce on each feature. -1, 1\n    and 0 respectively correspond to a positive constraint, negative\n    constraint and no constraint. Read more in the :ref:`User Guide\n    <monotonic_cst_gbdt>`.\n", "    Used to determine when to \"early stop\". The fitting process is\n    stopped when none of the last ``n_iter_no_change`` scores are better\n    than the ``n_iter_no_change - 1`` -th-to-last one, up to some\n    tolerance. Only used if early stopping is performed.\n", "    Pseudo-random number generator to control the subsampling in the\n    binning process, and the train/validation data split if early stopping\n    is enabled.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    Scoring parameter to use for early stopping. It can be a single\n    string (see :ref:`scoring_parameter`) or a callable (see\n    :ref:`scoring`). If None, the estimator's default scorer\n    is used. If ``scoring='loss'``, early stopping is checked\n    w.r.t the loss value. Only used if early stopping is performed.\n", "    The absolute tolerance to use when comparing scores. The higher the\n    tolerance, the more likely we are to early stop: higher tolerance\n    means that it will be harder for subsequent iterations to be\n    considered an improvement upon the reference score.\n", "    Proportion (or absolute size) of training data to set aside as\n    validation data for early stopping. If None, early stopping is done on\n    the training data. Only used if early stopping is performed.\n", "    The verbosity level. If not zero, print some information about the\n    fitting process.\n", "    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble. For results to be valid, the\n    estimator should be re-trained on the same data only.\n    See :term:`the Glossary <warm_start>`.\n"]}, "knn": {"docs_name": "sklearn.neighbors.KNeighborsClassifier", "preset_params": ["base knn", "knn dist"], "param_names": ["algorithm", "leaf_size", "metric", "metric_params", "n_jobs", "n_neighbors", "p", "weights"], "default_param_types": ["str", "int", "str", "NoneType", "NoneType", "int", "int", "str"], "default_params": ["'auto'", "30", "'minkowski'", "None", "None", "5", "2", "'uniform'"], "options": [["'auto'", "'ball_tree'", "'kd_tree'", "'brute'"], "", "", "", "", "", "", ["'uniform'", "'distance'"]], "descrs": ["    Algorithm used to compute the nearest neighbors:\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n", "    Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem.\n", "    the distance metric to use for the tree.  The default metric is\n    minkowski, and with p=2 is equivalent to the standard Euclidean\n    metric. See the documentation of :class:`DistanceMetric` for a\n    list of available metrics.\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`,\n    in which case only \"nonzero\" elements may be considered neighbors.\n", "    Additional keyword arguments for the metric function.\n", "    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n    Doesn't affect :meth:`fit` method.\n", "    Number of neighbors to use by default for :meth:`kneighbors` queries.\n", "    Power parameter for the Minkowski metric. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n", "    weight function used in prediction.  Possible values:\n    - 'uniform' : uniform weights.  All points in each neighborhood\n      are weighted equally.\n    - 'distance' : weight points by the inverse of their distance.\n      in this case, closer neighbors of a query point will have a\n      greater influence than neighbors which are further away.\n    - [callable] : a user-defined function which accepts an\n      array of distances, and returns an array of the same shape\n      containing the weights.\n"]}, "lasso": {"docs_name": "sklearn.linear_model.LogisticRegression", "preset_params": ["base lasso", "lasso C", "lasso C extra"], "param_names": ["C", "class_weight", "dual", "fit_intercept", "intercept_scaling", "l1_ratio", "max_iter", "multi_class", "n_jobs", "penalty", "random_state", "solver", "tol", "verbose", "warm_start"], "default_param_types": ["float", "NoneType", "bool", "bool", "int", "NoneType", "int", "str", "NoneType", "str", "NoneType", "str", "float", "int", "bool"], "default_params": ["1.0", "None", "False", "True", "1", "None", "100", "'auto'", "None", "'l2'", "None", "'lbfgs'", "0.0001", "0", "False"], "options": ["", ["None", "balanced"], ["True", "False"], ["True", "False"], "", "", "", ["'auto'", "'ovr'", "'multinomial'"], "", ["'l1'", "'l2'", "'elasticnet'", "'none'"], "", ["'newton-cg'", "'lbfgs'", "'liblinear'", "'sag'", "'saga'"], "", "", ["True", "False"]], "descrs": ["    Inverse of regularization strength; must be a positive float.\n    Like in support vector machines, smaller values specify stronger\n    regularization.\n", "    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one.\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n    .. versionadded:: 0.17\n       *class_weight='balanced'*\n", "    Dual or primal formulation. Dual formulation is only implemented for\n    l2 penalty with liblinear solver. Prefer dual=False when\n    n_samples > n_features.\n", "    Specifies if a constant (a.k.a. bias or intercept) should be\n    added to the decision function.\n", "    Useful only when the solver 'liblinear' is used\n    and self.fit_intercept is set to True. In this case, x becomes\n    [x, self.intercept_scaling],\n    i.e. a \"synthetic\" feature with constant value equal to\n    intercept_scaling is appended to the instance vector.\n    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n    Note! the synthetic feature weight is subject to l1/l2 regularization\n    as all other features.\n    To lessen the effect of regularization on synthetic feature weight\n    (and therefore on the intercept) intercept_scaling has to be increased.\n", "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n    combination of L1 and L2.\n", "    Maximum number of iterations taken for the solvers to converge.\n", "    If the option chosen is 'ovr', then a binary problem is fit for each\n    label. For 'multinomial' the loss minimised is the multinomial loss fit\n    across the entire probability distribution, *even when the data is\n    binary*. 'multinomial' is unavailable when solver='liblinear'.\n    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n    and otherwise selects 'multinomial'.\n    .. versionadded:: 0.18\n       Stochastic Average Gradient descent solver for 'multinomial' case.\n    .. versionchanged:: 0.22\n        Default changed from 'ovr' to 'auto' in 0.22.\n", "    Number of CPU cores used when parallelizing over classes if\n    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n    set to 'liblinear' regardless of whether 'multi_class' is specified or\n    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors.\n    See :term:`Glossary <n_jobs>` for more details.\n", "    Used to specify the norm used in the penalization. The 'newton-cg',\n    'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n    only supported by the 'saga' solver. If 'none' (not supported by the\n    liblinear solver), no regularization is applied.\n    .. versionadded:: 0.19\n       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n", "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n    data. See :term:`Glossary <random_state>` for details.\n", "    Algorithm to use in the optimization problem.\n    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n      'saga' are faster for large ones.\n    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n      schemes.\n    - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n    - 'liblinear' and 'saga' also handle L1 penalty\n    - 'saga' also supports 'elasticnet' penalty\n    - 'liblinear' does not support setting ``penalty='none'``\n    Note that 'sag' and 'saga' fast convergence is only guaranteed on\n    features with approximately the same scale. You can\n    preprocess the data with a scaler from sklearn.preprocessing.\n    .. versionadded:: 0.17\n       Stochastic Average Gradient descent solver.\n    .. versionadded:: 0.19\n       SAGA solver.\n    .. versionchanged:: 0.22\n        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n", "    Tolerance for stopping criteria.\n", "    For the liblinear and lbfgs solvers set verbose to any positive\n    number for verbosity.\n", "    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n    .. versionadded:: 0.17\n       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n"]}, "linear svm": {"docs_name": "sklearn.svm.LinearSVC", "preset_params": ["base linear svc", "linear svc dist"], "param_names": ["C", "class_weight", "dual", "fit_intercept", "intercept_scaling", "loss", "max_iter", "multi_class", "penalty", "random_state", "tol", "verbose"], "default_param_types": ["float", "NoneType", "bool", "bool", "int", "str", "int", "str", "str", "NoneType", "float", "int"], "default_params": ["1.0", "None", "True", "True", "1", "'squared_hinge'", "1000", "'ovr'", "'l2'", "None", "0.0001", "0"], "options": ["", ["None", "balanced"], ["True", "False"], ["True", "False"], "", ["'hinge'", "'squared_hinge'"], "", ["'ovr'", "'crammer_singer'"], ["'l1'", "'l2'"], "", "", ""], "descrs": ["    Regularization parameter. The strength of the regularization is\n    inversely proportional to C. Must be strictly positive.\n", "    Set the parameter C of class i to ``class_weight[i]*C`` for\n    SVC. If not given, all classes are supposed to have\n    weight one.\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n", "    Select the algorithm to either solve the dual or primal\n    optimization problem. Prefer dual=False when n_samples > n_features.\n", "    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be already centered).\n", "    When self.fit_intercept is True, instance vector x becomes\n    ``[x, self.intercept_scaling]``,\n    i.e. a \"synthetic\" feature with constant value equals to\n    intercept_scaling is appended to the instance vector.\n    The intercept becomes intercept_scaling * synthetic feature weight\n    Note! the synthetic feature weight is subject to l1/l2 regularization\n    as all other features.\n    To lessen the effect of regularization on synthetic feature weight\n    (and therefore on the intercept) intercept_scaling has to be increased.\n", "    Specifies the loss function. 'hinge' is the standard SVM loss\n    (used e.g. by the SVC class) while 'squared_hinge' is the\n    square of the hinge loss.\n", "    The maximum number of iterations to be run.\n", "    Determines the multi-class strategy if `y` contains more than\n    two classes.\n    ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while\n    ``\"crammer_singer\"`` optimizes a joint objective over all classes.\n    While `crammer_singer` is interesting from a theoretical perspective\n    as it is consistent, it is seldom used in practice as it rarely leads\n    to better accuracy and is more expensive to compute.\n    If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual\n    will be ignored.\n", "    Specifies the norm used in the penalization. The 'l2'\n    penalty is the standard used in SVC. The 'l1' leads to ``coef_``\n    vectors that are sparse.\n", "    Controls the pseudo random number generation for shuffling the data for\n    the dual coordinate descent (if ``dual=True``). When ``dual=False`` the\n    underlying implementation of :class:`LinearSVC` is not random and\n    ``random_state`` has no effect on the results.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    Tolerance for stopping criteria.\n", "    Enable verbose output. Note that this setting takes advantage of a\n    per-process runtime setting in liblinear that, if enabled, may not work\n    properly in a multithreaded context.\n"]}, "logistic": {"docs_name": "sklearn.linear_model.LogisticRegression", "preset_params": ["base logistic"], "param_names": ["C", "class_weight", "dual", "fit_intercept", "intercept_scaling", "l1_ratio", "max_iter", "multi_class", "n_jobs", "penalty", "random_state", "solver", "tol", "verbose", "warm_start"], "default_param_types": ["float", "NoneType", "bool", "bool", "int", "NoneType", "int", "str", "NoneType", "str", "NoneType", "str", "float", "int", "bool"], "default_params": ["1.0", "None", "False", "True", "1", "None", "100", "'auto'", "None", "'l2'", "None", "'lbfgs'", "0.0001", "0", "False"], "options": ["", ["None", "balanced"], ["True", "False"], ["True", "False"], "", "", "", ["'auto'", "'ovr'", "'multinomial'"], "", ["'l1'", "'l2'", "'elasticnet'", "'none'"], "", ["'newton-cg'", "'lbfgs'", "'liblinear'", "'sag'", "'saga'"], "", "", ["True", "False"]], "descrs": ["    Inverse of regularization strength; must be a positive float.\n    Like in support vector machines, smaller values specify stronger\n    regularization.\n", "    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one.\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n    .. versionadded:: 0.17\n       *class_weight='balanced'*\n", "    Dual or primal formulation. Dual formulation is only implemented for\n    l2 penalty with liblinear solver. Prefer dual=False when\n    n_samples > n_features.\n", "    Specifies if a constant (a.k.a. bias or intercept) should be\n    added to the decision function.\n", "    Useful only when the solver 'liblinear' is used\n    and self.fit_intercept is set to True. In this case, x becomes\n    [x, self.intercept_scaling],\n    i.e. a \"synthetic\" feature with constant value equal to\n    intercept_scaling is appended to the instance vector.\n    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n    Note! the synthetic feature weight is subject to l1/l2 regularization\n    as all other features.\n    To lessen the effect of regularization on synthetic feature weight\n    (and therefore on the intercept) intercept_scaling has to be increased.\n", "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n    combination of L1 and L2.\n", "    Maximum number of iterations taken for the solvers to converge.\n", "    If the option chosen is 'ovr', then a binary problem is fit for each\n    label. For 'multinomial' the loss minimised is the multinomial loss fit\n    across the entire probability distribution, *even when the data is\n    binary*. 'multinomial' is unavailable when solver='liblinear'.\n    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n    and otherwise selects 'multinomial'.\n    .. versionadded:: 0.18\n       Stochastic Average Gradient descent solver for 'multinomial' case.\n    .. versionchanged:: 0.22\n        Default changed from 'ovr' to 'auto' in 0.22.\n", "    Number of CPU cores used when parallelizing over classes if\n    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n    set to 'liblinear' regardless of whether 'multi_class' is specified or\n    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors.\n    See :term:`Glossary <n_jobs>` for more details.\n", "    Used to specify the norm used in the penalization. The 'newton-cg',\n    'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n    only supported by the 'saga' solver. If 'none' (not supported by the\n    liblinear solver), no regularization is applied.\n    .. versionadded:: 0.19\n       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n", "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n    data. See :term:`Glossary <random_state>` for details.\n", "    Algorithm to use in the optimization problem.\n    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n      'saga' are faster for large ones.\n    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n      schemes.\n    - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n    - 'liblinear' and 'saga' also handle L1 penalty\n    - 'saga' also supports 'elasticnet' penalty\n    - 'liblinear' does not support setting ``penalty='none'``\n    Note that 'sag' and 'saga' fast convergence is only guaranteed on\n    features with approximately the same scale. You can\n    preprocess the data with a scaler from sklearn.preprocessing.\n    .. versionadded:: 0.17\n       Stochastic Average Gradient descent solver.\n    .. versionadded:: 0.19\n       SAGA solver.\n    .. versionchanged:: 0.22\n        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n", "    Tolerance for stopping criteria.\n", "    For the liblinear and lbfgs solvers set verbose to any positive\n    number for verbosity.\n", "    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n    .. versionadded:: 0.17\n       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n"]}, "mlp": {"docs_name": "ABCD_ML.extensions.MLP.MLPClassifier_Wrapper", "preset_params": ["default", "mlp dist 3 layer", "mlp dist es 3 layer", "mlp dist 2 layer", "mlp dist es 2 layer", "mlp dist 1 layer", "mlp dist es 1 layer"], "param_names": ["activation", "alpha", "batch_size", "beta_1", "beta_2", "early_stopping", "epsilon", "hidden_layer_sizes", "learning_rate", "learning_rate_init", "max_fun", "max_iter", "momentum", "n_iter_no_change", "nesterovs_momentum", "power_t", "random_state", "shuffle", "solver", "tol", "validation_fraction", "verbose", "warm_start"], "default_param_types": ["str", "float", "str", "float", "float", "bool", "float", "tuple", "str", "float", "int", "int", "float", "int", "bool", "float", "NoneType", "bool", "str", "float", "float", "bool", "bool"], "default_params": ["'relu'", "0.0001", "'auto'", "0.9", "0.999", "False", "1e-08", "(100,)", "'constant'", "0.001", "15000", "200", "0.9", "10", "True", "0.5", "None", "True", "'adam'", "0.0001", "0.1", "False", "False"], "options": [["'identity'", "'logistic'", "'tanh'", "'relu'"], "", "", "", "", ["True", "False"], "", "", ["'constant'", "'invscaling'", "'adaptive'"], "", "", "", "", "", ["True", "False"], "", "", ["True", "False"], ["'lbfgs'", "'sgd'", "'adam'"], "", "", ["True", "False"], ["True", "False"]], "descrs": ["    Activation function for the hidden layer.\n    - 'identity', no-op activation, useful to implement linear bottleneck,\n      returns f(x) = x\n    - 'logistic', the logistic sigmoid function,\n      returns f(x) = 1 / (1 + exp(-x)).\n    - 'tanh', the hyperbolic tan function,\n      returns f(x) = tanh(x).\n    - 'relu', the rectified linear unit function,\n      returns f(x) = max(0, x)\n", "    L2 penalty (regularization term) parameter.\n", "    Size of minibatches for stochastic optimizers.\n    If the solver is 'lbfgs', the classifier will not use minibatch.\n    When set to \"auto\", `batch_size=min(200, n_samples)`\n", "    Exponential decay rate for estimates of first moment vector in adam,\n    should be in [0, 1). Only used when solver='adam'\n", "    Exponential decay rate for estimates of second moment vector in adam,\n    should be in [0, 1). Only used when solver='adam'\n", "    Whether to use early stopping to terminate training when validation\n    score is not improving. If set to true, it will automatically set\n    aside 10% of training data as validation and terminate training when\n    validation score is not improving by at least tol for\n    ``n_iter_no_change`` consecutive epochs. The split is stratified,\n    except in a multilabel setting.\n    Only effective when solver='sgd' or 'adam'\n", "    Value for numerical stability in adam. Only used when solver='adam'\n", "    The ith element represents the number of neurons in the ith\n    hidden layer.\n", "    Learning rate schedule for weight updates.\n    - 'constant' is a constant learning rate given by\n      'learning_rate_init'.\n    - 'invscaling' gradually decreases the learning rate at each\n      time step 't' using an inverse scaling exponent of 'power_t'.\n      effective_learning_rate = learning_rate_init / pow(t, power_t)\n    - 'adaptive' keeps the learning rate constant to\n      'learning_rate_init' as long as training loss keeps decreasing.\n      Each time two consecutive epochs fail to decrease training loss by at\n      least tol, or fail to increase validation score by at least tol if\n      'early_stopping' is on, the current learning rate is divided by 5.\n    Only used when ``solver='sgd'``.\n", "    The initial learning rate used. It controls the step-size\n    in updating the weights. Only used when solver='sgd' or 'adam'.\n", "    Only used when solver='lbfgs'. Maximum number of loss function calls.\n    The solver iterates until convergence (determined by 'tol'), number\n    of iterations reaches max_iter, or this number of loss function calls.\n    Note that number of loss function calls will be greater than or equal\n    to the number of iterations for the `MLPClassifier`.\n    .. versionadded:: 0.22\n", "    Maximum number of iterations. The solver iterates until convergence\n    (determined by 'tol') or this number of iterations. For stochastic\n    solvers ('sgd', 'adam'), note that this determines the number of epochs\n    (how many times each data point will be used), not the number of\n    gradient steps.\n", "    Momentum for gradient descent update. Should be between 0 and 1. Only\n    used when solver='sgd'.\n", "    Maximum number of epochs to not meet ``tol`` improvement.\n    Only effective when solver='sgd' or 'adam'\n    .. versionadded:: 0.20\n", "    Whether to use Nesterov's momentum. Only used when solver='sgd' and\n    momentum > 0.\n", "    The exponent for inverse scaling learning rate.\n    It is used in updating effective learning rate when the learning_rate\n    is set to 'invscaling'. Only used when solver='sgd'.\n", "    Determines random number generation for weights and bias\n    initialization, train-test split if early stopping is used, and batch\n    sampling when solver='sgd' or 'adam'.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    Whether to shuffle samples in each iteration. Only used when\n    solver='sgd' or 'adam'.\n", "    The solver for weight optimization.\n    - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n    - 'sgd' refers to stochastic gradient descent.\n    - 'adam' refers to a stochastic gradient-based optimizer proposed\n      by Kingma, Diederik, and Jimmy Ba\n    Note: The default solver 'adam' works pretty well on relatively\n    large datasets (with thousands of training samples or more) in terms of\n    both training time and validation score.\n    For small datasets, however, 'lbfgs' can converge faster and perform\n    better.\n", "    Tolerance for the optimization. When the loss or score is not improving\n    by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\n    unless ``learning_rate`` is set to 'adaptive', convergence is\n    considered to be reached and training stops.\n", "    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if early_stopping is True\n", "    Whether to print progress messages to stdout.\n", "    When set to True, reuse the solution of the previous\n    call to fit as initialization, otherwise, just erase the\n    previous solution. See :term:`the Glossary <warm_start>`.\n"]}, "pa": {"docs_name": "sklearn.linear_model.PassiveAggressiveClassifier", "preset_params": ["default"], "param_names": ["C", "average", "class_weight", "early_stopping", "fit_intercept", "loss", "max_iter", "n_iter_no_change", "n_jobs", "random_state", "shuffle", "tol", "validation_fraction", "verbose", "warm_start"], "default_param_types": ["float", "bool", "NoneType", "bool", "bool", "str", "int", "int", "NoneType", "NoneType", "bool", "float", "float", "int", "bool"], "default_params": ["1.0", "False", "None", "False", "True", "'hinge'", "1000", "5", "None", "None", "True", "0.001", "0.1", "0", "False"], "options": ["", ["True", "False"], ["None", "balanced"], ["True", "False"], ["True", "False"], "", "", "", "", "", ["True", "False"], "", "", "", ["True", "False"]], "descrs": ["    Maximum step size (regularization). Defaults to 1.0.\n", "    When set to True, computes the averaged SGD weights and stores the\n    result in the ``coef_`` attribute. If set to an int greater than 1,\n    averaging will begin once the total number of samples seen reaches\n    average. So average=10 will begin averaging after seeing 10 samples.\n    .. versionadded:: 0.19\n       parameter *average* to use weights averaging in SGD\n", "    Preset for the class_weight fit parameter.\n    Weights associated with classes. If not given, all classes\n    are supposed to have weight one.\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n    .. versionadded:: 0.17\n       parameter *class_weight* to automatically weight samples.\n", "    Whether to use early stopping to terminate training when validation.\n    score is not improving. If set to True, it will automatically set aside\n    a stratified fraction of training data as validation and terminate\n    training when validation score is not improving by at least tol for\n    n_iter_no_change consecutive epochs.\n    .. versionadded:: 0.20\n", "    Whether the intercept should be estimated or not. If False, the\n    data is assumed to be already centered.\n", "    The loss function to be used:\n    hinge: equivalent to PA-I in the reference paper.\n    squared_hinge: equivalent to PA-II in the reference paper.\n", "    The maximum number of passes over the training data (aka epochs).\n    It only impacts the behavior in the ``fit`` method, and not the\n    :meth:`partial_fit` method.\n    .. versionadded:: 0.19\n", "    Number of iterations with no improvement to wait before early stopping.\n    .. versionadded:: 0.20\n", "    The number of CPUs to use to do the OVA (One Versus All, for\n    multi-class problems) computation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n", "    Used to shuffle the training data, when ``shuffle`` is set to\n    ``True``. Pass an int for reproducible output across multiple\n    function calls.\n    See :term:`Glossary <random_state>`.\n", "    Whether or not the training data should be shuffled after each epoch.\n", "    The stopping criterion. If it is not None, the iterations will stop\n    when (loss > previous_loss - tol).\n    .. versionadded:: 0.19\n", "    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if early_stopping is True.\n    .. versionadded:: 0.20\n", "    The verbosity level\n", "    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    See :term:`the Glossary <warm_start>`.\n    Repeatedly calling fit or partial_fit when warm_start is True can\n    result in a different solution than when calling fit a single time\n    because of the way the data is shuffled.\n"]}, "random forest": {"docs_name": "sklearn.ensemble.RandomForestClassifier", "preset_params": ["base rf regressor", "rf classifier dist"], "param_names": ["bootstrap", "ccp_alpha", "class_weight", "criterion", "max_depth", "max_features", "max_leaf_nodes", "max_samples", "min_impurity_decrease", "min_impurity_split", "min_samples_leaf", "min_samples_split", "min_weight_fraction_leaf", "n_estimators", "n_jobs", "oob_score", "random_state", "verbose", "warm_start"], "default_param_types": ["bool", "float", "NoneType", "str", "NoneType", "str", "NoneType", "NoneType", "float", "NoneType", "int", "int", "float", "int", "NoneType", "bool", "NoneType", "int", "bool"], "default_params": ["True", "0.0", "None", "'gini'", "None", "'auto'", "None", "None", "0.0", "None", "1", "2", "0.0", "100", "None", "False", "None", "0", "False"], "options": [["True", "False"], "", ["None", "balanced"], ["\"gini\"", "\"entropy\""], "", ["\"auto\"", "\"sqrt\"", "\"log2\""], "", "", "", "", "", "", "", "", "", ["True", "False"], "", "", ["True", "False"]], "descrs": ["    Whether bootstrap samples are used when building trees. If False, the\n    whole dataset is used to build each tree.\n", "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n    .. versionadded:: 0.22\n", "    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n    The \"balanced_subsample\" mode is the same as \"balanced\" except that\n    weights are computed based on the bootstrap sample for every tree\n    grown.\n    For multi-output, the weights of each column of y will be multiplied.\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n", "    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n    Note: this parameter is tree-specific.\n", "    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n", "    The number of features to consider when looking for the best split:\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `int(max_features * n_features)` features are considered at each\n      split.\n    - If \"auto\", then `max_features=sqrt(n_features)`.\n    - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n", "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n", "    If bootstrap is True, the number of samples to draw from X\n    to train each base estimator.\n    - If None (default), then draw `X.shape[0]` samples.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n      `max_samples` should be in the interval `(0, 1)`.\n    .. versionadded:: 0.22\n", "    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n    The weighted impurity decrease equation is the following::\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n    .. versionadded:: 0.19\n", "    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n", "    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum number of samples required to split an internal node:\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n", "    The number of trees in the forest.\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n", "    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    <n_jobs>` for more details.\n", "    Whether to use out-of-bag samples to estimate\n    the generalization accuracy.\n", "    Controls both the randomness of the bootstrapping of the samples used\n    when building trees (if ``bootstrap=True``) and the sampling of the\n    features to consider when looking for the best split at each node\n    (if ``max_features < n_features``).\n    See :term:`Glossary <random_state>` for details.\n", "    Controls the verbosity when fitting and predicting.\n", "    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`the Glossary <warm_start>`.\n"]}, "ridge": {"docs_name": "sklearn.linear_model.LogisticRegression", "preset_params": ["base ridge", "ridge C", "ridge C extra"], "param_names": ["C", "class_weight", "dual", "fit_intercept", "intercept_scaling", "l1_ratio", "max_iter", "multi_class", "n_jobs", "penalty", "random_state", "solver", "tol", "verbose", "warm_start"], "default_param_types": ["float", "NoneType", "bool", "bool", "int", "NoneType", "int", "str", "NoneType", "str", "NoneType", "str", "float", "int", "bool"], "default_params": ["1.0", "None", "False", "True", "1", "None", "100", "'auto'", "None", "'l2'", "None", "'lbfgs'", "0.0001", "0", "False"], "options": ["", ["None", "balanced"], ["True", "False"], ["True", "False"], "", "", "", ["'auto'", "'ovr'", "'multinomial'"], "", ["'l1'", "'l2'", "'elasticnet'", "'none'"], "", ["'newton-cg'", "'lbfgs'", "'liblinear'", "'sag'", "'saga'"], "", "", ["True", "False"]], "descrs": ["    Inverse of regularization strength; must be a positive float.\n    Like in support vector machines, smaller values specify stronger\n    regularization.\n", "    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one.\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n    .. versionadded:: 0.17\n       *class_weight='balanced'*\n", "    Dual or primal formulation. Dual formulation is only implemented for\n    l2 penalty with liblinear solver. Prefer dual=False when\n    n_samples > n_features.\n", "    Specifies if a constant (a.k.a. bias or intercept) should be\n    added to the decision function.\n", "    Useful only when the solver 'liblinear' is used\n    and self.fit_intercept is set to True. In this case, x becomes\n    [x, self.intercept_scaling],\n    i.e. a \"synthetic\" feature with constant value equal to\n    intercept_scaling is appended to the instance vector.\n    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n    Note! the synthetic feature weight is subject to l1/l2 regularization\n    as all other features.\n    To lessen the effect of regularization on synthetic feature weight\n    (and therefore on the intercept) intercept_scaling has to be increased.\n", "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n    combination of L1 and L2.\n", "    Maximum number of iterations taken for the solvers to converge.\n", "    If the option chosen is 'ovr', then a binary problem is fit for each\n    label. For 'multinomial' the loss minimised is the multinomial loss fit\n    across the entire probability distribution, *even when the data is\n    binary*. 'multinomial' is unavailable when solver='liblinear'.\n    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n    and otherwise selects 'multinomial'.\n    .. versionadded:: 0.18\n       Stochastic Average Gradient descent solver for 'multinomial' case.\n    .. versionchanged:: 0.22\n        Default changed from 'ovr' to 'auto' in 0.22.\n", "    Number of CPU cores used when parallelizing over classes if\n    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n    set to 'liblinear' regardless of whether 'multi_class' is specified or\n    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors.\n    See :term:`Glossary <n_jobs>` for more details.\n", "    Used to specify the norm used in the penalization. The 'newton-cg',\n    'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n    only supported by the 'saga' solver. If 'none' (not supported by the\n    liblinear solver), no regularization is applied.\n    .. versionadded:: 0.19\n       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n", "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n    data. See :term:`Glossary <random_state>` for details.\n", "    Algorithm to use in the optimization problem.\n    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n      'saga' are faster for large ones.\n    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n      schemes.\n    - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n    - 'liblinear' and 'saga' also handle L1 penalty\n    - 'saga' also supports 'elasticnet' penalty\n    - 'liblinear' does not support setting ``penalty='none'``\n    Note that 'sag' and 'saga' fast convergence is only guaranteed on\n    features with approximately the same scale. You can\n    preprocess the data with a scaler from sklearn.preprocessing.\n    .. versionadded:: 0.17\n       Stochastic Average Gradient descent solver.\n    .. versionadded:: 0.19\n       SAGA solver.\n    .. versionchanged:: 0.22\n        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n", "    Tolerance for stopping criteria.\n", "    For the liblinear and lbfgs solvers set verbose to any positive\n    number for verbosity.\n", "    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n    .. versionadded:: 0.17\n       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n"]}, "sgd": {"docs_name": "sklearn.linear_model.SGDClassifier", "preset_params": ["base sgd", "sgd classifier"], "param_names": ["alpha", "average", "class_weight", "early_stopping", "epsilon", "eta0", "fit_intercept", "l1_ratio", "learning_rate", "loss", "max_iter", "n_iter_no_change", "n_jobs", "penalty", "power_t", "random_state", "shuffle", "tol", "validation_fraction", "verbose", "warm_start"], "default_param_types": ["float", "bool", "NoneType", "bool", "float", "float", "bool", "float", "str", "str", "int", "int", "NoneType", "str", "float", "NoneType", "bool", "float", "float", "int", "bool"], "default_params": ["0.0001", "False", "None", "False", "0.1", "0.0", "True", "0.15", "'optimal'", "'hinge'", "1000", "5", "None", "'l2'", "0.5", "None", "True", "0.001", "0.1", "0", "False"], "options": ["", ["True", "False"], ["None", "balanced"], ["True", "False"], "", "", ["True", "False"], "", "", "", "", "", "", ["'l2'", "'l1'", "'elasticnet'"], "", "", ["True", "False"], "", "", "", ["True", "False"]], "descrs": ["    Constant that multiplies the regularization term. The higher the\n    value, the stronger the regularization.\n    Also used to compute the learning rate when set to `learning_rate` is\n    set to 'optimal'.\n", "    When set to True, computes the averaged SGD weights accross all\n    updates and stores the result in the ``coef_`` attribute. If set to\n    an int greater than 1, averaging will begin once the total number of\n    samples seen reaches `average`. So ``average=10`` will begin\n    averaging after seeing 10 samples.\n", "    Preset for the class_weight fit parameter.\n    Weights associated with classes. If not given, all classes\n    are supposed to have weight one.\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n", "    Whether to use early stopping to terminate training when validation\n    score is not improving. If set to True, it will automatically set aside\n    a stratified fraction of training data as validation and terminate\n    training when validation score returned by the `score` method is not\n    improving by at least tol for n_iter_no_change consecutive epochs.\n    .. versionadded:: 0.20\n        Added 'early_stopping' option\n", "    Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n    'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n    For 'huber', determines the threshold at which it becomes less\n    important to get the prediction exactly right.\n    For epsilon-insensitive, any differences between the current prediction\n    and the correct label are ignored if they are less than this threshold.\n", "    The initial learning rate for the 'constant', 'invscaling' or\n    'adaptive' schedules. The default value is 0.0 as eta0 is not used by\n    the default schedule 'optimal'.\n", "    Whether the intercept should be estimated or not. If False, the\n    data is assumed to be already centered.\n", "    The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n    l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n    Only used if `penalty` is 'elasticnet'.\n", "    The learning rate schedule:\n    - 'constant': `eta = eta0`\n    - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n      where t0 is chosen by a heuristic proposed by Leon Bottou.\n    - 'invscaling': `eta = eta0 / pow(t, power_t)`\n    - 'adaptive': eta = eta0, as long as the training keeps decreasing.\n      Each time n_iter_no_change consecutive epochs fail to decrease the\n      training loss by tol or fail to increase validation score by tol if\n      early_stopping is True, the current learning rate is divided by 5.\n        .. versionadded:: 0.20\n            Added 'adaptive' option\n", "    The loss function to be used. Defaults to 'hinge', which gives a\n    linear SVM.\n    The possible options are 'hinge', 'log', 'modified_huber',\n    'squared_hinge', 'perceptron', or a regression loss: 'squared_loss',\n    'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n    The 'log' loss gives logistic regression, a probabilistic classifier.\n    'modified_huber' is another smooth loss that brings tolerance to\n    outliers as well as probability estimates.\n    'squared_hinge' is like hinge but is quadratically penalized.\n    'perceptron' is the linear loss used by the perceptron algorithm.\n    The other losses are designed for regression but can be useful in\n    classification as well; see\n    :class:`~sklearn.linear_model.SGDRegressor` for a description.\n    More details about the losses formulas can be found in the\n    :ref:`User Guide <sgd_mathematical_formulation>`.\n", "    The maximum number of passes over the training data (aka epochs).\n    It only impacts the behavior in the ``fit`` method, and not the\n    :meth:`partial_fit` method.\n    .. versionadded:: 0.19\n", "    Number of iterations with no improvement to wait before early stopping.\n    .. versionadded:: 0.20\n        Added 'n_iter_no_change' option\n", "    The number of CPUs to use to do the OVA (One Versus All, for\n    multi-class problems) computation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n", "    The penalty (aka regularization term) to be used. Defaults to 'l2'\n    which is the standard regularizer for linear SVM models. 'l1' and\n    'elasticnet' might bring sparsity to the model (feature selection)\n    not achievable with 'l2'.\n", "    The exponent for inverse scaling learning rate [default 0.5].\n", "    Used for shuffling the data, when ``shuffle`` is set to ``True``.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    Whether or not the training data should be shuffled after each epoch.\n", "    The stopping criterion. If it is not None, training will stop\n    when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n    epochs.\n    .. versionadded:: 0.19\n", "    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if `early_stopping` is True.\n    .. versionadded:: 0.20\n        Added 'validation_fraction' option\n", "    The verbosity level.\n", "    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    See :term:`the Glossary <warm_start>`.\n    Repeatedly calling fit or partial_fit when warm_start is True can\n    result in a different solution than when calling fit a single time\n    because of the way the data is shuffled.\n    If a dynamic learning rate is used, the learning rate is adapted\n    depending on the number of samples already seen. Calling ``fit`` resets\n    this counter, while ``partial_fit`` will result in increasing the\n    existing counter.\n"]}, "svm": {"docs_name": "sklearn.svm.SVC", "preset_params": ["base svm classifier", "svm classifier dist"], "param_names": ["C", "break_ties", "cache_size", "class_weight", "coef0", "decision_function_shape", "degree", "gamma", "kernel", "max_iter", "probability", "random_state", "shrinking", "tol", "verbose"], "default_param_types": ["float", "bool", "int", "NoneType", "float", "str", "int", "str", "str", "int", "bool", "NoneType", "bool", "float", "bool"], "default_params": ["1.0", "False", "200", "None", "0.0", "'ovr'", "3", "'scale'", "'rbf'", "-1", "False", "None", "True", "0.001", "False"], "options": ["", ["True", "False"], "", ["None", "balanced"], "", ["'ovo'", "'ovr'"], "", ["'scale'", "'auto'"], ["'linear'", "'poly'", "'rbf'", "'sigmoid'", "'precomputed'"], "", ["True", "False"], "", ["True", "False"], "", ["True", "False"]], "descrs": ["    Regularization parameter. The strength of the regularization is\n    inversely proportional to C. Must be strictly positive. The penalty\n    is a squared l2 penalty.\n", "    If true, ``decision_function_shape='ovr'``, and number of classes > 2,\n    :term:`predict` will break ties according to the confidence values of\n    :term:`decision_function`; otherwise the first class among the tied\n    classes is returned. Please note that breaking ties comes at a\n    relatively high computational cost compared to a simple predict.\n    .. versionadded:: 0.22\n", "    Specify the size of the kernel cache (in MB).\n", "    Set the parameter C of class i to class_weight[i]*C for\n    SVC. If not given, all classes are supposed to have\n    weight one.\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n", "    Independent term in kernel function.\n    It is only significant in 'poly' and 'sigmoid'.\n", "    Whether to return a one-vs-rest ('ovr') decision function of shape\n    (n_samples, n_classes) as all other classifiers, or the original\n    one-vs-one ('ovo') decision function of libsvm which has shape\n    (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one\n    ('ovo') is always used as multi-class strategy. The parameter is\n    ignored for binary classification.\n    .. versionchanged:: 0.19\n        decision_function_shape is 'ovr' by default.\n    .. versionadded:: 0.17\n       *decision_function_shape='ovr'* is recommended.\n    .. versionchanged:: 0.17\n       Deprecated *decision_function_shape='ovo' and None*.\n", "    Degree of the polynomial kernel function ('poly').\n    Ignored by all other kernels.\n", "    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n    - if ``gamma='scale'`` (default) is passed then it uses\n      1 / (n_features * X.var()) as value of gamma,\n    - if 'auto', uses 1 / n_features.\n    .. versionchanged:: 0.22\n       The default value of ``gamma`` changed from 'auto' to 'scale'.\n", "    Specifies the kernel type to be used in the algorithm.\n    It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n    a callable.\n    If none is given, 'rbf' will be used. If a callable is given it is\n    used to pre-compute the kernel matrix from data matrices; that matrix\n    should be an array of shape ``(n_samples, n_samples)``.\n", "    Hard limit on iterations within solver, or -1 for no limit.\n", "    Whether to enable probability estimates. This must be enabled prior\n    to calling `fit`, will slow down that method as it internally uses\n    5-fold cross-validation, and `predict_proba` may be inconsistent with\n    `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\n", "    Controls the pseudo random number generation for shuffling the data for\n    probability estimates. Ignored when `probability` is False.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    Whether to use the shrinking heuristic.\n    See the :ref:`User Guide <shrinking_svm>`.\n", "    Tolerance for stopping criterion.\n", "    Enable verbose output. Note that this setting takes advantage of a\n    per-process runtime setting in libsvm that, if enabled, may not work\n    properly in a multithreaded context.\n"]}}, "regression": {"ard": {"docs_name": "sklearn.linear_model.ARDRegression", "preset_params": ["default"], "param_names": ["alpha_1", "alpha_2", "compute_score", "copy_X", "fit_intercept", "lambda_1", "lambda_2", "n_iter", "normalize", "threshold_lambda", "tol", "verbose"], "default_param_types": ["float", "float", "bool", "bool", "bool", "float", "float", "int", "bool", "float", "float", "bool"], "default_params": ["1e-06", "1e-06", "False", "True", "True", "1e-06", "1e-06", "300", "False", "10000.0", "0.001", "False"], "options": ["", "", ["True", "False"], ["True", "False"], ["True", "False"], "", "", "", ["True", "False"], "", "", ["True", "False"]], "descrs": ["    Hyper-parameter : shape parameter for the Gamma distribution prior\n    over the alpha parameter.\n", "    Hyper-parameter : inverse scale parameter (rate parameter) for the\n    Gamma distribution prior over the alpha parameter.\n", "    If True, compute the objective function at each step of the model.\n", "    If True, X will be copied; else, it may be overwritten.\n", "    whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n", "    Hyper-parameter : shape parameter for the Gamma distribution prior\n    over the lambda parameter.\n", "    Hyper-parameter : inverse scale parameter (rate parameter) for the\n    Gamma distribution prior over the lambda parameter.\n", "    Maximum number of iterations.\n", "    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n", "    threshold for removing (pruning) weights with high precision from\n    the computation.\n", "    Stop the algorithm if w has converged.\n", "    Verbose mode when fitting the model.\n"]}, "bayesian ridge": {"docs_name": "sklearn.linear_model.BayesianRidge", "preset_params": ["default"], "param_names": ["alpha_1", "alpha_2", "alpha_init", "compute_score", "copy_X", "fit_intercept", "lambda_1", "lambda_2", "lambda_init", "n_iter", "normalize", "tol", "verbose"], "default_param_types": ["float", "float", "NoneType", "bool", "bool", "bool", "float", "float", "NoneType", "int", "bool", "float", "bool"], "default_params": ["1e-06", "1e-06", "None", "False", "True", "True", "1e-06", "1e-06", "None", "300", "False", "0.001", "False"], "options": ["", "", "", ["True", "False"], ["True", "False"], ["True", "False"], "", "", "", "", ["True", "False"], "", ["True", "False"]], "descrs": ["    Hyper-parameter : shape parameter for the Gamma distribution prior\n    over the alpha parameter.\n", "    Hyper-parameter : inverse scale parameter (rate parameter) for the\n    Gamma distribution prior over the alpha parameter.\n", "    Initial value for alpha (precision of the noise).\n    If not set, alpha_init is 1/Var(y).\n        .. versionadded:: 0.22\n", "    If True, compute the log marginal likelihood at each iteration of the\n    optimization.\n", "    If True, X will be copied; else, it may be overwritten.\n", "    Whether to calculate the intercept for this model.\n    The intercept is not treated as a probabilistic parameter\n    and thus has no associated variance. If set\n    to False, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n", "    Hyper-parameter : shape parameter for the Gamma distribution prior\n    over the lambda parameter.\n", "    Hyper-parameter : inverse scale parameter (rate parameter) for the\n    Gamma distribution prior over the lambda parameter.\n", "    Initial value for lambda (precision of the weights).\n    If not set, lambda_init is 1.\n        .. versionadded:: 0.22\n", "    Maximum number of iterations. Should be greater than or equal to 1.\n", "    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n", "    Stop the algorithm if w has converged.\n", "    Verbose mode when fitting the model.\n"]}, "dt": {"docs_name": "sklearn.tree.DecisionTreeRegressor", "preset_params": ["default", "dt dist"], "param_names": ["ccp_alpha", "criterion", "max_depth", "max_features", "max_leaf_nodes", "min_impurity_decrease", "min_impurity_split", "min_samples_leaf", "min_samples_split", "min_weight_fraction_leaf", "presort", "random_state", "splitter"], "default_param_types": ["float", "str", "NoneType", "NoneType", "NoneType", "float", "NoneType", "int", "int", "float", "str", "NoneType", "str"], "default_params": ["0.0", "'mse'", "None", "None", "None", "0.0", "None", "1", "2", "0.0", "'deprecated'", "None", "'best'"], "options": ["", ["\"mse\"", "\"friedman_mse\"", "\"mae\""], "", ["\"auto\"", "\"sqrt\"", "\"log2\""], "", "", "", "", "", "", "", "", ["\"best\"", "\"random\""]], "descrs": ["    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n    .. versionadded:: 0.22\n", "    The function to measure the quality of a split. Supported criteria\n    are \"mse\" for the mean squared error, which is equal to variance\n    reduction as feature selection criterion and minimizes the L2 loss\n    using the mean of each terminal node, \"friedman_mse\", which uses mean\n    squared error with Friedman's improvement score for potential splits,\n    and \"mae\" for the mean absolute error, which minimizes the L1 loss\n    using the median of each terminal node.\n    .. versionadded:: 0.18\n       Mean Absolute Error (MAE) criterion.\n", "    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n", "    The number of features to consider when looking for the best split:\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `int(max_features * n_features)` features are considered at each\n      split.\n    - If \"auto\", then `max_features=n_features`.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n", "    Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n", "    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n    The weighted impurity decrease equation is the following::\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n    .. versionadded:: 0.19\n", "    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n", "    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum number of samples required to split an internal node:\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n", "    This parameter is deprecated and will be removed in v0.24.\n    .. deprecated:: 0.22\n", "    Controls the randomness of the estimator. The features are always\n    randomly permuted at each split, even if ``splitter`` is set to\n    ``\"best\"``. When ``max_features < n_features``, the algorithm will\n    select ``max_features`` at random at each split before finding the best\n    split among them. But the best found split may vary across different\n    runs, even if ``max_features=n_features``. That is the case, if the\n    improvement of the criterion is identical for several splits and one\n    split has to be selected at random. To obtain a deterministic behaviour\n    during fitting, ``random_state`` has to be fixed to an integer.\n    See :term:`Glossary <random_state>` for details.\n", "    The strategy used to choose the split at each node. Supported\n    strategies are \"best\" to choose the best split and \"random\" to choose\n    the best random split.\n"]}, "elastic net": {"docs_name": "sklearn.linear_model.ElasticNet", "preset_params": ["base elastic net", "elastic regression", "elastic regression extra"], "param_names": ["alpha", "copy_X", "fit_intercept", "l1_ratio", "max_iter", "normalize", "positive", "precompute", "random_state", "selection", "tol", "warm_start"], "default_param_types": ["float", "bool", "bool", "float", "int", "bool", "bool", "bool", "NoneType", "str", "float", "bool"], "default_params": ["1.0", "True", "True", "0.5", "1000", "False", "False", "False", "None", "'cyclic'", "0.0001", "False"], "options": ["", ["True", "False"], ["True", "False"], "", "", ["True", "False"], ["True", "False"], ["True", "False"], "", ["'cyclic'", "'random'"], "", ["True", "False"]], "descrs": ["    Constant that multiplies the penalty terms. Defaults to 1.0.\n    See the notes for the exact mathematical meaning of this\n    parameter. ``alpha = 0`` is equivalent to an ordinary least square,\n    solved by the :class:`LinearRegression` object. For numerical\n    reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n    Given this, you should use the :class:`LinearRegression` object.\n", "    If ``True``, X will be copied; else, it may be overwritten.\n", "    Whether the intercept should be estimated or not. If ``False``, the\n    data is assumed to be already centered.\n", "    The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For\n    ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it\n    is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a\n    combination of L1 and L2.\n", "    The maximum number of iterations\n", "    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n", "    When set to ``True``, forces the coefficients to be positive.\n", "    Whether to use a precomputed Gram matrix to speed up\n    calculations. The Gram matrix can also be passed as argument.\n    For sparse input this option is always ``True`` to preserve sparsity.\n", "    The seed of the pseudo random number generator that selects a random\n    feature to update. Used when ``selection`` == 'random'.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    If set to 'random', a random coefficient is updated every iteration\n    rather than looping over features sequentially by default. This\n    (setting to 'random') often leads to significantly faster convergence\n    especially when tol is higher than 1e-4.\n", "    The tolerance for the optimization: if the updates are\n    smaller than ``tol``, the optimization code checks the\n    dual gap for optimality and continues until it is smaller\n    than ``tol``.\n", "    When set to ``True``, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    See :term:`the Glossary <warm_start>`.\n"]}, "et": {"docs_name": "sklearn.ensemble.ExtraTreesRegressor", "preset_params": ["default"], "param_names": ["bootstrap", "ccp_alpha", "criterion", "max_depth", "max_features", "max_leaf_nodes", "max_samples", "min_impurity_decrease", "min_impurity_split", "min_samples_leaf", "min_samples_split", "min_weight_fraction_leaf", "n_estimators", "n_jobs", "oob_score", "random_state", "verbose", "warm_start"], "default_param_types": ["bool", "float", "str", "NoneType", "str", "NoneType", "NoneType", "float", "NoneType", "int", "int", "float", "int", "NoneType", "bool", "NoneType", "int", "bool"], "default_params": ["False", "0.0", "'mse'", "None", "'auto'", "None", "None", "0.0", "None", "1", "2", "0.0", "100", "None", "False", "None", "0", "False"], "options": [["True", "False"], "", ["\"mse\"", "\"mae\""], "", ["\"auto\"", "\"sqrt\"", "\"log2\""], "", "", "", "", "", "", "", "", "", ["True", "False"], "", "", ["True", "False"]], "descrs": ["    Whether bootstrap samples are used when building trees. If False, the\n    whole dataset is used to build each tree.\n", "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n    .. versionadded:: 0.22\n", "    The function to measure the quality of a split. Supported criteria\n    are \"mse\" for the mean squared error, which is equal to variance\n    reduction as feature selection criterion, and \"mae\" for the mean\n    absolute error.\n    .. versionadded:: 0.18\n       Mean Absolute Error (MAE) criterion.\n", "    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n", "    The number of features to consider when looking for the best split:\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `int(max_features * n_features)` features are considered at each\n      split.\n    - If \"auto\", then `max_features=n_features`.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n", "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n", "    If bootstrap is True, the number of samples to draw from X\n    to train each base estimator.\n    - If None (default), then draw `X.shape[0]` samples.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n      `max_samples` should be in the interval `(0, 1)`.\n    .. versionadded:: 0.22\n", "    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n    The weighted impurity decrease equation is the following::\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n    .. versionadded:: 0.19\n", "    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n", "    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum number of samples required to split an internal node:\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n", "    The number of trees in the forest.\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n", "    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    <n_jobs>` for more details.\n", "    Whether to use out-of-bag samples to estimate the R^2 on unseen data.\n", "    Controls 3 sources of randomness:\n    - the bootstrapping of the samples used when building trees\n      (if ``bootstrap=True``)\n    - the sampling of the features to consider when looking for the best\n      split at each node (if ``max_features < n_features``)\n    - the draw of the splits for each of the `max_features`\n    See :term:`Glossary <random_state>` for details.\n", "    Controls the verbosity when fitting and predicting.\n", "    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`the Glossary <warm_start>`.\n"]}, "gb": {"docs_name": "sklearn.ensemble.GradientBoostingRegressor", "preset_params": ["default"], "param_names": ["alpha", "ccp_alpha", "criterion", "init", "learning_rate", "loss", "max_depth", "max_features", "max_leaf_nodes", "min_impurity_decrease", "min_impurity_split", "min_samples_leaf", "min_samples_split", "min_weight_fraction_leaf", "n_estimators", "n_iter_no_change", "presort", "random_state", "subsample", "tol", "validation_fraction", "verbose", "warm_start"], "default_param_types": ["float", "float", "str", "NoneType", "float", "str", "int", "NoneType", "NoneType", "float", "NoneType", "int", "int", "float", "int", "NoneType", "str", "NoneType", "float", "float", "float", "int", "bool"], "default_params": ["0.9", "0.0", "'friedman_mse'", "None", "0.1", "'ls'", "3", "None", "None", "0.0", "None", "1", "2", "0.0", "100", "None", "'deprecated'", "None", "1.0", "0.0001", "0.1", "0", "False"], "options": ["", "", ["'friedman_mse'", "'mse'", "'mae'"], "", "", ["'ls'", "'lad'", "'huber'", "'quantile'"], "", ["'auto'", "'sqrt'", "'log2'"], "", "", "", "", "", "", "", "", "", "", "", "", "", "", ["True", "False"]], "descrs": ["    The alpha-quantile of the huber loss function and the quantile\n    loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n", "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n    .. versionadded:: 0.22\n", "    The function to measure the quality of a split. Supported criteria\n    are \"friedman_mse\" for the mean squared error with improvement\n    score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n    the mean absolute error. The default value of \"friedman_mse\" is\n    generally the best as it can provide a better approximation in\n    some cases.\n    .. versionadded:: 0.18\n", "    An estimator object that is used to compute the initial predictions.\n    ``init`` has to provide :term:`fit` and :term:`predict`. If 'zero', the\n    initial raw predictions are set to zero. By default a\n    ``DummyEstimator`` is used, predicting either the average target value\n    (for loss='ls'), or a quantile for the other losses.\n", "    learning rate shrinks the contribution of each tree by `learning_rate`.\n    There is a trade-off between learning_rate and n_estimators.\n", "    loss function to be optimized. 'ls' refers to least squares\n    regression. 'lad' (least absolute deviation) is a highly robust\n    loss function solely based on order information of the input\n    variables. 'huber' is a combination of the two. 'quantile'\n    allows quantile regression (use `alpha` to specify the quantile).\n", "    maximum depth of the individual regression estimators. The maximum\n    depth limits the number of nodes in the tree. Tune this parameter\n    for best performance; the best value depends on the interaction\n    of the input variables.\n", "    The number of features to consider when looking for the best split:\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `int(max_features * n_features)` features are considered at each\n      split.\n    - If \"auto\", then `max_features=n_features`.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n    Choosing `max_features < n_features` leads to a reduction of variance\n    and an increase in bias.\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n", "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n", "    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n    The weighted impurity decrease equation is the following::\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n    .. versionadded:: 0.19\n", "    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n", "    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum number of samples required to split an internal node:\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n", "    The number of boosting stages to perform. Gradient boosting\n    is fairly robust to over-fitting so a large number usually\n    results in better performance.\n", "    ``n_iter_no_change`` is used to decide if early stopping will be used\n    to terminate training when validation score is not improving. By\n    default it is set to None to disable early stopping. If set to a\n    number, it will set aside ``validation_fraction`` size of the training\n    data as validation and terminate training when validation score is not\n    improving in all of the previous ``n_iter_no_change`` numbers of\n    iterations.\n    .. versionadded:: 0.20\n", "    This parameter is deprecated and will be removed in v0.24.\n    .. deprecated :: 0.22\n", "    Controls the random seed given to each Tree estimator at each\n    boosting iteration.\n    In addition, it controls the random permutation of the features at\n    each split (see Notes for more details).\n    It also controls the random spliting of the training data to obtain a\n    validation set if `n_iter_no_change` is not None.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    The fraction of samples to be used for fitting the individual base\n    learners. If smaller than 1.0 this results in Stochastic Gradient\n    Boosting. `subsample` interacts with the parameter `n_estimators`.\n    Choosing `subsample < 1.0` leads to a reduction of variance\n    and an increase in bias.\n", "    Tolerance for the early stopping. When the loss is not improving\n    by at least tol for ``n_iter_no_change`` iterations (if set to a\n    number), the training stops.\n    .. versionadded:: 0.20\n", "    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if ``n_iter_no_change`` is set to an integer.\n    .. versionadded:: 0.20\n", "    Enable verbose output. If 1 then it prints progress and performance\n    once in a while (the more trees the lower the frequency). If greater\n    than 1 then it prints progress and performance for every tree.\n", "    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just erase the\n    previous solution. See :term:`the Glossary <warm_start>`.\n"]}, "gp": {"docs_name": "sklearn.gaussian_process.GaussianProcessRegressor", "preset_params": ["base gp regressor"], "param_names": ["alpha", "copy_X_train", "kernel", "n_restarts_optimizer", "normalize_y", "optimizer", "random_state"], "default_param_types": ["float", "bool", "NoneType", "int", "bool", "str", "NoneType"], "default_params": ["1e-10", "True", "None", "0", "False", "'fmin_l_bfgs_b'", "None"], "options": ["", ["True", "False"], "", "", ["True", "False"], "", ""], "descrs": ["    Value added to the diagonal of the kernel matrix during fitting.\n    Larger values correspond to increased noise level in the observations.\n    This can also prevent a potential numerical issue during fitting, by\n    ensuring that the calculated values form a positive definite matrix.\n    If an array is passed, it must have the same number of entries as the\n    data used for fitting and is used as datapoint-dependent noise level.\n    Note that this is equivalent to adding a WhiteKernel with c=alpha.\n    Allowing to specify the noise level directly as a parameter is mainly\n    for convenience and for consistency with Ridge.\n", "    If True, a persistent copy of the training data is stored in the\n    object. Otherwise, just a reference to the training data is stored,\n    which might cause predictions to change if the data is modified\n    externally.\n", "    The kernel specifying the covariance function of the GP. If None is\n    passed, the kernel \"1.0 * RBF(1.0)\" is used as default. Note that\n    the kernel's hyperparameters are optimized during fitting.\n", "    The number of restarts of the optimizer for finding the kernel's\n    parameters which maximize the log-marginal likelihood. The first run\n    of the optimizer is performed from the kernel's initial parameters,\n    the remaining ones (if any) from thetas sampled log-uniform randomly\n    from the space of allowed theta-values. If greater than 0, all bounds\n    must be finite. Note that n_restarts_optimizer == 0 implies that one\n    run is performed.\n", "    Whether the target values y are normalized, the mean and variance of\n    the target values are set equal to 0 and 1 respectively. This is\n    recommended for cases where zero-mean, unit-variance priors are used.\n    Note that, in this implementation, the normalisation is reversed\n    before the GP predictions are reported.\n    .. versionchanged:: 0.23\n", "    Can either be one of the internally supported optimizers for optimizing\n    the kernel's parameters, specified by a string, or an externally\n    defined optimizer passed as a callable. If a callable is passed, it\n    must have the signature::\n        def optimizer(obj_func, initial_theta, bounds):\n            # * 'obj_func' is the objective function to be minimized, which\n            #   takes the hyperparameters theta as parameter and an\n            #   optional flag eval_gradient, which determines if the\n            #   gradient is returned additionally to the function value\n            # * 'initial_theta': the initial value for theta, which can be\n            #   used by local optimizers\n            # * 'bounds': the bounds on the values of theta\n            ....\n            # Returned are the best found hyperparameters theta and\n            # the corresponding value of the target function.\n            return theta_opt, func_min\n    Per default, the 'L-BGFS-B' algorithm from scipy.optimize.minimize\n    is used. If None is passed, the kernel's parameters are kept fixed.\n    Available internal optimizers are::\n        'fmin_l_bfgs_b'\n", "    Determines random number generation used to initialize the centers.\n    Pass an int for reproducible results across multiple function calls.\n    See :term: `Glossary <random_state>`.\n"]}, "hgb": {"docs_name": "sklearn.ensemble.gradient_boosting.HistGradientBoostingRegressor", "preset_params": ["default"], "param_names": ["early_stopping", "l2_regularization", "learning_rate", "loss", "max_bins", "max_depth", "max_iter", "max_leaf_nodes", "min_samples_leaf", "monotonic_cst", "n_iter_no_change", "random_state", "scoring", "tol", "validation_fraction", "verbose", "warm_start"], "default_param_types": ["str", "float", "float", "str", "int", "NoneType", "int", "int", "int", "NoneType", "int", "NoneType", "str", "float", "float", "int", "bool"], "default_params": ["'auto'", "0.0", "0.1", "'least_squares'", "255", "None", "100", "31", "20", "None", "10", "None", "'loss'", "1e-07", "0.1", "0", "False"], "options": ["", "", "", ["'least_squares'", "'least_absolute_deviation'", "'poisson'"], "", "", "", "", "", "", "", "", "", "", "", "", ["True", "False"]], "descrs": ["    If 'auto', early stopping is enabled if the sample size is larger than\n    10000. If True, early stopping is enabled, otherwise early stopping is\n    disabled.\n", "    The L2 regularization parameter. Use ``0`` for no regularization\n    (default).\n", "    The learning rate, also known as *shrinkage*. This is used as a\n    multiplicative factor for the leaves values. Use ``1`` for no\n    shrinkage.\n", "    The loss function to use in the boosting process. Note that the\n    \"least squares\" and \"poisson\" losses actually implement\n    \"half least squares loss\" and \"half poisson deviance\" to simplify the\n    computation of the gradient. Furthermore, \"poisson\" loss internally\n    uses a log-link and requires ``y >= 0``\n", "    The maximum number of bins to use for non-missing values. Before\n    training, each feature of the input array `X` is binned into\n    integer-valued bins, which allows for a much faster training stage.\n    Features with a small number of unique values may use less than\n    ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\n    is always reserved for missing values. Must be no larger than 255.\n", "    The maximum depth of each tree. The depth of a tree is the number of\n    edges to go from the root to the deepest leaf.\n    Depth isn't constrained by default.\n", "    The maximum number of iterations of the boosting process, i.e. the\n    maximum number of trees.\n", "    The maximum number of leaves for each tree. Must be strictly greater\n    than 1. If None, there is no maximum limit.\n", "    The minimum number of samples per leaf. For small datasets with less\n    than a few hundred samples, it is recommended to lower this value\n    since only very shallow trees would be built.\n", "    Indicates the monotonic constraint to enforce on each feature. -1, 1\n    and 0 respectively correspond to a positive constraint, negative\n    constraint and no constraint. Read more in the :ref:`User Guide\n    <monotonic_cst_gbdt>`.\n", "    Used to determine when to \"early stop\". The fitting process is\n    stopped when none of the last ``n_iter_no_change`` scores are better\n    than the ``n_iter_no_change - 1`` -th-to-last one, up to some\n    tolerance. Only used if early stopping is performed.\n", "    Pseudo-random number generator to control the subsampling in the\n    binning process, and the train/validation data split if early stopping\n    is enabled.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    Scoring parameter to use for early stopping. It can be a single\n    string (see :ref:`scoring_parameter`) or a callable (see\n    :ref:`scoring`). If None, the estimator's default scorer is used. If\n    ``scoring='loss'``, early stopping is checked w.r.t the loss value.\n    Only used if early stopping is performed.\n", "    The absolute tolerance to use when comparing scores during early\n    stopping. The higher the tolerance, the more likely we are to early\n    stop: higher tolerance means that it will be harder for subsequent\n    iterations to be considered an improvement upon the reference score.\n", "    Proportion (or absolute size) of training data to set aside as\n    validation data for early stopping. If None, early stopping is done on\n    the training data. Only used if early stopping is performed.\n", "    The verbosity level. If not zero, print some information about the\n    fitting process.\n", "    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble. For results to be valid, the\n    estimator should be re-trained on the same data only.\n    See :term:`the Glossary <warm_start>`.\n"]}, "knn": {"docs_name": "sklearn.neighbors.KNeighborsRegressor", "preset_params": ["base knn regression", "knn dist regression"], "param_names": ["algorithm", "leaf_size", "metric", "metric_params", "n_jobs", "n_neighbors", "p", "weights"], "default_param_types": ["str", "int", "str", "NoneType", "NoneType", "int", "int", "str"], "default_params": ["'auto'", "30", "'minkowski'", "None", "None", "5", "2", "'uniform'"], "options": [["'auto'", "'ball_tree'", "'kd_tree'", "'brute'"], "", "", "", "", "", "", ["'uniform'", "'distance'"]], "descrs": ["    Algorithm used to compute the nearest neighbors:\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n", "    Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem.\n", "    the distance metric to use for the tree.  The default metric is\n    minkowski, and with p=2 is equivalent to the standard Euclidean\n    metric. See the documentation of :class:`DistanceMetric` for a\n    list of available metrics.\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`,\n    in which case only \"nonzero\" elements may be considered neighbors.\n", "    Additional keyword arguments for the metric function.\n", "    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n    Doesn't affect :meth:`fit` method.\n", "    Number of neighbors to use by default for :meth:`kneighbors` queries.\n", "    Power parameter for the Minkowski metric. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n", "    weight function used in prediction.  Possible values:\n    - 'uniform' : uniform weights.  All points in each neighborhood\n      are weighted equally.\n    - 'distance' : weight points by the inverse of their distance.\n      in this case, closer neighbors of a query point will have a\n      greater influence than neighbors which are further away.\n    - [callable] : a user-defined function which accepts an\n      array of distances, and returns an array of the same shape\n      containing the weights.\n    Uniform weights are used by default.\n"]}, "lasso": {"docs_name": "sklearn.linear_model.Lasso", "preset_params": ["base lasso regressor", "lasso regressor dist"], "param_names": ["alpha", "copy_X", "fit_intercept", "max_iter", "normalize", "positive", "precompute", "random_state", "selection", "tol", "warm_start"], "default_param_types": ["float", "bool", "bool", "int", "bool", "bool", "bool", "NoneType", "str", "float", "bool"], "default_params": ["1.0", "True", "True", "1000", "False", "False", "False", "None", "'cyclic'", "0.0001", "False"], "options": ["", ["True", "False"], ["True", "False"], "", ["True", "False"], ["True", "False"], ["True", "False"], "", ["'cyclic'", "'random'"], "", ["True", "False"]], "descrs": ["    Constant that multiplies the L1 term. Defaults to 1.0.\n    ``alpha = 0`` is equivalent to an ordinary least square, solved\n    by the :class:`LinearRegression` object. For numerical\n    reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n    Given this, you should use the :class:`LinearRegression` object.\n", "    If ``True``, X will be copied; else, it may be overwritten.\n", "    Whether to calculate the intercept for this model. If set\n    to False, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n", "    The maximum number of iterations\n", "    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n", "    When set to ``True``, forces the coefficients to be positive.\n", "    Whether to use a precomputed Gram matrix to speed up\n    calculations. If set to ``'auto'`` let us decide. The Gram\n    matrix can also be passed as argument. For sparse input\n    this option is always ``True`` to preserve sparsity.\n", "    The seed of the pseudo random number generator that selects a random\n    feature to update. Used when ``selection`` == 'random'.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    If set to 'random', a random coefficient is updated every iteration\n    rather than looping over features sequentially by default. This\n    (setting to 'random') often leads to significantly faster convergence\n    especially when tol is higher than 1e-4.\n", "    The tolerance for the optimization: if the updates are\n    smaller than ``tol``, the optimization code checks the\n    dual gap for optimality and continues until it is smaller\n    than ``tol``.\n", "    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    See :term:`the Glossary <warm_start>`.\n"]}, "linear": {"docs_name": "sklearn.linear_model.LinearRegression", "preset_params": ["base linear"], "param_names": ["copy_X", "fit_intercept", "n_jobs", "normalize"], "default_param_types": ["bool", "bool", "NoneType", "bool"], "default_params": ["True", "True", "None", "False"], "options": [["True", "False"], ["True", "False"], "", ["True", "False"]], "descrs": ["    If True, X will be copied; else, it may be overwritten.\n", "    Whether to calculate the intercept for this model. If set\n    to False, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n", "    The number of jobs to use for the computation. This will only provide\n    speedup for n_targets > 1 and sufficient large problems.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n", "    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on\n    an estimator with ``normalize=False``.\n"]}, "linear svm": {"docs_name": "sklearn.svm.LinearSVR", "preset_params": ["base linear svr", "linear svr dist"], "param_names": ["C", "dual", "epsilon", "fit_intercept", "intercept_scaling", "loss", "max_iter", "random_state", "tol", "verbose"], "default_param_types": ["float", "bool", "float", "bool", "float", "str", "int", "NoneType", "float", "int"], "default_params": ["1.0", "True", "0.0", "True", "1.0", "'epsilon_insensitive'", "1000", "None", "0.0001", "0"], "options": ["", ["True", "False"], "", ["True", "False"], "", ["'epsilon_insensitive'", "'squared_epsilon_insensitive'"], "", "", "", ""], "descrs": ["    Regularization parameter. The strength of the regularization is\n    inversely proportional to C. Must be strictly positive.\n", "    Select the algorithm to either solve the dual or primal\n    optimization problem. Prefer dual=False when n_samples > n_features.\n", "    Epsilon parameter in the epsilon-insensitive loss function. Note\n    that the value of this parameter depends on the scale of the target\n    variable y. If unsure, set ``epsilon=0``.\n", "    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be already centered).\n", "    When self.fit_intercept is True, instance vector x becomes\n    [x, self.intercept_scaling],\n    i.e. a \"synthetic\" feature with constant value equals to\n    intercept_scaling is appended to the instance vector.\n    The intercept becomes intercept_scaling * synthetic feature weight\n    Note! the synthetic feature weight is subject to l1/l2 regularization\n    as all other features.\n    To lessen the effect of regularization on synthetic feature weight\n    (and therefore on the intercept) intercept_scaling has to be increased.\n", "    Specifies the loss function. The epsilon-insensitive loss\n    (standard SVR) is the L1 loss, while the squared epsilon-insensitive\n    loss ('squared_epsilon_insensitive') is the L2 loss.\n", "    The maximum number of iterations to be run.\n", "    Controls the pseudo random number generation for shuffling the data.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    Tolerance for stopping criteria.\n", "    Enable verbose output. Note that this setting takes advantage of a\n    per-process runtime setting in liblinear that, if enabled, may not work\n    properly in a multithreaded context.\n"]}, "mlp": {"docs_name": "ABCD_ML.extensions.MLP.MLPRegressor_Wrapper", "preset_params": ["default", "mlp dist 3 layer", "mlp dist es 3 layer", "mlp dist 2 layer", "mlp dist es 2 layer", "mlp dist 1 layer", "mlp dist es 1 layer"], "param_names": ["activation", "alpha", "batch_size", "beta_1", "beta_2", "early_stopping", "epsilon", "hidden_layer_sizes", "learning_rate", "learning_rate_init", "max_fun", "max_iter", "momentum", "n_iter_no_change", "nesterovs_momentum", "power_t", "random_state", "shuffle", "solver", "tol", "validation_fraction", "verbose", "warm_start"], "default_param_types": ["str", "float", "str", "float", "float", "bool", "float", "tuple", "str", "float", "int", "int", "float", "int", "bool", "float", "NoneType", "bool", "str", "float", "float", "bool", "bool"], "default_params": ["'relu'", "0.0001", "'auto'", "0.9", "0.999", "False", "1e-08", "(100,)", "'constant'", "0.001", "15000", "200", "0.9", "10", "True", "0.5", "None", "True", "'adam'", "0.0001", "0.1", "False", "False"], "options": [["'identity'", "'logistic'", "'tanh'", "'relu'"], "", "", "", "", ["True", "False"], "", "", ["'constant'", "'invscaling'", "'adaptive'"], "", "", "", "", "", ["True", "False"], "", "", ["True", "False"], ["'lbfgs'", "'sgd'", "'adam'"], "", "", ["True", "False"], ["True", "False"]], "descrs": ["    Activation function for the hidden layer.\n    - 'identity', no-op activation, useful to implement linear bottleneck,\n      returns f(x) = x\n    - 'logistic', the logistic sigmoid function,\n      returns f(x) = 1 / (1 + exp(-x)).\n    - 'tanh', the hyperbolic tan function,\n      returns f(x) = tanh(x).\n    - 'relu', the rectified linear unit function,\n      returns f(x) = max(0, x)\n", "    L2 penalty (regularization term) parameter.\n", "    Size of minibatches for stochastic optimizers.\n    If the solver is 'lbfgs', the classifier will not use minibatch.\n    When set to \"auto\", `batch_size=min(200, n_samples)`\n", "    Exponential decay rate for estimates of first moment vector in adam,\n    should be in [0, 1). Only used when solver='adam'\n", "    Exponential decay rate for estimates of second moment vector in adam,\n    should be in [0, 1). Only used when solver='adam'\n", "    Whether to use early stopping to terminate training when validation\n    score is not improving. If set to true, it will automatically set\n    aside 10% of training data as validation and terminate training when\n    validation score is not improving by at least ``tol`` for\n    ``n_iter_no_change`` consecutive epochs.\n    Only effective when solver='sgd' or 'adam'\n", "    Value for numerical stability in adam. Only used when solver='adam'\n", "    The ith element represents the number of neurons in the ith\n    hidden layer.\n", "    Learning rate schedule for weight updates.\n    - 'constant' is a constant learning rate given by\n      'learning_rate_init'.\n    - 'invscaling' gradually decreases the learning rate ``learning_rate_``\n      at each time step 't' using an inverse scaling exponent of 'power_t'.\n      effective_learning_rate = learning_rate_init / pow(t, power_t)\n    - 'adaptive' keeps the learning rate constant to\n      'learning_rate_init' as long as training loss keeps decreasing.\n      Each time two consecutive epochs fail to decrease training loss by at\n      least tol, or fail to increase validation score by at least tol if\n      'early_stopping' is on, the current learning rate is divided by 5.\n    Only used when solver='sgd'.\n", "    The initial learning rate used. It controls the step-size\n    in updating the weights. Only used when solver='sgd' or 'adam'.\n", "    Only used when solver='lbfgs'. Maximum number of function calls.\n    The solver iterates until convergence (determined by 'tol'), number\n    of iterations reaches max_iter, or this number of function calls.\n    Note that number of function calls will be greater than or equal to\n    the number of iterations for the MLPRegressor.\n    .. versionadded:: 0.22\n", "    Maximum number of iterations. The solver iterates until convergence\n    (determined by 'tol') or this number of iterations. For stochastic\n    solvers ('sgd', 'adam'), note that this determines the number of epochs\n    (how many times each data point will be used), not the number of\n    gradient steps.\n", "    Momentum for gradient descent update.  Should be between 0 and 1. Only\n    used when solver='sgd'.\n", "    Maximum number of epochs to not meet ``tol`` improvement.\n    Only effective when solver='sgd' or 'adam'\n    .. versionadded:: 0.20\n", "    Whether to use Nesterov's momentum. Only used when solver='sgd' and\n    momentum > 0.\n", "    The exponent for inverse scaling learning rate.\n    It is used in updating effective learning rate when the learning_rate\n    is set to 'invscaling'. Only used when solver='sgd'.\n", "    Determines random number generation for weights and bias\n    initialization, train-test split if early stopping is used, and batch\n    sampling when solver='sgd' or 'adam'.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    Whether to shuffle samples in each iteration. Only used when\n    solver='sgd' or 'adam'.\n", "    The solver for weight optimization.\n    - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n    - 'sgd' refers to stochastic gradient descent.\n    - 'adam' refers to a stochastic gradient-based optimizer proposed by\n      Kingma, Diederik, and Jimmy Ba\n    Note: The default solver 'adam' works pretty well on relatively\n    large datasets (with thousands of training samples or more) in terms of\n    both training time and validation score.\n    For small datasets, however, 'lbfgs' can converge faster and perform\n    better.\n", "    Tolerance for the optimization. When the loss or score is not improving\n    by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\n    unless ``learning_rate`` is set to 'adaptive', convergence is\n    considered to be reached and training stops.\n", "    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if early_stopping is True\n", "    Whether to print progress messages to stdout.\n", "    When set to True, reuse the solution of the previous\n    call to fit as initialization, otherwise, just erase the\n    previous solution. See :term:`the Glossary <warm_start>`.\n"]}, "random forest": {"docs_name": "sklearn.ensemble.RandomForestRegressor", "preset_params": ["base rf", "rf dist"], "param_names": ["bootstrap", "ccp_alpha", "criterion", "max_depth", "max_features", "max_leaf_nodes", "max_samples", "min_impurity_decrease", "min_impurity_split", "min_samples_leaf", "min_samples_split", "min_weight_fraction_leaf", "n_estimators", "n_jobs", "oob_score", "random_state", "verbose", "warm_start"], "default_param_types": ["bool", "float", "str", "NoneType", "str", "NoneType", "NoneType", "float", "NoneType", "int", "int", "float", "int", "NoneType", "bool", "NoneType", "int", "bool"], "default_params": ["True", "0.0", "'mse'", "None", "'auto'", "None", "None", "0.0", "None", "1", "2", "0.0", "100", "None", "False", "None", "0", "False"], "options": [["True", "False"], "", ["\"mse\"", "\"mae\""], "", ["\"auto\"", "\"sqrt\"", "\"log2\""], "", "", "", "", "", "", "", "", "", ["True", "False"], "", "", ["True", "False"]], "descrs": ["    Whether bootstrap samples are used when building trees. If False, the\n    whole dataset is used to build each tree.\n", "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n    .. versionadded:: 0.22\n", "    The function to measure the quality of a split. Supported criteria\n    are \"mse\" for the mean squared error, which is equal to variance\n    reduction as feature selection criterion, and \"mae\" for the mean\n    absolute error.\n    .. versionadded:: 0.18\n       Mean Absolute Error (MAE) criterion.\n", "    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n", "    The number of features to consider when looking for the best split:\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `int(max_features * n_features)` features are considered at each\n      split.\n    - If \"auto\", then `max_features=n_features`.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n", "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n", "    If bootstrap is True, the number of samples to draw from X\n    to train each base estimator.\n    - If None (default), then draw `X.shape[0]` samples.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n      `max_samples` should be in the interval `(0, 1)`.\n    .. versionadded:: 0.22\n", "    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n    The weighted impurity decrease equation is the following::\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n    .. versionadded:: 0.19\n", "    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n", "    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum number of samples required to split an internal node:\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n", "    The number of trees in the forest.\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n", "    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    <n_jobs>` for more details.\n", "    whether to use out-of-bag samples to estimate\n    the R^2 on unseen data.\n", "    Controls both the randomness of the bootstrapping of the samples used\n    when building trees (if ``bootstrap=True``) and the sampling of the\n    features to consider when looking for the best split at each node\n    (if ``max_features < n_features``).\n    See :term:`Glossary <random_state>` for details.\n", "    Controls the verbosity when fitting and predicting.\n", "    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`the Glossary <warm_start>`.\n"]}, "ridge": {"docs_name": "sklearn.linear_model.Ridge", "preset_params": ["base ridge regressor", "ridge regressor dist"], "param_names": ["alpha", "copy_X", "fit_intercept", "max_iter", "normalize", "random_state", "solver", "tol"], "default_param_types": ["float", "bool", "bool", "NoneType", "bool", "NoneType", "str", "float"], "default_params": ["1.0", "True", "True", "None", "False", "None", "'auto'", "0.001"], "options": [["float", "ndarray of shape (n_targets", ")"], ["True", "False"], ["True", "False"], "", ["True", "False"], "", ["'auto'", "'svd'", "'cholesky'", "'lsqr'", "'sparse_cg'", "'sag'", "'saga'"], ""], "descrs": ["    Regularization strength; must be a positive float. Regularization\n    improves the conditioning of the problem and reduces the variance of\n    the estimates. Larger values specify stronger regularization.\n    Alpha corresponds to ``1 / (2C)`` in other linear models such as\n    :class:`~sklearn.linear_model.LogisticRegression` or\n    :class:`sklearn.svm.LinearSVC`. If an array is passed, penalties are\n    assumed to be specific to the targets. Hence they must correspond in\n    number.\n", "    If True, X will be copied; else, it may be overwritten.\n", "    Whether to fit the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. ``X`` and ``y`` are expected to be centered).\n", "    Maximum number of iterations for conjugate gradient solver.\n    For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n    by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n", "    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n", "    Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n    See :term:`Glossary <random_state>` for details.\n    .. versionadded:: 0.17\n       `random_state` to support Stochastic Average Gradient.\n", "    Solver to use in the computational routines:\n    - 'auto' chooses the solver automatically based on the type of data.\n    - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n      coefficients. More stable for singular matrices than 'cholesky'.\n    - 'cholesky' uses the standard scipy.linalg.solve function to\n      obtain a closed-form solution.\n    - 'sparse_cg' uses the conjugate gradient solver as found in\n      scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n      more appropriate than 'cholesky' for large-scale data\n      (possibility to set `tol` and `max_iter`).\n    - 'lsqr' uses the dedicated regularized least-squares routine\n      scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n      procedure.\n    - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n      its improved, unbiased version named SAGA. Both methods also use an\n      iterative procedure, and are often faster than other solvers when\n      both n_samples and n_features are large. Note that 'sag' and\n      'saga' fast convergence is only guaranteed on features with\n      approximately the same scale. You can preprocess the data with a\n      scaler from sklearn.preprocessing.\n    All last five solvers support both dense and sparse data. However, only\n    'sag' and 'sparse_cg' supports sparse input when `fit_intercept` is\n    True.\n    .. versionadded:: 0.17\n       Stochastic Average Gradient descent solver.\n    .. versionadded:: 0.19\n       SAGA solver.\n", "    Precision of the solution.\n"]}, "svm": {"docs_name": "sklearn.svm.SVR", "preset_params": ["base svm", "svm dist"], "param_names": ["C", "cache_size", "coef0", "degree", "epsilon", "gamma", "kernel", "max_iter", "shrinking", "tol", "verbose"], "default_param_types": ["float", "int", "float", "int", "float", "str", "str", "int", "bool", "float", "bool"], "default_params": ["1.0", "200", "0.0", "3", "0.1", "'scale'", "'rbf'", "-1", "True", "0.001", "False"], "options": ["", "", "", "", "", ["'scale'", "'auto'"], ["'linear'", "'poly'", "'rbf'", "'sigmoid'", "'precomputed'"], "", ["True", "False"], "", ["True", "False"]], "descrs": ["    Regularization parameter. The strength of the regularization is\n    inversely proportional to C. Must be strictly positive.\n    The penalty is a squared l2 penalty.\n", "    Specify the size of the kernel cache (in MB).\n", "    Independent term in kernel function.\n    It is only significant in 'poly' and 'sigmoid'.\n", "    Degree of the polynomial kernel function ('poly').\n    Ignored by all other kernels.\n", "     Epsilon in the epsilon-SVR model. It specifies the epsilon-tube\n     within which no penalty is associated in the training loss function\n     with points predicted within a distance epsilon from the actual\n     value.\n", "    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n    - if ``gamma='scale'`` (default) is passed then it uses\n      1 / (n_features * X.var()) as value of gamma,\n    - if 'auto', uses 1 / n_features.\n    .. versionchanged:: 0.22\n       The default value of ``gamma`` changed from 'auto' to 'scale'.\n", "     Specifies the kernel type to be used in the algorithm.\n     It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n     a callable.\n     If none is given, 'rbf' will be used. If a callable is given it is\n     used to precompute the kernel matrix.\n", "    Hard limit on iterations within solver, or -1 for no limit.\n", "    Whether to use the shrinking heuristic.\n    See the :ref:`User Guide <shrinking_svm>`.\n", "    Tolerance for stopping criterion.\n", "    Enable verbose output. Note that this setting takes advantage of a\n    per-process runtime setting in libsvm that, if enabled, may not work\n    properly in a multithreaded context.\n"]}, "tweedie": {"docs_name": "sklearn.linear_model.glm.TweedieRegressor", "preset_params": ["default"], "param_names": ["alpha", "fit_intercept", "link", "max_iter", "power", "tol", "verbose", "warm_start"], "default_param_types": ["float", "bool", "str", "int", "float", "float", "int", "bool"], "default_params": ["1.0", "True", "'auto'", "100", "0.0", "0.0001", "0", "False"], "options": ["", ["True", "False"], ["'auto'", "'identity'", "'log'"], "", "", "", "", ["True", "False"]], "descrs": ["    Constant that multiplies the penalty term and thus determines the\n    regularization strength. ``alpha = 0`` is equivalent to unpenalized\n    GLMs. In this case, the design matrix `X` must have full column rank\n    (no collinearities).\n", "    Specifies if a constant (a.k.a. bias or intercept) should be\n    added to the linear predictor (X @ coef + intercept).\n", "    The link function of the GLM, i.e. mapping from linear predictor\n    `X @ coeff + intercept` to prediction `y_pred`. Option 'auto' sets\n    the link depending on the chosen family as follows:\n    - 'identity' for Normal distribution\n    - 'log' for Poisson,  Gamma and Inverse Gaussian distributions\n", "    The maximal number of iterations for the solver.\n", "        The power determines the underlying target distribution according\n        to the following table:\n        +-------+------------------------+\n        | Power | Distribution           |\n        +=======+========================+\n        | 0     | Normal                 |\n        +-------+------------------------+\n        | 1     | Poisson                |\n        +-------+------------------------+\n        | (1,2) | Compound Poisson Gamma |\n        +-------+------------------------+\n        | 2     | Gamma                  |\n        +-------+------------------------+\n        | 3     | Inverse Gaussian       |\n        +-------+------------------------+\n        For ``0 < power < 1``, no distribution exists.\n", "    Stopping criterion. For the lbfgs solver,\n    the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\n    where ``g_j`` is the j-th component of the gradient (derivative) of\n    the objective function.\n", "    For the lbfgs solver set verbose to any positive number for verbosity.\n", "    If set to ``True``, reuse the solution of the previous call to ``fit``\n    as initialization for ``coef_`` and ``intercept_`` .\n"]}}, "categorical": {"dt": {"docs_name": "sklearn.tree.DecisionTreeClassifier", "preset_params": ["default", "dt classifier dist"], "param_names": ["ccp_alpha", "class_weight", "criterion", "max_depth", "max_features", "max_leaf_nodes", "min_impurity_decrease", "min_impurity_split", "min_samples_leaf", "min_samples_split", "min_weight_fraction_leaf", "presort", "random_state", "splitter"], "default_param_types": ["float", "NoneType", "str", "NoneType", "NoneType", "NoneType", "float", "NoneType", "int", "int", "float", "str", "NoneType", "str"], "default_params": ["0.0", "None", "'gini'", "None", "None", "None", "0.0", "None", "1", "2", "0.0", "'deprecated'", "None", "'best'"], "options": ["", ["None", "balanced"], ["\"gini\"", "\"entropy\""], "", ["\"auto\"", "\"sqrt\"", "\"log2\""], "", "", "", "", "", "", "", "", ["\"best\"", "\"random\""]], "descrs": ["    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n    .. versionadded:: 0.22\n", "    Weights associated with classes in the form ``{class_label: weight}``.\n    If None, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n    For multi-output, the weights of each column of y will be multiplied.\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n", "    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n", "    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n", "    The number of features to consider when looking for the best split:\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `int(max_features * n_features)` features are considered at each\n          split.\n        - If \"auto\", then `max_features=sqrt(n_features)`.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n", "    Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n", "    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n    The weighted impurity decrease equation is the following::\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n    .. versionadded:: 0.19\n", "    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n", "    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum number of samples required to split an internal node:\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n", "    This parameter is deprecated and will be removed in v0.24.\n    .. deprecated:: 0.22\n", "    Controls the randomness of the estimator. The features are always\n    randomly permuted at each split, even if ``splitter`` is set to\n    ``\"best\"``. When ``max_features < n_features``, the algorithm will\n    select ``max_features`` at random at each split before finding the best\n    split among them. But the best found split may vary across different\n    runs, even if ``max_features=n_features``. That is the case, if the\n    improvement of the criterion is identical for several splits and one\n    split has to be selected at random. To obtain a deterministic behaviour\n    during fitting, ``random_state`` has to be fixed to an integer.\n    See :term:`Glossary <random_state>` for details.\n", "    The strategy used to choose the split at each node. Supported\n    strategies are \"best\" to choose the best split and \"random\" to choose\n    the best random split.\n"]}, "elastic net": {"docs_name": "sklearn.linear_model.LogisticRegression", "preset_params": ["base elastic", "elastic classifier", "elastic clf v2", "elastic classifier extra"], "param_names": ["C", "class_weight", "dual", "fit_intercept", "intercept_scaling", "l1_ratio", "max_iter", "multi_class", "n_jobs", "penalty", "random_state", "solver", "tol", "verbose", "warm_start"], "default_param_types": ["float", "NoneType", "bool", "bool", "int", "NoneType", "int", "str", "NoneType", "str", "NoneType", "str", "float", "int", "bool"], "default_params": ["1.0", "None", "False", "True", "1", "None", "100", "'auto'", "None", "'l2'", "None", "'lbfgs'", "0.0001", "0", "False"], "options": ["", ["None", "balanced"], ["True", "False"], ["True", "False"], "", "", "", ["'auto'", "'ovr'", "'multinomial'"], "", ["'l1'", "'l2'", "'elasticnet'", "'none'"], "", ["'newton-cg'", "'lbfgs'", "'liblinear'", "'sag'", "'saga'"], "", "", ["True", "False"]], "descrs": ["    Inverse of regularization strength; must be a positive float.\n    Like in support vector machines, smaller values specify stronger\n    regularization.\n", "    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one.\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n    .. versionadded:: 0.17\n       *class_weight='balanced'*\n", "    Dual or primal formulation. Dual formulation is only implemented for\n    l2 penalty with liblinear solver. Prefer dual=False when\n    n_samples > n_features.\n", "    Specifies if a constant (a.k.a. bias or intercept) should be\n    added to the decision function.\n", "    Useful only when the solver 'liblinear' is used\n    and self.fit_intercept is set to True. In this case, x becomes\n    [x, self.intercept_scaling],\n    i.e. a \"synthetic\" feature with constant value equal to\n    intercept_scaling is appended to the instance vector.\n    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n    Note! the synthetic feature weight is subject to l1/l2 regularization\n    as all other features.\n    To lessen the effect of regularization on synthetic feature weight\n    (and therefore on the intercept) intercept_scaling has to be increased.\n", "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n    combination of L1 and L2.\n", "    Maximum number of iterations taken for the solvers to converge.\n", "    If the option chosen is 'ovr', then a binary problem is fit for each\n    label. For 'multinomial' the loss minimised is the multinomial loss fit\n    across the entire probability distribution, *even when the data is\n    binary*. 'multinomial' is unavailable when solver='liblinear'.\n    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n    and otherwise selects 'multinomial'.\n    .. versionadded:: 0.18\n       Stochastic Average Gradient descent solver for 'multinomial' case.\n    .. versionchanged:: 0.22\n        Default changed from 'ovr' to 'auto' in 0.22.\n", "    Number of CPU cores used when parallelizing over classes if\n    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n    set to 'liblinear' regardless of whether 'multi_class' is specified or\n    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors.\n    See :term:`Glossary <n_jobs>` for more details.\n", "    Used to specify the norm used in the penalization. The 'newton-cg',\n    'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n    only supported by the 'saga' solver. If 'none' (not supported by the\n    liblinear solver), no regularization is applied.\n    .. versionadded:: 0.19\n       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n", "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n    data. See :term:`Glossary <random_state>` for details.\n", "    Algorithm to use in the optimization problem.\n    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n      'saga' are faster for large ones.\n    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n      schemes.\n    - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n    - 'liblinear' and 'saga' also handle L1 penalty\n    - 'saga' also supports 'elasticnet' penalty\n    - 'liblinear' does not support setting ``penalty='none'``\n    Note that 'sag' and 'saga' fast convergence is only guaranteed on\n    features with approximately the same scale. You can\n    preprocess the data with a scaler from sklearn.preprocessing.\n    .. versionadded:: 0.17\n       Stochastic Average Gradient descent solver.\n    .. versionadded:: 0.19\n       SAGA solver.\n    .. versionchanged:: 0.22\n        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n", "    Tolerance for stopping criteria.\n", "    For the liblinear and lbfgs solvers set verbose to any positive\n    number for verbosity.\n", "    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n    .. versionadded:: 0.17\n       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n"]}, "et": {"docs_name": "sklearn.ensemble.ExtraTreesClassifier", "preset_params": ["default"], "param_names": ["bootstrap", "ccp_alpha", "class_weight", "criterion", "max_depth", "max_features", "max_leaf_nodes", "max_samples", "min_impurity_decrease", "min_impurity_split", "min_samples_leaf", "min_samples_split", "min_weight_fraction_leaf", "n_estimators", "n_jobs", "oob_score", "random_state", "verbose", "warm_start"], "default_param_types": ["bool", "float", "NoneType", "str", "NoneType", "str", "NoneType", "NoneType", "float", "NoneType", "int", "int", "float", "int", "NoneType", "bool", "NoneType", "int", "bool"], "default_params": ["False", "0.0", "None", "'gini'", "None", "'auto'", "None", "None", "0.0", "None", "1", "2", "0.0", "100", "None", "False", "None", "0", "False"], "options": [["True", "False"], "", ["None", "balanced"], ["\"gini\"", "\"entropy\""], "", ["\"auto\"", "\"sqrt\"", "\"log2\""], "", "", "", "", "", "", "", "", "", ["True", "False"], "", "", ["True", "False"]], "descrs": ["    Whether bootstrap samples are used when building trees. If False, the\n    whole dataset is used to build each tree.\n", "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n    .. versionadded:: 0.22\n", "    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n    The \"balanced_subsample\" mode is the same as \"balanced\" except that\n    weights are computed based on the bootstrap sample for every tree\n    grown.\n    For multi-output, the weights of each column of y will be multiplied.\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n", "    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n", "    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n", "    The number of features to consider when looking for the best split:\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `int(max_features * n_features)` features are considered at each\n      split.\n    - If \"auto\", then `max_features=sqrt(n_features)`.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n", "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n", "    If bootstrap is True, the number of samples to draw from X\n    to train each base estimator.\n    - If None (default), then draw `X.shape[0]` samples.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n      `max_samples` should be in the interval `(0, 1)`.\n    .. versionadded:: 0.22\n", "    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n    The weighted impurity decrease equation is the following::\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n    .. versionadded:: 0.19\n", "    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n", "    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum number of samples required to split an internal node:\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n", "    The number of trees in the forest.\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n", "    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    <n_jobs>` for more details.\n", "    Whether to use out-of-bag samples to estimate\n    the generalization accuracy.\n", "    Controls 3 sources of randomness:\n    - the bootstrapping of the samples used when building trees\n      (if ``bootstrap=True``)\n    - the sampling of the features to consider when looking for the best\n      split at each node (if ``max_features < n_features``)\n    - the draw of the splits for each of the `max_features`\n    See :term:`Glossary <random_state>` for details.\n", "    Controls the verbosity when fitting and predicting.\n", "    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`the Glossary <warm_start>`.\n"]}, "gaussian nb": {"docs_name": "sklearn.naive_bayes.GaussianNB", "preset_params": ["base gnb"], "param_names": ["priors", "var_smoothing"], "default_param_types": ["NoneType", "float"], "default_params": ["None", "1e-09"], "options": ["", ""], "descrs": ["    Prior probabilities of the classes. If specified the priors are not\n    adjusted according to the data.\n", "    Portion of the largest variance of all features that is added to\n    variances for calculation stability.\n    .. versionadded:: 0.20\n"]}, "gb": {"docs_name": "sklearn.ensemble.GradientBoostingClassifier", "preset_params": ["default"], "param_names": ["ccp_alpha", "criterion", "init", "learning_rate", "loss", "max_depth", "max_features", "max_leaf_nodes", "min_impurity_decrease", "min_impurity_split", "min_samples_leaf", "min_samples_split", "min_weight_fraction_leaf", "n_estimators", "n_iter_no_change", "presort", "random_state", "subsample", "tol", "validation_fraction", "verbose", "warm_start"], "default_param_types": ["float", "str", "NoneType", "float", "str", "int", "NoneType", "NoneType", "float", "NoneType", "int", "int", "float", "int", "NoneType", "str", "NoneType", "float", "float", "float", "int", "bool"], "default_params": ["0.0", "'friedman_mse'", "None", "0.1", "'deviance'", "3", "None", "None", "0.0", "None", "1", "2", "0.0", "100", "None", "'deprecated'", "None", "1.0", "0.0001", "0.1", "0", "False"], "options": ["", ["'friedman_mse'", "'mse'", "'mae'"], "", "", ["'deviance'", "'exponential'"], "", ["'auto'", "'sqrt'", "'log2'"], "", "", "", "", "", "", "", "", "", "", "", "", "", "", ["True", "False"]], "descrs": ["    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n    .. versionadded:: 0.22\n", "    The function to measure the quality of a split. Supported criteria\n    are 'friedman_mse' for the mean squared error with improvement\n    score by Friedman, 'mse' for mean squared error, and 'mae' for\n    the mean absolute error. The default value of 'friedman_mse' is\n    generally the best as it can provide a better approximation in\n    some cases.\n    .. versionadded:: 0.18\n", "    An estimator object that is used to compute the initial predictions.\n    ``init`` has to provide :meth:`fit` and :meth:`predict_proba`. If\n    'zero', the initial raw predictions are set to zero. By default, a\n    ``DummyEstimator`` predicting the classes priors is used.\n", "    learning rate shrinks the contribution of each tree by `learning_rate`.\n    There is a trade-off between learning_rate and n_estimators.\n", "    loss function to be optimized. 'deviance' refers to\n    deviance (= logistic regression) for classification\n    with probabilistic outputs. For loss 'exponential' gradient\n    boosting recovers the AdaBoost algorithm.\n", "    maximum depth of the individual regression estimators. The maximum\n    depth limits the number of nodes in the tree. Tune this parameter\n    for best performance; the best value depends on the interaction\n    of the input variables.\n", "    The number of features to consider when looking for the best split:\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `int(max_features * n_features)` features are considered at each\n      split.\n    - If 'auto', then `max_features=sqrt(n_features)`.\n    - If 'sqrt', then `max_features=sqrt(n_features)`.\n    - If 'log2', then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n    Choosing `max_features < n_features` leads to a reduction of variance\n    and an increase in bias.\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n", "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n", "    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n    The weighted impurity decrease equation is the following::\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n    .. versionadded:: 0.19\n", "    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n", "    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum number of samples required to split an internal node:\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n", "    The number of boosting stages to perform. Gradient boosting\n    is fairly robust to over-fitting so a large number usually\n    results in better performance.\n", "    ``n_iter_no_change`` is used to decide if early stopping will be used\n    to terminate training when validation score is not improving. By\n    default it is set to None to disable early stopping. If set to a\n    number, it will set aside ``validation_fraction`` size of the training\n    data as validation and terminate training when validation score is not\n    improving in all of the previous ``n_iter_no_change`` numbers of\n    iterations. The split is stratified.\n    .. versionadded:: 0.20\n", "    This parameter is deprecated and will be removed in v0.24.\n    .. deprecated :: 0.22\n", "    Controls the random seed given to each Tree estimator at each\n    boosting iteration.\n    In addition, it controls the random permutation of the features at\n    each split (see Notes for more details).\n    It also controls the random spliting of the training data to obtain a\n    validation set if `n_iter_no_change` is not None.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    The fraction of samples to be used for fitting the individual base\n    learners. If smaller than 1.0 this results in Stochastic Gradient\n    Boosting. `subsample` interacts with the parameter `n_estimators`.\n    Choosing `subsample < 1.0` leads to a reduction of variance\n    and an increase in bias.\n", "    Tolerance for the early stopping. When the loss is not improving\n    by at least tol for ``n_iter_no_change`` iterations (if set to a\n    number), the training stops.\n    .. versionadded:: 0.20\n", "    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if ``n_iter_no_change`` is set to an integer.\n    .. versionadded:: 0.20\n", "    Enable verbose output. If 1 then it prints progress and performance\n    once in a while (the more trees the lower the frequency). If greater\n    than 1 then it prints progress and performance for every tree.\n", "    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just erase the\n    previous solution. See :term:`the Glossary <warm_start>`.\n"]}, "gp": {"docs_name": "sklearn.gaussian_process.GaussianProcessClassifier", "preset_params": ["base gp classifier"], "param_names": ["copy_X_train", "kernel", "max_iter_predict", "multi_class", "n_jobs", "n_restarts_optimizer", "optimizer", "random_state", "warm_start"], "default_param_types": ["bool", "NoneType", "int", "str", "NoneType", "int", "str", "NoneType", "bool"], "default_params": ["True", "None", "100", "'one_vs_rest'", "None", "0", "'fmin_l_bfgs_b'", "None", "False"], "options": [["True", "False"], "", "", ["'one_vs_rest'", "'one_vs_one'"], "", "", "", "", ["True", "False"]], "descrs": ["    If True, a persistent copy of the training data is stored in the\n    object. Otherwise, just a reference to the training data is stored,\n    which might cause predictions to change if the data is modified\n    externally.\n", "    The kernel specifying the covariance function of the GP. If None is\n    passed, the kernel \"1.0 * RBF(1.0)\" is used as default. Note that\n    the kernel's hyperparameters are optimized during fitting.\n", "    The maximum number of iterations in Newton's method for approximating\n    the posterior during predict. Smaller values will reduce computation\n    time at the cost of worse results.\n", "    Specifies how multi-class classification problems are handled.\n    Supported are 'one_vs_rest' and 'one_vs_one'. In 'one_vs_rest',\n    one binary Gaussian process classifier is fitted for each class, which\n    is trained to separate this class from the rest. In 'one_vs_one', one\n    binary Gaussian process classifier is fitted for each pair of classes,\n    which is trained to separate these two classes. The predictions of\n    these binary predictors are combined into multi-class predictions.\n    Note that 'one_vs_one' does not support predicting probability\n    estimates.\n", "    The number of jobs to use for the computation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n", "    The number of restarts of the optimizer for finding the kernel's\n    parameters which maximize the log-marginal likelihood. The first run\n    of the optimizer is performed from the kernel's initial parameters,\n    the remaining ones (if any) from thetas sampled log-uniform randomly\n    from the space of allowed theta-values. If greater than 0, all bounds\n    must be finite. Note that n_restarts_optimizer=0 implies that one\n    run is performed.\n", "    Can either be one of the internally supported optimizers for optimizing\n    the kernel's parameters, specified by a string, or an externally\n    defined optimizer passed as a callable. If a callable is passed, it\n    must have the  signature::\n        def optimizer(obj_func, initial_theta, bounds):\n            # * 'obj_func' is the objective function to be maximized, which\n            #   takes the hyperparameters theta as parameter and an\n            #   optional flag eval_gradient, which determines if the\n            #   gradient is returned additionally to the function value\n            # * 'initial_theta': the initial value for theta, which can be\n            #   used by local optimizers\n            # * 'bounds': the bounds on the values of theta\n            ....\n            # Returned are the best found hyperparameters theta and\n            # the corresponding value of the target function.\n            return theta_opt, func_min\n    Per default, the 'L-BFGS-B' algorithm from scipy.optimize.minimize\n    is used. If None is passed, the kernel's parameters are kept fixed.\n    Available internal optimizers are::\n        'fmin_l_bfgs_b'\n", "    Determines random number generation used to initialize the centers.\n    Pass an int for reproducible results across multiple function calls.\n    See :term: `Glossary <random_state>`.\n", "    If warm-starts are enabled, the solution of the last Newton iteration\n    on the Laplace approximation of the posterior mode is used as\n    initialization for the next call of _posterior_mode(). This can speed\n    up convergence when _posterior_mode is called several times on similar\n    problems as in hyperparameter optimization. See :term:`the Glossary\n    <warm_start>`.\n"]}, "hgb": {"docs_name": "sklearn.ensemble.gradient_boosting.HistGradientBoostingClassifier", "preset_params": ["default"], "param_names": ["early_stopping", "l2_regularization", "learning_rate", "loss", "max_bins", "max_depth", "max_iter", "max_leaf_nodes", "min_samples_leaf", "monotonic_cst", "n_iter_no_change", "random_state", "scoring", "tol", "validation_fraction", "verbose", "warm_start"], "default_param_types": ["str", "float", "float", "str", "int", "NoneType", "int", "int", "int", "NoneType", "int", "NoneType", "str", "float", "float", "int", "bool"], "default_params": ["'auto'", "0.0", "0.1", "'auto'", "255", "None", "100", "31", "20", "None", "10", "None", "'loss'", "1e-07", "0.1", "0", "False"], "options": ["", "", "", ["'auto'", "'binary_crossentropy'", "'categorical_crossentropy'"], "", "", "", "", "", "", "", "", "", "", "", "", ["True", "False"]], "descrs": ["    If 'auto', early stopping is enabled if the sample size is larger than\n    10000. If True, early stopping is enabled, otherwise early stopping is\n    disabled.\n", "    The L2 regularization parameter. Use 0 for no regularization.\n", "    The learning rate, also known as *shrinkage*. This is used as a\n    multiplicative factor for the leaves values. Use ``1`` for no\n    shrinkage.\n", "    The loss function to use in the boosting process. 'binary_crossentropy'\n    (also known as logistic loss) is used for binary classification and\n    generalizes to 'categorical_crossentropy' for multiclass\n    classification. 'auto' will automatically choose either loss depending\n    on the nature of the problem.\n", "    The maximum number of bins to use for non-missing values. Before\n    training, each feature of the input array `X` is binned into\n    integer-valued bins, which allows for a much faster training stage.\n    Features with a small number of unique values may use less than\n    ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\n    is always reserved for missing values. Must be no larger than 255.\n", "    The maximum depth of each tree. The depth of a tree is the number of\n    edges to go from the root to the deepest leaf.\n    Depth isn't constrained by default.\n", "    The maximum number of iterations of the boosting process, i.e. the\n    maximum number of trees for binary classification. For multiclass\n    classification, `n_classes` trees per iteration are built.\n", "    The maximum number of leaves for each tree. Must be strictly greater\n    than 1. If None, there is no maximum limit.\n", "    The minimum number of samples per leaf. For small datasets with less\n    than a few hundred samples, it is recommended to lower this value\n    since only very shallow trees would be built.\n", "    Indicates the monotonic constraint to enforce on each feature. -1, 1\n    and 0 respectively correspond to a positive constraint, negative\n    constraint and no constraint. Read more in the :ref:`User Guide\n    <monotonic_cst_gbdt>`.\n", "    Used to determine when to \"early stop\". The fitting process is\n    stopped when none of the last ``n_iter_no_change`` scores are better\n    than the ``n_iter_no_change - 1`` -th-to-last one, up to some\n    tolerance. Only used if early stopping is performed.\n", "    Pseudo-random number generator to control the subsampling in the\n    binning process, and the train/validation data split if early stopping\n    is enabled.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    Scoring parameter to use for early stopping. It can be a single\n    string (see :ref:`scoring_parameter`) or a callable (see\n    :ref:`scoring`). If None, the estimator's default scorer\n    is used. If ``scoring='loss'``, early stopping is checked\n    w.r.t the loss value. Only used if early stopping is performed.\n", "    The absolute tolerance to use when comparing scores. The higher the\n    tolerance, the more likely we are to early stop: higher tolerance\n    means that it will be harder for subsequent iterations to be\n    considered an improvement upon the reference score.\n", "    Proportion (or absolute size) of training data to set aside as\n    validation data for early stopping. If None, early stopping is done on\n    the training data. Only used if early stopping is performed.\n", "    The verbosity level. If not zero, print some information about the\n    fitting process.\n", "    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble. For results to be valid, the\n    estimator should be re-trained on the same data only.\n    See :term:`the Glossary <warm_start>`.\n"]}, "knn": {"docs_name": "sklearn.neighbors.KNeighborsClassifier", "preset_params": ["base knn", "knn dist"], "param_names": ["algorithm", "leaf_size", "metric", "metric_params", "n_jobs", "n_neighbors", "p", "weights"], "default_param_types": ["str", "int", "str", "NoneType", "NoneType", "int", "int", "str"], "default_params": ["'auto'", "30", "'minkowski'", "None", "None", "5", "2", "'uniform'"], "options": [["'auto'", "'ball_tree'", "'kd_tree'", "'brute'"], "", "", "", "", "", "", ["'uniform'", "'distance'"]], "descrs": ["    Algorithm used to compute the nearest neighbors:\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n", "    Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem.\n", "    the distance metric to use for the tree.  The default metric is\n    minkowski, and with p=2 is equivalent to the standard Euclidean\n    metric. See the documentation of :class:`DistanceMetric` for a\n    list of available metrics.\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`,\n    in which case only \"nonzero\" elements may be considered neighbors.\n", "    Additional keyword arguments for the metric function.\n", "    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n    Doesn't affect :meth:`fit` method.\n", "    Number of neighbors to use by default for :meth:`kneighbors` queries.\n", "    Power parameter for the Minkowski metric. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n", "    weight function used in prediction.  Possible values:\n    - 'uniform' : uniform weights.  All points in each neighborhood\n      are weighted equally.\n    - 'distance' : weight points by the inverse of their distance.\n      in this case, closer neighbors of a query point will have a\n      greater influence than neighbors which are further away.\n    - [callable] : a user-defined function which accepts an\n      array of distances, and returns an array of the same shape\n      containing the weights.\n"]}, "lasso": {"docs_name": "sklearn.linear_model.LogisticRegression", "preset_params": ["base lasso", "lasso C", "lasso C extra"], "param_names": ["C", "class_weight", "dual", "fit_intercept", "intercept_scaling", "l1_ratio", "max_iter", "multi_class", "n_jobs", "penalty", "random_state", "solver", "tol", "verbose", "warm_start"], "default_param_types": ["float", "NoneType", "bool", "bool", "int", "NoneType", "int", "str", "NoneType", "str", "NoneType", "str", "float", "int", "bool"], "default_params": ["1.0", "None", "False", "True", "1", "None", "100", "'auto'", "None", "'l2'", "None", "'lbfgs'", "0.0001", "0", "False"], "options": ["", ["None", "balanced"], ["True", "False"], ["True", "False"], "", "", "", ["'auto'", "'ovr'", "'multinomial'"], "", ["'l1'", "'l2'", "'elasticnet'", "'none'"], "", ["'newton-cg'", "'lbfgs'", "'liblinear'", "'sag'", "'saga'"], "", "", ["True", "False"]], "descrs": ["    Inverse of regularization strength; must be a positive float.\n    Like in support vector machines, smaller values specify stronger\n    regularization.\n", "    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one.\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n    .. versionadded:: 0.17\n       *class_weight='balanced'*\n", "    Dual or primal formulation. Dual formulation is only implemented for\n    l2 penalty with liblinear solver. Prefer dual=False when\n    n_samples > n_features.\n", "    Specifies if a constant (a.k.a. bias or intercept) should be\n    added to the decision function.\n", "    Useful only when the solver 'liblinear' is used\n    and self.fit_intercept is set to True. In this case, x becomes\n    [x, self.intercept_scaling],\n    i.e. a \"synthetic\" feature with constant value equal to\n    intercept_scaling is appended to the instance vector.\n    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n    Note! the synthetic feature weight is subject to l1/l2 regularization\n    as all other features.\n    To lessen the effect of regularization on synthetic feature weight\n    (and therefore on the intercept) intercept_scaling has to be increased.\n", "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n    combination of L1 and L2.\n", "    Maximum number of iterations taken for the solvers to converge.\n", "    If the option chosen is 'ovr', then a binary problem is fit for each\n    label. For 'multinomial' the loss minimised is the multinomial loss fit\n    across the entire probability distribution, *even when the data is\n    binary*. 'multinomial' is unavailable when solver='liblinear'.\n    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n    and otherwise selects 'multinomial'.\n    .. versionadded:: 0.18\n       Stochastic Average Gradient descent solver for 'multinomial' case.\n    .. versionchanged:: 0.22\n        Default changed from 'ovr' to 'auto' in 0.22.\n", "    Number of CPU cores used when parallelizing over classes if\n    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n    set to 'liblinear' regardless of whether 'multi_class' is specified or\n    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors.\n    See :term:`Glossary <n_jobs>` for more details.\n", "    Used to specify the norm used in the penalization. The 'newton-cg',\n    'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n    only supported by the 'saga' solver. If 'none' (not supported by the\n    liblinear solver), no regularization is applied.\n    .. versionadded:: 0.19\n       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n", "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n    data. See :term:`Glossary <random_state>` for details.\n", "    Algorithm to use in the optimization problem.\n    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n      'saga' are faster for large ones.\n    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n      schemes.\n    - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n    - 'liblinear' and 'saga' also handle L1 penalty\n    - 'saga' also supports 'elasticnet' penalty\n    - 'liblinear' does not support setting ``penalty='none'``\n    Note that 'sag' and 'saga' fast convergence is only guaranteed on\n    features with approximately the same scale. You can\n    preprocess the data with a scaler from sklearn.preprocessing.\n    .. versionadded:: 0.17\n       Stochastic Average Gradient descent solver.\n    .. versionadded:: 0.19\n       SAGA solver.\n    .. versionchanged:: 0.22\n        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n", "    Tolerance for stopping criteria.\n", "    For the liblinear and lbfgs solvers set verbose to any positive\n    number for verbosity.\n", "    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n    .. versionadded:: 0.17\n       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n"]}, "linear svm": {"docs_name": "sklearn.svm.LinearSVC", "preset_params": ["base linear svc", "linear svc dist"], "param_names": ["C", "class_weight", "dual", "fit_intercept", "intercept_scaling", "loss", "max_iter", "multi_class", "penalty", "random_state", "tol", "verbose"], "default_param_types": ["float", "NoneType", "bool", "bool", "int", "str", "int", "str", "str", "NoneType", "float", "int"], "default_params": ["1.0", "None", "True", "True", "1", "'squared_hinge'", "1000", "'ovr'", "'l2'", "None", "0.0001", "0"], "options": ["", ["None", "balanced"], ["True", "False"], ["True", "False"], "", ["'hinge'", "'squared_hinge'"], "", ["'ovr'", "'crammer_singer'"], ["'l1'", "'l2'"], "", "", ""], "descrs": ["    Regularization parameter. The strength of the regularization is\n    inversely proportional to C. Must be strictly positive.\n", "    Set the parameter C of class i to ``class_weight[i]*C`` for\n    SVC. If not given, all classes are supposed to have\n    weight one.\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n", "    Select the algorithm to either solve the dual or primal\n    optimization problem. Prefer dual=False when n_samples > n_features.\n", "    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be already centered).\n", "    When self.fit_intercept is True, instance vector x becomes\n    ``[x, self.intercept_scaling]``,\n    i.e. a \"synthetic\" feature with constant value equals to\n    intercept_scaling is appended to the instance vector.\n    The intercept becomes intercept_scaling * synthetic feature weight\n    Note! the synthetic feature weight is subject to l1/l2 regularization\n    as all other features.\n    To lessen the effect of regularization on synthetic feature weight\n    (and therefore on the intercept) intercept_scaling has to be increased.\n", "    Specifies the loss function. 'hinge' is the standard SVM loss\n    (used e.g. by the SVC class) while 'squared_hinge' is the\n    square of the hinge loss.\n", "    The maximum number of iterations to be run.\n", "    Determines the multi-class strategy if `y` contains more than\n    two classes.\n    ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while\n    ``\"crammer_singer\"`` optimizes a joint objective over all classes.\n    While `crammer_singer` is interesting from a theoretical perspective\n    as it is consistent, it is seldom used in practice as it rarely leads\n    to better accuracy and is more expensive to compute.\n    If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual\n    will be ignored.\n", "    Specifies the norm used in the penalization. The 'l2'\n    penalty is the standard used in SVC. The 'l1' leads to ``coef_``\n    vectors that are sparse.\n", "    Controls the pseudo random number generation for shuffling the data for\n    the dual coordinate descent (if ``dual=True``). When ``dual=False`` the\n    underlying implementation of :class:`LinearSVC` is not random and\n    ``random_state`` has no effect on the results.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    Tolerance for stopping criteria.\n", "    Enable verbose output. Note that this setting takes advantage of a\n    per-process runtime setting in liblinear that, if enabled, may not work\n    properly in a multithreaded context.\n"]}, "logistic": {"docs_name": "sklearn.linear_model.LogisticRegression", "preset_params": ["base logistic"], "param_names": ["C", "class_weight", "dual", "fit_intercept", "intercept_scaling", "l1_ratio", "max_iter", "multi_class", "n_jobs", "penalty", "random_state", "solver", "tol", "verbose", "warm_start"], "default_param_types": ["float", "NoneType", "bool", "bool", "int", "NoneType", "int", "str", "NoneType", "str", "NoneType", "str", "float", "int", "bool"], "default_params": ["1.0", "None", "False", "True", "1", "None", "100", "'auto'", "None", "'l2'", "None", "'lbfgs'", "0.0001", "0", "False"], "options": ["", ["None", "balanced"], ["True", "False"], ["True", "False"], "", "", "", ["'auto'", "'ovr'", "'multinomial'"], "", ["'l1'", "'l2'", "'elasticnet'", "'none'"], "", ["'newton-cg'", "'lbfgs'", "'liblinear'", "'sag'", "'saga'"], "", "", ["True", "False"]], "descrs": ["    Inverse of regularization strength; must be a positive float.\n    Like in support vector machines, smaller values specify stronger\n    regularization.\n", "    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one.\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n    .. versionadded:: 0.17\n       *class_weight='balanced'*\n", "    Dual or primal formulation. Dual formulation is only implemented for\n    l2 penalty with liblinear solver. Prefer dual=False when\n    n_samples > n_features.\n", "    Specifies if a constant (a.k.a. bias or intercept) should be\n    added to the decision function.\n", "    Useful only when the solver 'liblinear' is used\n    and self.fit_intercept is set to True. In this case, x becomes\n    [x, self.intercept_scaling],\n    i.e. a \"synthetic\" feature with constant value equal to\n    intercept_scaling is appended to the instance vector.\n    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n    Note! the synthetic feature weight is subject to l1/l2 regularization\n    as all other features.\n    To lessen the effect of regularization on synthetic feature weight\n    (and therefore on the intercept) intercept_scaling has to be increased.\n", "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n    combination of L1 and L2.\n", "    Maximum number of iterations taken for the solvers to converge.\n", "    If the option chosen is 'ovr', then a binary problem is fit for each\n    label. For 'multinomial' the loss minimised is the multinomial loss fit\n    across the entire probability distribution, *even when the data is\n    binary*. 'multinomial' is unavailable when solver='liblinear'.\n    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n    and otherwise selects 'multinomial'.\n    .. versionadded:: 0.18\n       Stochastic Average Gradient descent solver for 'multinomial' case.\n    .. versionchanged:: 0.22\n        Default changed from 'ovr' to 'auto' in 0.22.\n", "    Number of CPU cores used when parallelizing over classes if\n    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n    set to 'liblinear' regardless of whether 'multi_class' is specified or\n    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors.\n    See :term:`Glossary <n_jobs>` for more details.\n", "    Used to specify the norm used in the penalization. The 'newton-cg',\n    'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n    only supported by the 'saga' solver. If 'none' (not supported by the\n    liblinear solver), no regularization is applied.\n    .. versionadded:: 0.19\n       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n", "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n    data. See :term:`Glossary <random_state>` for details.\n", "    Algorithm to use in the optimization problem.\n    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n      'saga' are faster for large ones.\n    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n      schemes.\n    - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n    - 'liblinear' and 'saga' also handle L1 penalty\n    - 'saga' also supports 'elasticnet' penalty\n    - 'liblinear' does not support setting ``penalty='none'``\n    Note that 'sag' and 'saga' fast convergence is only guaranteed on\n    features with approximately the same scale. You can\n    preprocess the data with a scaler from sklearn.preprocessing.\n    .. versionadded:: 0.17\n       Stochastic Average Gradient descent solver.\n    .. versionadded:: 0.19\n       SAGA solver.\n    .. versionchanged:: 0.22\n        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n", "    Tolerance for stopping criteria.\n", "    For the liblinear and lbfgs solvers set verbose to any positive\n    number for verbosity.\n", "    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n    .. versionadded:: 0.17\n       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n"]}, "mlp": {"docs_name": "ABCD_ML.extensions.MLP.MLPClassifier_Wrapper", "preset_params": ["default", "mlp dist 3 layer", "mlp dist es 3 layer", "mlp dist 2 layer", "mlp dist es 2 layer", "mlp dist 1 layer", "mlp dist es 1 layer"], "param_names": ["activation", "alpha", "batch_size", "beta_1", "beta_2", "early_stopping", "epsilon", "hidden_layer_sizes", "learning_rate", "learning_rate_init", "max_fun", "max_iter", "momentum", "n_iter_no_change", "nesterovs_momentum", "power_t", "random_state", "shuffle", "solver", "tol", "validation_fraction", "verbose", "warm_start"], "default_param_types": ["str", "float", "str", "float", "float", "bool", "float", "tuple", "str", "float", "int", "int", "float", "int", "bool", "float", "NoneType", "bool", "str", "float", "float", "bool", "bool"], "default_params": ["'relu'", "0.0001", "'auto'", "0.9", "0.999", "False", "1e-08", "(100,)", "'constant'", "0.001", "15000", "200", "0.9", "10", "True", "0.5", "None", "True", "'adam'", "0.0001", "0.1", "False", "False"], "options": [["'identity'", "'logistic'", "'tanh'", "'relu'"], "", "", "", "", ["True", "False"], "", "", ["'constant'", "'invscaling'", "'adaptive'"], "", "", "", "", "", ["True", "False"], "", "", ["True", "False"], ["'lbfgs'", "'sgd'", "'adam'"], "", "", ["True", "False"], ["True", "False"]], "descrs": ["    Activation function for the hidden layer.\n    - 'identity', no-op activation, useful to implement linear bottleneck,\n      returns f(x) = x\n    - 'logistic', the logistic sigmoid function,\n      returns f(x) = 1 / (1 + exp(-x)).\n    - 'tanh', the hyperbolic tan function,\n      returns f(x) = tanh(x).\n    - 'relu', the rectified linear unit function,\n      returns f(x) = max(0, x)\n", "    L2 penalty (regularization term) parameter.\n", "    Size of minibatches for stochastic optimizers.\n    If the solver is 'lbfgs', the classifier will not use minibatch.\n    When set to \"auto\", `batch_size=min(200, n_samples)`\n", "    Exponential decay rate for estimates of first moment vector in adam,\n    should be in [0, 1). Only used when solver='adam'\n", "    Exponential decay rate for estimates of second moment vector in adam,\n    should be in [0, 1). Only used when solver='adam'\n", "    Whether to use early stopping to terminate training when validation\n    score is not improving. If set to true, it will automatically set\n    aside 10% of training data as validation and terminate training when\n    validation score is not improving by at least tol for\n    ``n_iter_no_change`` consecutive epochs. The split is stratified,\n    except in a multilabel setting.\n    Only effective when solver='sgd' or 'adam'\n", "    Value for numerical stability in adam. Only used when solver='adam'\n", "    The ith element represents the number of neurons in the ith\n    hidden layer.\n", "    Learning rate schedule for weight updates.\n    - 'constant' is a constant learning rate given by\n      'learning_rate_init'.\n    - 'invscaling' gradually decreases the learning rate at each\n      time step 't' using an inverse scaling exponent of 'power_t'.\n      effective_learning_rate = learning_rate_init / pow(t, power_t)\n    - 'adaptive' keeps the learning rate constant to\n      'learning_rate_init' as long as training loss keeps decreasing.\n      Each time two consecutive epochs fail to decrease training loss by at\n      least tol, or fail to increase validation score by at least tol if\n      'early_stopping' is on, the current learning rate is divided by 5.\n    Only used when ``solver='sgd'``.\n", "    The initial learning rate used. It controls the step-size\n    in updating the weights. Only used when solver='sgd' or 'adam'.\n", "    Only used when solver='lbfgs'. Maximum number of loss function calls.\n    The solver iterates until convergence (determined by 'tol'), number\n    of iterations reaches max_iter, or this number of loss function calls.\n    Note that number of loss function calls will be greater than or equal\n    to the number of iterations for the `MLPClassifier`.\n    .. versionadded:: 0.22\n", "    Maximum number of iterations. The solver iterates until convergence\n    (determined by 'tol') or this number of iterations. For stochastic\n    solvers ('sgd', 'adam'), note that this determines the number of epochs\n    (how many times each data point will be used), not the number of\n    gradient steps.\n", "    Momentum for gradient descent update. Should be between 0 and 1. Only\n    used when solver='sgd'.\n", "    Maximum number of epochs to not meet ``tol`` improvement.\n    Only effective when solver='sgd' or 'adam'\n    .. versionadded:: 0.20\n", "    Whether to use Nesterov's momentum. Only used when solver='sgd' and\n    momentum > 0.\n", "    The exponent for inverse scaling learning rate.\n    It is used in updating effective learning rate when the learning_rate\n    is set to 'invscaling'. Only used when solver='sgd'.\n", "    Determines random number generation for weights and bias\n    initialization, train-test split if early stopping is used, and batch\n    sampling when solver='sgd' or 'adam'.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    Whether to shuffle samples in each iteration. Only used when\n    solver='sgd' or 'adam'.\n", "    The solver for weight optimization.\n    - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n    - 'sgd' refers to stochastic gradient descent.\n    - 'adam' refers to a stochastic gradient-based optimizer proposed\n      by Kingma, Diederik, and Jimmy Ba\n    Note: The default solver 'adam' works pretty well on relatively\n    large datasets (with thousands of training samples or more) in terms of\n    both training time and validation score.\n    For small datasets, however, 'lbfgs' can converge faster and perform\n    better.\n", "    Tolerance for the optimization. When the loss or score is not improving\n    by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\n    unless ``learning_rate`` is set to 'adaptive', convergence is\n    considered to be reached and training stops.\n", "    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if early_stopping is True\n", "    Whether to print progress messages to stdout.\n", "    When set to True, reuse the solution of the previous\n    call to fit as initialization, otherwise, just erase the\n    previous solution. See :term:`the Glossary <warm_start>`.\n"]}, "pa": {"docs_name": "sklearn.linear_model.PassiveAggressiveClassifier", "preset_params": ["default"], "param_names": ["C", "average", "class_weight", "early_stopping", "fit_intercept", "loss", "max_iter", "n_iter_no_change", "n_jobs", "random_state", "shuffle", "tol", "validation_fraction", "verbose", "warm_start"], "default_param_types": ["float", "bool", "NoneType", "bool", "bool", "str", "int", "int", "NoneType", "NoneType", "bool", "float", "float", "int", "bool"], "default_params": ["1.0", "False", "None", "False", "True", "'hinge'", "1000", "5", "None", "None", "True", "0.001", "0.1", "0", "False"], "options": ["", ["True", "False"], ["None", "balanced"], ["True", "False"], ["True", "False"], "", "", "", "", "", ["True", "False"], "", "", "", ["True", "False"]], "descrs": ["    Maximum step size (regularization). Defaults to 1.0.\n", "    When set to True, computes the averaged SGD weights and stores the\n    result in the ``coef_`` attribute. If set to an int greater than 1,\n    averaging will begin once the total number of samples seen reaches\n    average. So average=10 will begin averaging after seeing 10 samples.\n    .. versionadded:: 0.19\n       parameter *average* to use weights averaging in SGD\n", "    Preset for the class_weight fit parameter.\n    Weights associated with classes. If not given, all classes\n    are supposed to have weight one.\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n    .. versionadded:: 0.17\n       parameter *class_weight* to automatically weight samples.\n", "    Whether to use early stopping to terminate training when validation.\n    score is not improving. If set to True, it will automatically set aside\n    a stratified fraction of training data as validation and terminate\n    training when validation score is not improving by at least tol for\n    n_iter_no_change consecutive epochs.\n    .. versionadded:: 0.20\n", "    Whether the intercept should be estimated or not. If False, the\n    data is assumed to be already centered.\n", "    The loss function to be used:\n    hinge: equivalent to PA-I in the reference paper.\n    squared_hinge: equivalent to PA-II in the reference paper.\n", "    The maximum number of passes over the training data (aka epochs).\n    It only impacts the behavior in the ``fit`` method, and not the\n    :meth:`partial_fit` method.\n    .. versionadded:: 0.19\n", "    Number of iterations with no improvement to wait before early stopping.\n    .. versionadded:: 0.20\n", "    The number of CPUs to use to do the OVA (One Versus All, for\n    multi-class problems) computation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n", "    Used to shuffle the training data, when ``shuffle`` is set to\n    ``True``. Pass an int for reproducible output across multiple\n    function calls.\n    See :term:`Glossary <random_state>`.\n", "    Whether or not the training data should be shuffled after each epoch.\n", "    The stopping criterion. If it is not None, the iterations will stop\n    when (loss > previous_loss - tol).\n    .. versionadded:: 0.19\n", "    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if early_stopping is True.\n    .. versionadded:: 0.20\n", "    The verbosity level\n", "    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    See :term:`the Glossary <warm_start>`.\n    Repeatedly calling fit or partial_fit when warm_start is True can\n    result in a different solution than when calling fit a single time\n    because of the way the data is shuffled.\n"]}, "random forest": {"docs_name": "sklearn.ensemble.RandomForestClassifier", "preset_params": ["base rf regressor", "rf classifier dist"], "param_names": ["bootstrap", "ccp_alpha", "class_weight", "criterion", "max_depth", "max_features", "max_leaf_nodes", "max_samples", "min_impurity_decrease", "min_impurity_split", "min_samples_leaf", "min_samples_split", "min_weight_fraction_leaf", "n_estimators", "n_jobs", "oob_score", "random_state", "verbose", "warm_start"], "default_param_types": ["bool", "float", "NoneType", "str", "NoneType", "str", "NoneType", "NoneType", "float", "NoneType", "int", "int", "float", "int", "NoneType", "bool", "NoneType", "int", "bool"], "default_params": ["True", "0.0", "None", "'gini'", "None", "'auto'", "None", "None", "0.0", "None", "1", "2", "0.0", "100", "None", "False", "None", "0", "False"], "options": [["True", "False"], "", ["None", "balanced"], ["\"gini\"", "\"entropy\""], "", ["\"auto\"", "\"sqrt\"", "\"log2\""], "", "", "", "", "", "", "", "", "", ["True", "False"], "", "", ["True", "False"]], "descrs": ["    Whether bootstrap samples are used when building trees. If False, the\n    whole dataset is used to build each tree.\n", "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n    .. versionadded:: 0.22\n", "    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n    The \"balanced_subsample\" mode is the same as \"balanced\" except that\n    weights are computed based on the bootstrap sample for every tree\n    grown.\n    For multi-output, the weights of each column of y will be multiplied.\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n", "    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n    Note: this parameter is tree-specific.\n", "    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n", "    The number of features to consider when looking for the best split:\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `int(max_features * n_features)` features are considered at each\n      split.\n    - If \"auto\", then `max_features=sqrt(n_features)`.\n    - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n", "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n", "    If bootstrap is True, the number of samples to draw from X\n    to train each base estimator.\n    - If None (default), then draw `X.shape[0]` samples.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n      `max_samples` should be in the interval `(0, 1)`.\n    .. versionadded:: 0.22\n", "    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n    The weighted impurity decrease equation is the following::\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n    .. versionadded:: 0.19\n", "    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n", "    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum number of samples required to split an internal node:\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n", "    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n", "    The number of trees in the forest.\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n", "    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    <n_jobs>` for more details.\n", "    Whether to use out-of-bag samples to estimate\n    the generalization accuracy.\n", "    Controls both the randomness of the bootstrapping of the samples used\n    when building trees (if ``bootstrap=True``) and the sampling of the\n    features to consider when looking for the best split at each node\n    (if ``max_features < n_features``).\n    See :term:`Glossary <random_state>` for details.\n", "    Controls the verbosity when fitting and predicting.\n", "    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`the Glossary <warm_start>`.\n"]}, "ridge": {"docs_name": "sklearn.linear_model.LogisticRegression", "preset_params": ["base ridge", "ridge C", "ridge C extra"], "param_names": ["C", "class_weight", "dual", "fit_intercept", "intercept_scaling", "l1_ratio", "max_iter", "multi_class", "n_jobs", "penalty", "random_state", "solver", "tol", "verbose", "warm_start"], "default_param_types": ["float", "NoneType", "bool", "bool", "int", "NoneType", "int", "str", "NoneType", "str", "NoneType", "str", "float", "int", "bool"], "default_params": ["1.0", "None", "False", "True", "1", "None", "100", "'auto'", "None", "'l2'", "None", "'lbfgs'", "0.0001", "0", "False"], "options": ["", ["None", "balanced"], ["True", "False"], ["True", "False"], "", "", "", ["'auto'", "'ovr'", "'multinomial'"], "", ["'l1'", "'l2'", "'elasticnet'", "'none'"], "", ["'newton-cg'", "'lbfgs'", "'liblinear'", "'sag'", "'saga'"], "", "", ["True", "False"]], "descrs": ["    Inverse of regularization strength; must be a positive float.\n    Like in support vector machines, smaller values specify stronger\n    regularization.\n", "    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one.\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n    .. versionadded:: 0.17\n       *class_weight='balanced'*\n", "    Dual or primal formulation. Dual formulation is only implemented for\n    l2 penalty with liblinear solver. Prefer dual=False when\n    n_samples > n_features.\n", "    Specifies if a constant (a.k.a. bias or intercept) should be\n    added to the decision function.\n", "    Useful only when the solver 'liblinear' is used\n    and self.fit_intercept is set to True. In this case, x becomes\n    [x, self.intercept_scaling],\n    i.e. a \"synthetic\" feature with constant value equal to\n    intercept_scaling is appended to the instance vector.\n    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n    Note! the synthetic feature weight is subject to l1/l2 regularization\n    as all other features.\n    To lessen the effect of regularization on synthetic feature weight\n    (and therefore on the intercept) intercept_scaling has to be increased.\n", "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n    combination of L1 and L2.\n", "    Maximum number of iterations taken for the solvers to converge.\n", "    If the option chosen is 'ovr', then a binary problem is fit for each\n    label. For 'multinomial' the loss minimised is the multinomial loss fit\n    across the entire probability distribution, *even when the data is\n    binary*. 'multinomial' is unavailable when solver='liblinear'.\n    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n    and otherwise selects 'multinomial'.\n    .. versionadded:: 0.18\n       Stochastic Average Gradient descent solver for 'multinomial' case.\n    .. versionchanged:: 0.22\n        Default changed from 'ovr' to 'auto' in 0.22.\n", "    Number of CPU cores used when parallelizing over classes if\n    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n    set to 'liblinear' regardless of whether 'multi_class' is specified or\n    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors.\n    See :term:`Glossary <n_jobs>` for more details.\n", "    Used to specify the norm used in the penalization. The 'newton-cg',\n    'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n    only supported by the 'saga' solver. If 'none' (not supported by the\n    liblinear solver), no regularization is applied.\n    .. versionadded:: 0.19\n       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n", "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n    data. See :term:`Glossary <random_state>` for details.\n", "    Algorithm to use in the optimization problem.\n    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n      'saga' are faster for large ones.\n    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n      schemes.\n    - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n    - 'liblinear' and 'saga' also handle L1 penalty\n    - 'saga' also supports 'elasticnet' penalty\n    - 'liblinear' does not support setting ``penalty='none'``\n    Note that 'sag' and 'saga' fast convergence is only guaranteed on\n    features with approximately the same scale. You can\n    preprocess the data with a scaler from sklearn.preprocessing.\n    .. versionadded:: 0.17\n       Stochastic Average Gradient descent solver.\n    .. versionadded:: 0.19\n       SAGA solver.\n    .. versionchanged:: 0.22\n        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n", "    Tolerance for stopping criteria.\n", "    For the liblinear and lbfgs solvers set verbose to any positive\n    number for verbosity.\n", "    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n    .. versionadded:: 0.17\n       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n"]}, "sgd": {"docs_name": "sklearn.linear_model.SGDClassifier", "preset_params": ["base sgd", "sgd classifier"], "param_names": ["alpha", "average", "class_weight", "early_stopping", "epsilon", "eta0", "fit_intercept", "l1_ratio", "learning_rate", "loss", "max_iter", "n_iter_no_change", "n_jobs", "penalty", "power_t", "random_state", "shuffle", "tol", "validation_fraction", "verbose", "warm_start"], "default_param_types": ["float", "bool", "NoneType", "bool", "float", "float", "bool", "float", "str", "str", "int", "int", "NoneType", "str", "float", "NoneType", "bool", "float", "float", "int", "bool"], "default_params": ["0.0001", "False", "None", "False", "0.1", "0.0", "True", "0.15", "'optimal'", "'hinge'", "1000", "5", "None", "'l2'", "0.5", "None", "True", "0.001", "0.1", "0", "False"], "options": ["", ["True", "False"], ["None", "balanced"], ["True", "False"], "", "", ["True", "False"], "", "", "", "", "", "", ["'l2'", "'l1'", "'elasticnet'"], "", "", ["True", "False"], "", "", "", ["True", "False"]], "descrs": ["    Constant that multiplies the regularization term. The higher the\n    value, the stronger the regularization.\n    Also used to compute the learning rate when set to `learning_rate` is\n    set to 'optimal'.\n", "    When set to True, computes the averaged SGD weights accross all\n    updates and stores the result in the ``coef_`` attribute. If set to\n    an int greater than 1, averaging will begin once the total number of\n    samples seen reaches `average`. So ``average=10`` will begin\n    averaging after seeing 10 samples.\n", "    Preset for the class_weight fit parameter.\n    Weights associated with classes. If not given, all classes\n    are supposed to have weight one.\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n", "    Whether to use early stopping to terminate training when validation\n    score is not improving. If set to True, it will automatically set aside\n    a stratified fraction of training data as validation and terminate\n    training when validation score returned by the `score` method is not\n    improving by at least tol for n_iter_no_change consecutive epochs.\n    .. versionadded:: 0.20\n        Added 'early_stopping' option\n", "    Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n    'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n    For 'huber', determines the threshold at which it becomes less\n    important to get the prediction exactly right.\n    For epsilon-insensitive, any differences between the current prediction\n    and the correct label are ignored if they are less than this threshold.\n", "    The initial learning rate for the 'constant', 'invscaling' or\n    'adaptive' schedules. The default value is 0.0 as eta0 is not used by\n    the default schedule 'optimal'.\n", "    Whether the intercept should be estimated or not. If False, the\n    data is assumed to be already centered.\n", "    The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n    l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n    Only used if `penalty` is 'elasticnet'.\n", "    The learning rate schedule:\n    - 'constant': `eta = eta0`\n    - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n      where t0 is chosen by a heuristic proposed by Leon Bottou.\n    - 'invscaling': `eta = eta0 / pow(t, power_t)`\n    - 'adaptive': eta = eta0, as long as the training keeps decreasing.\n      Each time n_iter_no_change consecutive epochs fail to decrease the\n      training loss by tol or fail to increase validation score by tol if\n      early_stopping is True, the current learning rate is divided by 5.\n        .. versionadded:: 0.20\n            Added 'adaptive' option\n", "    The loss function to be used. Defaults to 'hinge', which gives a\n    linear SVM.\n    The possible options are 'hinge', 'log', 'modified_huber',\n    'squared_hinge', 'perceptron', or a regression loss: 'squared_loss',\n    'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n    The 'log' loss gives logistic regression, a probabilistic classifier.\n    'modified_huber' is another smooth loss that brings tolerance to\n    outliers as well as probability estimates.\n    'squared_hinge' is like hinge but is quadratically penalized.\n    'perceptron' is the linear loss used by the perceptron algorithm.\n    The other losses are designed for regression but can be useful in\n    classification as well; see\n    :class:`~sklearn.linear_model.SGDRegressor` for a description.\n    More details about the losses formulas can be found in the\n    :ref:`User Guide <sgd_mathematical_formulation>`.\n", "    The maximum number of passes over the training data (aka epochs).\n    It only impacts the behavior in the ``fit`` method, and not the\n    :meth:`partial_fit` method.\n    .. versionadded:: 0.19\n", "    Number of iterations with no improvement to wait before early stopping.\n    .. versionadded:: 0.20\n        Added 'n_iter_no_change' option\n", "    The number of CPUs to use to do the OVA (One Versus All, for\n    multi-class problems) computation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n", "    The penalty (aka regularization term) to be used. Defaults to 'l2'\n    which is the standard regularizer for linear SVM models. 'l1' and\n    'elasticnet' might bring sparsity to the model (feature selection)\n    not achievable with 'l2'.\n", "    The exponent for inverse scaling learning rate [default 0.5].\n", "    Used for shuffling the data, when ``shuffle`` is set to ``True``.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    Whether or not the training data should be shuffled after each epoch.\n", "    The stopping criterion. If it is not None, training will stop\n    when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n    epochs.\n    .. versionadded:: 0.19\n", "    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if `early_stopping` is True.\n    .. versionadded:: 0.20\n        Added 'validation_fraction' option\n", "    The verbosity level.\n", "    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    See :term:`the Glossary <warm_start>`.\n    Repeatedly calling fit or partial_fit when warm_start is True can\n    result in a different solution than when calling fit a single time\n    because of the way the data is shuffled.\n    If a dynamic learning rate is used, the learning rate is adapted\n    depending on the number of samples already seen. Calling ``fit`` resets\n    this counter, while ``partial_fit`` will result in increasing the\n    existing counter.\n"]}, "svm": {"docs_name": "sklearn.svm.SVC", "preset_params": ["base svm classifier", "svm classifier dist"], "param_names": ["C", "break_ties", "cache_size", "class_weight", "coef0", "decision_function_shape", "degree", "gamma", "kernel", "max_iter", "probability", "random_state", "shrinking", "tol", "verbose"], "default_param_types": ["float", "bool", "int", "NoneType", "float", "str", "int", "str", "str", "int", "bool", "NoneType", "bool", "float", "bool"], "default_params": ["1.0", "False", "200", "None", "0.0", "'ovr'", "3", "'scale'", "'rbf'", "-1", "False", "None", "True", "0.001", "False"], "options": ["", ["True", "False"], "", ["None", "balanced"], "", ["'ovo'", "'ovr'"], "", ["'scale'", "'auto'"], ["'linear'", "'poly'", "'rbf'", "'sigmoid'", "'precomputed'"], "", ["True", "False"], "", ["True", "False"], "", ["True", "False"]], "descrs": ["    Regularization parameter. The strength of the regularization is\n    inversely proportional to C. Must be strictly positive. The penalty\n    is a squared l2 penalty.\n", "    If true, ``decision_function_shape='ovr'``, and number of classes > 2,\n    :term:`predict` will break ties according to the confidence values of\n    :term:`decision_function`; otherwise the first class among the tied\n    classes is returned. Please note that breaking ties comes at a\n    relatively high computational cost compared to a simple predict.\n    .. versionadded:: 0.22\n", "    Specify the size of the kernel cache (in MB).\n", "    Set the parameter C of class i to class_weight[i]*C for\n    SVC. If not given, all classes are supposed to have\n    weight one.\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n", "    Independent term in kernel function.\n    It is only significant in 'poly' and 'sigmoid'.\n", "    Whether to return a one-vs-rest ('ovr') decision function of shape\n    (n_samples, n_classes) as all other classifiers, or the original\n    one-vs-one ('ovo') decision function of libsvm which has shape\n    (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one\n    ('ovo') is always used as multi-class strategy. The parameter is\n    ignored for binary classification.\n    .. versionchanged:: 0.19\n        decision_function_shape is 'ovr' by default.\n    .. versionadded:: 0.17\n       *decision_function_shape='ovr'* is recommended.\n    .. versionchanged:: 0.17\n       Deprecated *decision_function_shape='ovo' and None*.\n", "    Degree of the polynomial kernel function ('poly').\n    Ignored by all other kernels.\n", "    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n    - if ``gamma='scale'`` (default) is passed then it uses\n      1 / (n_features * X.var()) as value of gamma,\n    - if 'auto', uses 1 / n_features.\n    .. versionchanged:: 0.22\n       The default value of ``gamma`` changed from 'auto' to 'scale'.\n", "    Specifies the kernel type to be used in the algorithm.\n    It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n    a callable.\n    If none is given, 'rbf' will be used. If a callable is given it is\n    used to pre-compute the kernel matrix from data matrices; that matrix\n    should be an array of shape ``(n_samples, n_samples)``.\n", "    Hard limit on iterations within solver, or -1 for no limit.\n", "    Whether to enable probability estimates. This must be enabled prior\n    to calling `fit`, will slow down that method as it internally uses\n    5-fold cross-validation, and `predict_proba` may be inconsistent with\n    `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\n", "    Controls the pseudo random number generation for shuffling the data for\n    probability estimates. Ignored when `probability` is False.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    Whether to use the shrinking heuristic.\n    See the :ref:`User Guide <shrinking_svm>`.\n", "    Tolerance for stopping criterion.\n", "    Enable verbose output. Note that this setting takes advantage of a\n    per-process runtime setting in libsvm that, if enabled, may not work\n    properly in a multithreaded context.\n"]}}}, "imputers": {"mean": {"docs_name": "sklearn.impute.SimpleImputer", "preset_params": ["mean imp"], "param_names": ["add_indicator", "copy", "fill_value", "missing_values", "strategy", "verbose"], "default_param_types": ["bool", "bool", "NoneType", "str", "str", "int"], "default_params": ["False", "True", "None", "'NaN'", "'mean'", "0"], "options": [["True", "False"], ["True", "False"], "", "", "", ""], "descrs": ["    If True, a :class:`MissingIndicator` transform will stack onto output\n    of the imputer's transform. This allows a predictive estimator\n    to account for missingness despite imputation. If a feature has no\n    missing values at fit/train time, the feature won't appear on\n    the missing indicator even if there are missing values at\n    transform/test time.\n", "    If True, a copy of X will be created. If False, imputation will\n    be done in-place whenever possible. Note that, in the following cases,\n    a new copy will always be made, even if `copy=False`:\n    - If X is not an array of floating values;\n    - If X is encoded as a CSR matrix;\n    - If add_indicator=True.\n", "    When strategy == \"constant\", fill_value is used to replace all\n    occurrences of missing_values.\n    If left to the default, fill_value will be 0 when imputing numerical\n    data and \"missing_value\" for strings or object data types.\n", "    The placeholder for the missing values. All occurrences of\n    `missing_values` will be imputed. For pandas' dataframes with\n    nullable integer dtypes with missing values, `missing_values`\n    should be set to `np.nan`, since `pd.NA` will be converted to `np.nan`.\n", "    The imputation strategy.\n    - If \"mean\", then replace missing values using the mean along\n      each column. Can only be used with numeric data.\n    - If \"median\", then replace missing values using the median along\n      each column. Can only be used with numeric data.\n    - If \"most_frequent\", then replace missing using the most frequent\n      value along each column. Can be used with strings or numeric data.\n    - If \"constant\", then replace missing values with fill_value. Can be\n      used with strings or numeric data.\n    .. versionadded:: 0.20\n       strategy=\"constant\" for fixed value imputation.\n", "    Controls the verbosity of the imputer.\n"]}, "median": {"docs_name": "sklearn.impute.SimpleImputer", "preset_params": ["median imp"], "param_names": ["add_indicator", "copy", "fill_value", "missing_values", "strategy", "verbose"], "default_param_types": ["bool", "bool", "NoneType", "str", "str", "int"], "default_params": ["False", "True", "None", "'NaN'", "'mean'", "0"], "options": [["True", "False"], ["True", "False"], "", "", "", ""], "descrs": ["    If True, a :class:`MissingIndicator` transform will stack onto output\n    of the imputer's transform. This allows a predictive estimator\n    to account for missingness despite imputation. If a feature has no\n    missing values at fit/train time, the feature won't appear on\n    the missing indicator even if there are missing values at\n    transform/test time.\n", "    If True, a copy of X will be created. If False, imputation will\n    be done in-place whenever possible. Note that, in the following cases,\n    a new copy will always be made, even if `copy=False`:\n    - If X is not an array of floating values;\n    - If X is encoded as a CSR matrix;\n    - If add_indicator=True.\n", "    When strategy == \"constant\", fill_value is used to replace all\n    occurrences of missing_values.\n    If left to the default, fill_value will be 0 when imputing numerical\n    data and \"missing_value\" for strings or object data types.\n", "    The placeholder for the missing values. All occurrences of\n    `missing_values` will be imputed. For pandas' dataframes with\n    nullable integer dtypes with missing values, `missing_values`\n    should be set to `np.nan`, since `pd.NA` will be converted to `np.nan`.\n", "    The imputation strategy.\n    - If \"mean\", then replace missing values using the mean along\n      each column. Can only be used with numeric data.\n    - If \"median\", then replace missing values using the median along\n      each column. Can only be used with numeric data.\n    - If \"most_frequent\", then replace missing using the most frequent\n      value along each column. Can be used with strings or numeric data.\n    - If \"constant\", then replace missing values with fill_value. Can be\n      used with strings or numeric data.\n    .. versionadded:: 0.20\n       strategy=\"constant\" for fixed value imputation.\n", "    Controls the verbosity of the imputer.\n"]}, "most frequent": {"docs_name": "sklearn.impute.SimpleImputer", "preset_params": ["most freq imp"], "param_names": ["add_indicator", "copy", "fill_value", "missing_values", "strategy", "verbose"], "default_param_types": ["bool", "bool", "NoneType", "str", "str", "int"], "default_params": ["False", "True", "None", "'NaN'", "'mean'", "0"], "options": [["True", "False"], ["True", "False"], "", "", "", ""], "descrs": ["    If True, a :class:`MissingIndicator` transform will stack onto output\n    of the imputer's transform. This allows a predictive estimator\n    to account for missingness despite imputation. If a feature has no\n    missing values at fit/train time, the feature won't appear on\n    the missing indicator even if there are missing values at\n    transform/test time.\n", "    If True, a copy of X will be created. If False, imputation will\n    be done in-place whenever possible. Note that, in the following cases,\n    a new copy will always be made, even if `copy=False`:\n    - If X is not an array of floating values;\n    - If X is encoded as a CSR matrix;\n    - If add_indicator=True.\n", "    When strategy == \"constant\", fill_value is used to replace all\n    occurrences of missing_values.\n    If left to the default, fill_value will be 0 when imputing numerical\n    data and \"missing_value\" for strings or object data types.\n", "    The placeholder for the missing values. All occurrences of\n    `missing_values` will be imputed. For pandas' dataframes with\n    nullable integer dtypes with missing values, `missing_values`\n    should be set to `np.nan`, since `pd.NA` will be converted to `np.nan`.\n", "    The imputation strategy.\n    - If \"mean\", then replace missing values using the mean along\n      each column. Can only be used with numeric data.\n    - If \"median\", then replace missing values using the median along\n      each column. Can only be used with numeric data.\n    - If \"most_frequent\", then replace missing using the most frequent\n      value along each column. Can be used with strings or numeric data.\n    - If \"constant\", then replace missing values with fill_value. Can be\n      used with strings or numeric data.\n    .. versionadded:: 0.20\n       strategy=\"constant\" for fixed value imputation.\n", "    Controls the verbosity of the imputer.\n"]}, "constant": {"docs_name": "sklearn.impute.SimpleImputer", "preset_params": ["constant imp"], "param_names": ["add_indicator", "copy", "fill_value", "missing_values", "strategy", "verbose"], "default_param_types": ["bool", "bool", "NoneType", "str", "str", "int"], "default_params": ["False", "True", "None", "'NaN'", "'mean'", "0"], "options": [["True", "False"], ["True", "False"], "", "", "", ""], "descrs": ["    If True, a :class:`MissingIndicator` transform will stack onto output\n    of the imputer's transform. This allows a predictive estimator\n    to account for missingness despite imputation. If a feature has no\n    missing values at fit/train time, the feature won't appear on\n    the missing indicator even if there are missing values at\n    transform/test time.\n", "    If True, a copy of X will be created. If False, imputation will\n    be done in-place whenever possible. Note that, in the following cases,\n    a new copy will always be made, even if `copy=False`:\n    - If X is not an array of floating values;\n    - If X is encoded as a CSR matrix;\n    - If add_indicator=True.\n", "    When strategy == \"constant\", fill_value is used to replace all\n    occurrences of missing_values.\n    If left to the default, fill_value will be 0 when imputing numerical\n    data and \"missing_value\" for strings or object data types.\n", "    The placeholder for the missing values. All occurrences of\n    `missing_values` will be imputed. For pandas' dataframes with\n    nullable integer dtypes with missing values, `missing_values`\n    should be set to `np.nan`, since `pd.NA` will be converted to `np.nan`.\n", "    The imputation strategy.\n    - If \"mean\", then replace missing values using the mean along\n      each column. Can only be used with numeric data.\n    - If \"median\", then replace missing values using the median along\n      each column. Can only be used with numeric data.\n    - If \"most_frequent\", then replace missing using the most frequent\n      value along each column. Can be used with strings or numeric data.\n    - If \"constant\", then replace missing values with fill_value. Can be\n      used with strings or numeric data.\n    .. versionadded:: 0.20\n       strategy=\"constant\" for fixed value imputation.\n", "    Controls the verbosity of the imputer.\n"]}, "iterative": {"docs_name": "sklearn.impute.IterativeImputer", "preset_params": ["iterative imp"], "param_names": ["add_indicator", "estimator", "imputation_order", "initial_strategy", "max_iter", "max_value", "min_value", "missing_values", "n_nearest_features", "random_state", "sample_posterior", "skip_complete", "tol", "verbose"], "default_param_types": ["bool", "NoneType", "str", "str", "int", "NoneType", "NoneType", "str", "NoneType", "NoneType", "bool", "bool", "float", "int"], "default_params": ["False", "None", "'ascending'", "'mean'", "10", "None", "None", "'NaN'", "None", "None", "False", "False", "0.001", "0"], "options": [["True", "False"], "", ["ascending", "descending", "roman", "arabic", "random"], ["mean", "median", "most_frequent", "constant"], "", "", "", "", "", "", ["True", "False"], ["True", "False"], "", ""], "descrs": ["    If True, a :class:`MissingIndicator` transform will stack onto output\n    of the imputer's transform. This allows a predictive estimator\n    to account for missingness despite imputation. If a feature has no\n    missing values at fit/train time, the feature won't appear on\n    the missing indicator even if there are missing values at\n    transform/test time.\n", "    The estimator to use at each step of the round-robin imputation.\n    If ``sample_posterior`` is True, the estimator must support\n    ``return_std`` in its ``predict`` method.\n", "    The order in which the features will be imputed. Possible values:\n    \"ascending\"\n        From features with fewest missing values to most.\n    \"descending\"\n        From features with most missing values to fewest.\n    \"roman\"\n        Left to right.\n    \"arabic\"\n        Right to left.\n    \"random\"\n        A random order for each round.\n", "    Which strategy to use to initialize the missing values. Same as the\n    ``strategy`` parameter in :class:`sklearn.impute.SimpleImputer`\n    Valid values: {\"mean\", \"median\", \"most_frequent\", or \"constant\"}.\n", "    Maximum number of imputation rounds to perform before returning the\n    imputations computed during the final round. A round is a single\n    imputation of each feature with missing values. The stopping criterion\n    is met once `abs(max(X_t - X_{t-1}))/abs(max(X[known_vals]))` < tol,\n    where `X_t` is `X` at iteration `t. Note that early stopping is only\n    applied if ``sample_posterior=False``.\n", "    Maximum possible imputed value. Broadcast to shape (n_features,) if\n    scalar. If array-like, expects shape (n_features,), one max value for\n    each feature. `None` (default) is converted to np.inf.\n", "    Minimum possible imputed value. Broadcast to shape (n_features,) if\n    scalar. If array-like, expects shape (n_features,), one min value for\n    each feature. `None` (default) is converted to -np.inf.\n", "    The placeholder for the missing values. All occurrences of\n    `missing_values` will be imputed. For pandas' dataframes with\n    nullable integer dtypes with missing values, `missing_values`\n    should be set to `np.nan`, since `pd.NA` will be converted to `np.nan`.\n", "    Number of other features to use to estimate the missing values of\n    each feature column. Nearness between features is measured using\n    the absolute correlation coefficient between each feature pair (after\n    initial imputation). To ensure coverage of features throughout the\n    imputation process, the neighbor features are not necessarily nearest,\n    but are drawn with probability proportional to correlation for each\n    imputed target feature. Can provide significant speed-up when the\n    number of features is huge. If ``None``, all features will be used.\n", "    The seed of the pseudo random number generator to use. Randomizes\n    selection of estimator features if n_nearest_features is not None, the\n    ``imputation_order`` if ``random``, and the sampling from posterior if\n    ``sample_posterior`` is True. Use an integer for determinism.\n    See :term:`the Glossary <random_state>`.\n", "    Whether to sample from the (Gaussian) predictive posterior of the\n    fitted estimator for each imputation. Estimator must support\n    ``return_std`` in its ``predict`` method if set to ``True``. Set to\n    ``True`` if using ``IterativeImputer`` for multiple imputations.\n", "    If ``True`` then features with missing values during ``transform``\n    which did not have any missing values during ``fit`` will be imputed\n    with the initial imputation method only. Set to ``True`` if you have\n    many features with no missing values at both ``fit`` and ``transform``\n    time to save compute.\n", "    Tolerance of the stopping condition.\n", "    Verbosity flag, controls the debug messages that are issued\n    as functions are evaluated. The higher, the more verbose. Can be 0, 1,\n    or 2.\n"]}}, "scalers": {"standard": {"docs_name": "sklearn.preprocessing.StandardScaler", "preset_params": ["base standard"], "param_names": ["copy", "with_mean", "with_std"], "default_param_types": ["bool", "bool", "bool"], "default_params": ["True", "True", "True"], "options": [["True", "False"], ["True", "False"], ["True", "False"]], "descrs": ["    If False, try to avoid a copy and do inplace scaling instead.\n    This is not guaranteed to always work inplace; e.g. if the data is\n    not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n    returned.\n", "    If True, center the data before scaling.\n    This does not work (and will raise an exception) when attempted on\n    sparse matrices, because centering them entails building a dense\n    matrix which in common use cases is likely to be too large to fit in\n    memory.\n", "    If True, scale the data to unit variance (or equivalently,\n    unit standard deviation).\n"]}, "minmax": {"docs_name": "sklearn.preprocessing.MinMaxScaler", "preset_params": ["base minmax"], "param_names": ["copy", "feature_range"], "default_param_types": ["bool", "tuple"], "default_params": ["True", "(0, 1)"], "options": [["True", "False"], ""], "descrs": ["    Set to False to perform inplace row normalization and avoid a\n    copy (if the input is already a numpy array).\n", "    Desired range of transformed data.\n"]}, "maxabs": {"docs_name": "sklearn.preprocessing.MaxAbsScaler", "preset_params": ["default"], "param_names": ["copy"], "default_param_types": ["bool"], "default_params": ["True"], "options": [["True", "False"]], "descrs": ["    Set to False to perform inplace scaling and avoid a copy (if the input\n    is already a numpy array).\n"]}, "robust": {"docs_name": "sklearn.preprocessing.RobustScaler", "preset_params": ["base robust", "robust gs"], "param_names": ["copy", "quantile_range", "with_centering", "with_scaling"], "default_param_types": ["bool", "tuple", "bool", "bool"], "default_params": ["True", "(25.0, 75.0)", "True", "True"], "options": [["True", "False"], "", ["True", "False"], ["True", "False"]], "descrs": ["    If False, try to avoid a copy and do inplace scaling instead.\n    This is not guaranteed to always work inplace; e.g. if the data is\n    not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n    returned.\n", "    Default: (25.0, 75.0) = (1st quantile, 3rd quantile) = IQR\n    Quantile range used to calculate ``scale_``.\n    .. versionadded:: 0.18\n", "    If True, center the data before scaling.\n    This will cause ``transform`` to raise an exception when attempted on\n    sparse matrices, because centering them entails building a dense\n    matrix which in common use cases is likely to be too large to fit in\n    memory.\n", "    If True, scale the data to interquartile range.\n"]}, "yeo": {"docs_name": "sklearn.preprocessing.PowerTransformer", "preset_params": ["base yeo"], "param_names": ["copy", "method", "standardize"], "default_param_types": ["bool", "str", "bool"], "default_params": ["True", "'yeo-johnson'", "True"], "options": [["True", "False"], "", ["True", "False"]], "descrs": ["    Set to False to perform inplace computation during transformation.\n", "    The power transform method. Available methods are:\n    - 'yeo-johnson' [1]_, works with positive and negative values\n    - 'box-cox' [2]_, only works with strictly positive values\n", "    Set to True to apply zero-mean, unit-variance normalization to the\n    transformed output.\n"]}, "boxcox": {"docs_name": "sklearn.preprocessing.PowerTransformer", "preset_params": ["base boxcox"], "param_names": ["copy", "method", "standardize"], "default_param_types": ["bool", "str", "bool"], "default_params": ["True", "'yeo-johnson'", "True"], "options": [["True", "False"], "", ["True", "False"]], "descrs": ["    Set to False to perform inplace computation during transformation.\n", "    The power transform method. Available methods are:\n    - 'yeo-johnson' [1]_, works with positive and negative values\n    - 'box-cox' [2]_, only works with strictly positive values\n", "    Set to True to apply zero-mean, unit-variance normalization to the\n    transformed output.\n"]}, "winsorize": {"docs_name": "ABCD_ML.extensions.Scalers.Winsorizer", "preset_params": ["base winsorize", "winsorize gs"], "param_names": ["copy", "quantile_range"], "default_param_types": ["bool", "tuple"], "default_params": ["True", "(5, 95)"], "options": [["True", "False"], ""], "descrs": ["    Make a copy of the data.\n", "    Default: (5.0, 95.0), the lower and upper range in which to clip\n    values to.\n"]}, "quantile norm": {"docs_name": "sklearn.preprocessing.QuantileTransformer", "preset_params": ["base quant norm"], "param_names": ["copy", "ignore_implicit_zeros", "n_quantiles", "output_distribution", "random_state", "subsample"], "default_param_types": ["bool", "bool", "int", "str", "NoneType", "int"], "default_params": ["True", "False", "1000", "'uniform'", "None", "100000"], "options": [["True", "False"], ["True", "False"], "", "", "", ""], "descrs": ["    Set to False to perform inplace transformation and avoid a copy (if the\n    input is already a numpy array).\n", "    Only applies to sparse matrices. If True, the sparse entries of the\n    matrix are discarded to compute the quantile statistics. If False,\n    these entries are treated as zeros.\n", "    Number of quantiles to be computed. It corresponds to the number\n    of landmarks used to discretize the cumulative distribution function.\n    If n_quantiles is larger than the number of samples, n_quantiles is set\n    to the number of samples as a larger number of quantiles does not give\n    a better approximation of the cumulative distribution function\n    estimator.\n", "    Marginal distribution for the transformed data. The choices are\n    'uniform' (default) or 'normal'.\n", "    Determines random number generation for subsampling and smoothing\n    noise.\n    Please see ``subsample`` for more details.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`\n", "    Maximum number of samples used to estimate the quantiles for\n    computational efficiency. Note that the subsampling procedure may\n    differ for value-identical sparse and dense matrices.\n"]}, "quantile uniform": {"docs_name": "sklearn.preprocessing.QuantileTransformer", "preset_params": ["base quant uniform"], "param_names": ["copy", "ignore_implicit_zeros", "n_quantiles", "output_distribution", "random_state", "subsample"], "default_param_types": ["bool", "bool", "int", "str", "NoneType", "int"], "default_params": ["True", "False", "1000", "'uniform'", "None", "100000"], "options": [["True", "False"], ["True", "False"], "", "", "", ""], "descrs": ["    Set to False to perform inplace transformation and avoid a copy (if the\n    input is already a numpy array).\n", "    Only applies to sparse matrices. If True, the sparse entries of the\n    matrix are discarded to compute the quantile statistics. If False,\n    these entries are treated as zeros.\n", "    Number of quantiles to be computed. It corresponds to the number\n    of landmarks used to discretize the cumulative distribution function.\n    If n_quantiles is larger than the number of samples, n_quantiles is set\n    to the number of samples as a larger number of quantiles does not give\n    a better approximation of the cumulative distribution function\n    estimator.\n", "    Marginal distribution for the transformed data. The choices are\n    'uniform' (default) or 'normal'.\n", "    Determines random number generation for subsampling and smoothing\n    noise.\n    Please see ``subsample`` for more details.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`\n", "    Maximum number of samples used to estimate the quantiles for\n    computational efficiency. Note that the subsampling procedure may\n    differ for value-identical sparse and dense matrices.\n"]}, "normalize": {"docs_name": "sklearn.preprocessing.Normalizer", "preset_params": ["default"], "param_names": ["copy", "norm"], "default_param_types": ["bool", "str"], "default_params": ["True", "'l2'"], "options": [["True", "False"], ""], "descrs": ["    set to False to perform inplace row normalization and avoid a\n    copy (if the input is already a numpy array or a scipy.sparse\n    CSR matrix).\n", "    The norm to use to normalize each non zero sample. If norm='max'\n    is used, values will be rescaled by the maximum of the absolute\n    values.\n"]}}, "transformers": {"pca": {"docs_name": "sklearn.decomposition.PCA", "preset_params": ["default", "pca var search"], "param_names": ["copy", "iterated_power", "n_components", "random_state", "svd_solver", "tol", "whiten"], "default_param_types": ["bool", "str", "NoneType", "NoneType", "str", "float", "bool"], "default_params": ["True", "'auto'", "None", "None", "'auto'", "0.0", "False"], "options": [["True", "False"], "", "", "", ["'auto'", "'full'", "'arpack'", "'randomized'"], "", ["True", "False"]], "descrs": ["    If False, data passed to fit are overwritten and running\n    fit(X).transform(X) will not yield the expected results,\n    use fit_transform(X) instead.\n", "    Number of iterations for the power method computed by\n    svd_solver == 'randomized'.\n    .. versionadded:: 0.18.0\n", "    Number of components to keep.\n    if n_components is not set all components are kept::\n        n_components == min(n_samples, n_features)\n    If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's\n    MLE is used to guess the dimension. Use of ``n_components == 'mle'``\n    will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\n    If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the\n    number of components such that the amount of variance that needs to be\n    explained is greater than the percentage specified by n_components.\n    If ``svd_solver == 'arpack'``, the number of components must be\n    strictly less than the minimum of n_features and n_samples.\n    Hence, the None case results in::\n        n_components == min(n_samples, n_features) - 1\n", "    Used when ``svd_solver`` == 'arpack' or 'randomized'. Pass an int\n    for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n    .. versionadded:: 0.18.0\n", "    If auto :\n        The solver is selected by a default policy based on `X.shape` and\n        `n_components`: if the input data is larger than 500x500 and the\n        number of components to extract is lower than 80% of the smallest\n        dimension of the data, then the more efficient 'randomized'\n        method is enabled. Otherwise the exact full SVD is computed and\n        optionally truncated afterwards.\n    If full :\n        run exact full SVD calling the standard LAPACK solver via\n        `scipy.linalg.svd` and select the components by postprocessing\n    If arpack :\n        run SVD truncated to n_components calling ARPACK solver via\n        `scipy.sparse.linalg.svds`. It requires strictly\n        0 < n_components < min(X.shape)\n    If randomized :\n        run randomized SVD by the method of Halko et al.\n    .. versionadded:: 0.18.0\n", "    Tolerance for singular values computed by svd_solver == 'arpack'.\n    .. versionadded:: 0.18.0\n", "    When True (False by default) the `components_` vectors are multiplied\n    by the square root of n_samples and then divided by the singular values\n    to ensure uncorrelated outputs with unit component-wise variances.\n    Whitening will remove some information from the transformed signal\n    (the relative variance scales of the components) but can sometime\n    improve the predictive accuracy of the downstream estimators by\n    making their data respect some hard-wired assumptions.\n"]}, "sparse pca": {"docs_name": "sklearn.decomposition.SparsePCA", "preset_params": ["default"], "param_names": ["U_init", "V_init", "alpha", "max_iter", "method", "n_components", "n_jobs", "normalize_components", "random_state", "ridge_alpha", "tol", "verbose"], "default_param_types": ["NoneType", "NoneType", "int", "int", "str", "NoneType", "NoneType", "str", "NoneType", "float", "float", "bool"], "default_params": ["None", "None", "1", "1000", "'lars'", "None", "None", "'deprecated'", "None", "0.01", "1e-08", "False"], "options": ["", "", "", "", ["'lars'", "'cd'"], "", "", "", "", "", "", ["True", "False"]], "descrs": ["    Initial values for the loadings for warm restart scenarios.\n", "    Initial values for the components for warm restart scenarios.\n", "    Sparsity controlling parameter. Higher values lead to sparser\n    components.\n", "    Maximum number of iterations to perform.\n", "    lars: uses the least angle regression method to solve the lasso problem\n    (linear_model.lars_path)\n    cd: uses the coordinate descent method to compute the\n    Lasso solution (linear_model.Lasso). Lars will be faster if\n    the estimated components are sparse.\n", "    Number of sparse atoms to extract.\n", "    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n", "    This parameter does not have any effect. The components are always\n    normalized.\n    .. versionadded:: 0.20\n    .. deprecated:: 0.22\n       ``normalize_components`` is deprecated in 0.22 and will be removed\n       in 0.24.\n", "    Used during dictionary learning. Pass an int for reproducible results\n    across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    Amount of ridge shrinkage to apply in order to improve\n    conditioning when calling the transform method.\n", "    Tolerance for the stopping condition.\n", "    Controls the verbosity; the higher, the more messages. Defaults to 0.\n"]}, "mini batch sparse pca": {"docs_name": "sklearn.decomposition.MiniBatchSparsePCA", "preset_params": ["default"], "param_names": ["alpha", "batch_size", "callback", "method", "n_components", "n_iter", "n_jobs", "normalize_components", "random_state", "ridge_alpha", "shuffle", "verbose"], "default_param_types": ["int", "int", "NoneType", "str", "NoneType", "int", "NoneType", "str", "NoneType", "float", "bool", "bool"], "default_params": ["1", "3", "None", "'lars'", "None", "100", "None", "'deprecated'", "None", "0.01", "True", "False"], "options": ["", "", "", ["'lars'", "'cd'"], "", "", "", "", "", "", ["True", "False"], ["True", "False"]], "descrs": ["    Sparsity controlling parameter. Higher values lead to sparser\n    components.\n", "    the number of features to take in each mini batch\n", "    callable that gets invoked every five iterations\n", "    lars: uses the least angle regression method to solve the lasso problem\n    (linear_model.lars_path)\n    cd: uses the coordinate descent method to compute the\n    Lasso solution (linear_model.Lasso). Lars will be faster if\n    the estimated components are sparse.\n", "    number of sparse atoms to extract\n", "    number of iterations to perform for each mini batch\n", "    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n", "    This parameter does not have any effect. The components are always\n    normalized.\n    .. versionadded:: 0.20\n    .. deprecated:: 0.22\n       ``normalize_components`` is deprecated in 0.22 and will be removed\n       in 0.24.\n", "    Used for random shuffling when ``shuffle`` is set to ``True``,\n    during online dictionary learning. Pass an int for reproducible results\n    across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    Amount of ridge shrinkage to apply in order to improve\n    conditioning when calling the transform method.\n", "    whether to shuffle the data before splitting it in batches\n", "    Controls the verbosity; the higher, the more messages. Defaults to 0.\n"]}, "factor analysis": {"docs_name": "sklearn.decomposition.FactorAnalysis", "preset_params": ["default"], "param_names": ["copy", "iterated_power", "max_iter", "n_components", "noise_variance_init", "random_state", "svd_method", "tol"], "default_param_types": ["bool", "int", "int", "NoneType", "NoneType", "int", "str", "float"], "default_params": ["True", "3", "1000", "None", "None", "0", "'randomized'", "0.01"], "options": [["True", "False"], "", "", "", "", "", ["'lapack'", "'randomized'"], ""], "descrs": ["    Whether to make a copy of X. If ``False``, the input X gets overwritten\n    during fitting.\n", "    Number of iterations for the power method. 3 by default. Only used\n    if ``svd_method`` equals 'randomized'\n", "    Maximum number of iterations.\n", "    Dimensionality of latent space, the number of components\n    of ``X`` that are obtained after ``transform``.\n    If None, n_components is set to the number of features.\n", "    The initial guess of the noise variance for each feature.\n    If None, it defaults to np.ones(n_features)\n", "    Only used when ``svd_method`` equals 'randomized'. Pass an int for\n    reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    Which SVD method to use. If 'lapack' use standard SVD from\n    scipy.linalg, if 'randomized' use fast ``randomized_svd`` function.\n    Defaults to 'randomized'. For most applications 'randomized' will\n    be sufficiently precise while providing significant speed gains.\n    Accuracy can also be improved by setting higher values for\n    `iterated_power`. If this is not sufficient, for maximum precision\n    you should choose 'lapack'.\n", "    Stopping tolerance for log-likelihood increase.\n"]}, "dictionary learning": {"docs_name": "sklearn.decomposition.DictionaryLearning", "preset_params": ["default"], "param_names": ["alpha", "code_init", "dict_init", "fit_algorithm", "max_iter", "n_components", "n_jobs", "positive_code", "positive_dict", "random_state", "split_sign", "tol", "transform_algorithm", "transform_alpha", "transform_max_iter", "transform_n_nonzero_coefs", "verbose"], "default_param_types": ["int", "NoneType", "NoneType", "str", "int", "NoneType", "NoneType", "bool", "bool", "NoneType", "bool", "float", "str", "NoneType", "int", "NoneType", "bool"], "default_params": ["1", "None", "None", "'lars'", "1000", "None", "None", "False", "False", "None", "False", "1e-08", "'omp'", "None", "1000", "None", "False"], "options": ["", "", "", ["'lars'", "'cd'"], "", "", "", ["True", "False"], ["True", "False"], "", ["True", "False"], "", ["'lasso_lars'", "'lasso_cd'", "'lars'", "'omp'", "'threshold'"], "", "", "", ["True", "False"]], "descrs": ["    sparsity controlling parameter\n", "    initial value for the code, for warm restart\n", "    initial values for the dictionary, for warm restart\n", "    lars: uses the least angle regression method to solve the lasso problem\n    (linear_model.lars_path)\n    cd: uses the coordinate descent method to compute the\n    Lasso solution (linear_model.Lasso). Lars will be faster if\n    the estimated components are sparse.\n    .. versionadded:: 0.17\n       *cd* coordinate descent method to improve speed.\n", "    maximum number of iterations to perform\n", "    number of dictionary elements to extract\n", "    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n", "    Whether to enforce positivity when finding the code.\n    .. versionadded:: 0.20\n", "    Whether to enforce positivity when finding the dictionary\n    .. versionadded:: 0.20\n", "    Used for initializing the dictionary when ``dict_init`` is not\n    specified, randomly shuffling the data when ``shuffle`` is set to\n    ``True``, and updating the dictionary. Pass an int for reproducible\n    results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    Whether to split the sparse feature vector into the concatenation of\n    its negative part and its positive part. This can improve the\n    performance of downstream classifiers.\n", "    tolerance for numerical error\n", "    Algorithm used to transform the data\n    lars: uses the least angle regression method (linear_model.lars_path)\n    lasso_lars: uses Lars to compute the Lasso solution\n    lasso_cd: uses the coordinate descent method to compute the\n    Lasso solution (linear_model.Lasso). lasso_lars will be faster if\n    the estimated components are sparse.\n    omp: uses orthogonal matching pursuit to estimate the sparse solution\n    threshold: squashes to zero all coefficients less than alpha from\n    the projection ``dictionary * X'``\n    .. versionadded:: 0.17\n       *lasso_cd* coordinate descent method to improve speed.\n", "    If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n    penalty applied to the L1 norm.\n    If `algorithm='threshold'`, `alpha` is the absolute value of the\n    threshold below which coefficients will be squashed to zero.\n    If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n    the reconstruction error targeted. In this case, it overrides\n    `n_nonzero_coefs`.\n", "    Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n    `lasso_lars`.\n    .. versionadded:: 0.22\n", "    Number of nonzero coefficients to target in each column of the\n    solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n    and is overridden by `alpha` in the `omp` case.\n", "    To control the verbosity of the procedure.\n"]}, "mini batch dictionary learning": {"docs_name": "sklearn.decomposition.MiniBatchDictionaryLearning", "preset_params": ["default"], "param_names": ["alpha", "batch_size", "dict_init", "fit_algorithm", "n_components", "n_iter", "n_jobs", "positive_code", "positive_dict", "random_state", "shuffle", "split_sign", "transform_algorithm", "transform_alpha", "transform_max_iter", "transform_n_nonzero_coefs", "verbose"], "default_param_types": ["int", "int", "NoneType", "str", "NoneType", "int", "NoneType", "bool", "bool", "NoneType", "bool", "bool", "str", "NoneType", "int", "NoneType", "bool"], "default_params": ["1", "3", "None", "'lars'", "None", "1000", "None", "False", "False", "None", "True", "False", "'omp'", "None", "1000", "None", "False"], "options": ["", "", "", ["'lars'", "'cd'"], "", "", "", ["True", "False"], ["True", "False"], "", ["True", "False"], ["True", "False"], ["'lasso_lars'", "'lasso_cd'", "'lars'", "'omp'", "'threshold'"], "", "", "", ["True", "False"]], "descrs": ["    sparsity controlling parameter\n", "    number of samples in each mini-batch\n", "    initial value of the dictionary for warm restart scenarios\n", "    lars: uses the least angle regression method to solve the lasso problem\n    (linear_model.lars_path)\n    cd: uses the coordinate descent method to compute the\n    Lasso solution (linear_model.Lasso). Lars will be faster if\n    the estimated components are sparse.\n", "    number of dictionary elements to extract\n", "    total number of iterations to perform\n", "    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n", "    Whether to enforce positivity when finding the code.\n    .. versionadded:: 0.20\n", "    Whether to enforce positivity when finding the dictionary.\n    .. versionadded:: 0.20\n", "    Used for initializing the dictionary when ``dict_init`` is not\n    specified, randomly shuffling the data when ``shuffle`` is set to\n    ``True``, and updating the dictionary. Pass an int for reproducible\n    results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    whether to shuffle the samples before forming batches\n", "    Whether to split the sparse feature vector into the concatenation of\n    its negative part and its positive part. This can improve the\n    performance of downstream classifiers.\n", "    Algorithm used to transform the data.\n    lars: uses the least angle regression method (linear_model.lars_path)\n    lasso_lars: uses Lars to compute the Lasso solution\n    lasso_cd: uses the coordinate descent method to compute the\n    Lasso solution (linear_model.Lasso). lasso_lars will be faster if\n    the estimated components are sparse.\n    omp: uses orthogonal matching pursuit to estimate the sparse solution\n    threshold: squashes to zero all coefficients less than alpha from\n    the projection dictionary * X'\n", "    If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n    penalty applied to the L1 norm.\n    If `algorithm='threshold'`, `alpha` is the absolute value of the\n    threshold below which coefficients will be squashed to zero.\n    If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n    the reconstruction error targeted. In this case, it overrides\n    `n_nonzero_coefs`.\n", "    Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n    `lasso_lars`.\n    .. versionadded:: 0.22\n", "    Number of nonzero coefficients to target in each column of the\n    solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n    and is overridden by `alpha` in the `omp` case.\n", "    To control the verbosity of the procedure.\n"]}, "fast ica": {"docs_name": "sklearn.decomposition.FastICA", "preset_params": ["default"], "param_names": ["algorithm", "fun", "fun_args", "max_iter", "n_components", "random_state", "tol", "w_init", "whiten"], "default_param_types": ["str", "str", "NoneType", "int", "NoneType", "NoneType", "float", "NoneType", "bool"], "default_params": ["'parallel'", "'logcosh'", "None", "200", "None", "None", "0.0001", "None", "True"], "options": [["'parallel'", "'deflation'"], "", "", "", "", "", "", "", ["True", "False"]], "descrs": ["    Apply parallel or deflational algorithm for FastICA.\n", "    The functional form of the G function used in the\n    approximation to neg-entropy. Could be either 'logcosh', 'exp',\n    or 'cube'.\n    You can also provide your own function. It should return a tuple\n    containing the value of the function, and of its derivative, in the\n    point. Example:\n    def my_g(x):\n        return x ** 3, (3 * x ** 2).mean(axis=-1)\n", "    Arguments to send to the functional form.\n    If empty and if fun='logcosh', fun_args will take value\n    {'alpha' : 1.0}.\n", "    Maximum number of iterations during fit.\n", "    Number of components to use. If none is passed, all are used.\n", "    Used to initialize ``w_init`` when not specified, with a\n    normal distribution. Pass an int, for reproducible results\n    across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    Tolerance on update at each iteration.\n", "    The mixing matrix to be used to initialize the algorithm.\n", "    If whiten is false, the data is already considered to be\n    whitened, and no whitening is performed.\n"]}, "incremental pca": {"docs_name": "sklearn.decomposition.IncrementalPCA", "preset_params": ["default"], "param_names": ["batch_size", "copy", "n_components", "whiten"], "default_param_types": ["NoneType", "bool", "NoneType", "bool"], "default_params": ["None", "True", "None", "False"], "options": ["", ["True", "False"], "", ["True", "False"]], "descrs": ["    The number of samples to use for each batch. Only used when calling\n    ``fit``. If ``batch_size`` is ``None``, then ``batch_size``\n    is inferred from the data and set to ``5 * n_features``, to provide a\n    balance between approximation accuracy and memory consumption.\n", "    If False, X will be overwritten. ``copy=False`` can be used to\n    save memory but is unsafe for general use.\n", "    Number of components to keep. If ``n_components `` is ``None``,\n    then ``n_components`` is set to ``min(n_samples, n_features)``.\n", "    When True (False by default) the ``components_`` vectors are divided\n    by ``n_samples`` times ``components_`` to ensure uncorrelated outputs\n    with unit component-wise variances.\n    Whitening will remove some information from the transformed signal\n    (the relative variance scales of the components) but can sometimes\n    improve the predictive accuracy of the downstream estimators by\n    making data respect some hard-wired assumptions.\n"]}, "kernel pca": {"docs_name": "sklearn.decomposition.KernelPCA", "preset_params": ["default"], "param_names": ["alpha", "coef0", "copy_X", "degree", "eigen_solver", "fit_inverse_transform", "gamma", "kernel", "kernel_params", "max_iter", "n_components", "n_jobs", "random_state", "remove_zero_eig", "tol"], "default_param_types": ["float", "int", "bool", "int", "str", "bool", "NoneType", "str", "NoneType", "NoneType", "NoneType", "NoneType", "NoneType", "bool", "int"], "default_params": ["1.0", "1", "True", "3", "'auto'", "False", "None", "'linear'", "None", "None", "None", "None", "None", "False", "0"], "options": ["", "", ["True", "False"], "", "", ["True", "False"], "", "", "", "", "", "", "", ["True", "False"], ""], "descrs": ["    Hyperparameter of the ridge regression that learns the\n    inverse transform (when fit_inverse_transform=True).\n", "    Independent term in poly and sigmoid kernels.\n    Ignored by other kernels.\n", "    If True, input X is copied and stored by the model in the `X_fit_`\n    attribute. If no further changes will be done to X, setting\n    `copy_X=False` saves memory by storing a reference.\n    .. versionadded:: 0.18\n", "    Degree for poly kernels. Ignored by other kernels.\n", "    Select eigensolver to use. If n_components is much less than\n    the number of training samples, arpack may be more efficient\n    than the dense eigensolver.\n", "    Learn the inverse transform for non-precomputed kernels.\n    (i.e. learn to find the pre-image of a point)\n", "    Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other\n    kernels.\n", "    Kernel. Default=\"linear\".\n", "    Parameters (keyword arguments) and values for kernel passed as\n    callable object. Ignored by other kernels.\n", "    Maximum number of iterations for arpack.\n    If None, optimal value will be chosen by arpack.\n", "    Number of components. If None, all non-zero components are kept.\n", "    The number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n    .. versionadded:: 0.18\n", "    Used when ``eigen_solver`` == 'arpack'. Pass an int for reproducible\n    results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n    .. versionadded:: 0.18\n", "    If True, then all components with zero eigenvalues are removed, so\n    that the number of components in the output may be < n_components\n    (and sometimes even zero due to numerical instability).\n    When n_components is None, this parameter is ignored and components\n    with zero eigenvalues are removed regardless.\n", "    Convergence tolerance for arpack.\n    If 0, optimal value will be chosen by arpack.\n"]}, "nmf": {"docs_name": "sklearn.decomposition.NMF", "preset_params": ["default"], "param_names": ["alpha", "beta_loss", "init", "l1_ratio", "max_iter", "n_components", "random_state", "shuffle", "solver", "tol", "verbose"], "default_param_types": ["float", "str", "NoneType", "float", "int", "NoneType", "NoneType", "bool", "str", "float", "int"], "default_params": ["0.0", "'frobenius'", "None", "0.0", "200", "None", "None", "False", "'cd'", "0.0001", "0"], "options": ["", "", "", "", "", "", "", ["True", "False"], "", "", ""], "descrs": ["    Constant that multiplies the regularization terms. Set it to zero to\n    have no regularization.\n    .. versionadded:: 0.17\n       *alpha* used in the Coordinate Descent solver.\n", "    String must be in {'frobenius', 'kullback-leibler', 'itakura-saito'}.\n    Beta divergence to be minimized, measuring the distance between X\n    and the dot product WH. Note that values different from 'frobenius'\n    (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n    fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n    matrix X cannot contain zeros. Used only in 'mu' solver.\n    .. versionadded:: 0.19\n", "    Method used to initialize the procedure.\n    Default: None.\n    Valid options:\n    - None: 'nndsvd' if n_components <= min(n_samples, n_features),\n        otherwise random.\n    - 'random': non-negative random matrices, scaled with:\n        sqrt(X.mean() / n_components)\n    - 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)\n        initialization (better for sparseness)\n    - 'nndsvda': NNDSVD with zeros filled with the average of X\n        (better when sparsity is not desired)\n    - 'nndsvdar': NNDSVD with zeros filled with small random values\n        (generally faster, less accurate alternative to NNDSVDa\n        for when sparsity is not desired)\n    - 'custom': use custom matrices W and H\n", "    The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n    For l1_ratio = 0 the penalty is an elementwise L2 penalty\n    (aka Frobenius Norm).\n    For l1_ratio = 1 it is an elementwise L1 penalty.\n    For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n    .. versionadded:: 0.17\n       Regularization parameter *l1_ratio* used in the Coordinate Descent\n       solver.\n", "    Maximum number of iterations before timing out.\n", "    Number of components, if n_components is not set all features\n    are kept.\n", "    Used for initialisation (when ``init`` == 'nndsvdar' or\n    'random'), and in Coordinate Descent. Pass an int for reproducible\n    results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    If true, randomize the order of coordinates in the CD solver.\n    .. versionadded:: 0.17\n       *shuffle* parameter used in the Coordinate Descent solver.\n", "    Numerical solver to use:\n    'cd' is a Coordinate Descent solver.\n    'mu' is a Multiplicative Update solver.\n    .. versionadded:: 0.17\n       Coordinate Descent solver.\n    .. versionadded:: 0.19\n       Multiplicative Update solver.\n", "    Tolerance of the stopping condition.\n", "    Whether to be verbose.\n"]}, "truncated svd": {"docs_name": "sklearn.decomposition.TruncatedSVD", "preset_params": ["default"], "param_names": ["algorithm", "n_components", "n_iter", "random_state", "tol"], "default_param_types": ["str", "int", "int", "NoneType", "float"], "default_params": ["'randomized'", "2", "5", "None", "0.0"], "options": ["", "", "", "", ""], "descrs": ["    SVD solver to use. Either \"arpack\" for the ARPACK wrapper in SciPy\n    (scipy.sparse.linalg.svds), or \"randomized\" for the randomized\n    algorithm due to Halko (2009).\n", "    Desired dimensionality of output data.\n    Must be strictly less than the number of features.\n    The default value is useful for visualisation. For LSA, a value of\n    100 is recommended.\n", "    Number of iterations for randomized SVD solver. Not used by ARPACK. The\n    default is larger than the default in\n    :func:`~sklearn.utils.extmath.randomized_svd` to handle sparse\n    matrices that may have large slowly decaying spectrum.\n", "    Used during randomized svd. Pass an int for reproducible results across\n    multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    Tolerance for ARPACK. 0 means machine precision. Ignored by randomized\n    SVD solver.\n"]}}, "feature_selectors": {"binary": {"rfe": {"docs_name": "sklearn.feature_selection.RFE", "preset_params": ["base rfe", "rfe num feats dist"], "param_names": ["estimator", "n_features_to_select", "step", "verbose"], "default_param_types": ["str", "NoneType", "int", "int"], "default_params": ["\"<class 'inspect._empty'>\"", "None", "1", "0"], "options": ["", "", "", ""], "descrs": ["    A supervised learning estimator with a ``fit`` method that provides\n    information about feature importance either through a ``coef_``\n    attribute or through a ``feature_importances_`` attribute.\n", "   The number of features to select. If `None`, half of the features are selected. If a floating point number between 0 and 1, that percentage of the total features will be selected.", "    If greater than or equal to 1, then ``step`` corresponds to the\n    (integer) number of features to remove at each iteration.\n    If within (0.0, 1.0), then ``step`` corresponds to the percentage\n    (rounded down) of features to remove at each iteration.\n", "    Controls verbosity of output.\n"]}, "selector": {"docs_name": "ABCD_ML.extensions.Feat_Selectors.FeatureSelector", "preset_params": ["random", "searchable"], "param_names": ["mask"], "default_param_types": ["str"], "default_params": ["'sets as random features'"], "options": [["'sets as random features'", "'sets as hyperparameters'"]], "descrs": ["    - 'sets as random features': Use random features.\n    - 'sets as hyperparameters': Each feature is set as a             hyperparameter, such that the parameter search can             tune if each feature is included or not.\n"]}, "univariate selection c": {"docs_name": "sklearn.feature_selection.SelectPercentile", "preset_params": ["base univar fs classifier", "univar fs classifier dist"], "param_names": ["percentile", "score_func"], "default_param_types": ["int", "str"], "default_params": ["10", "'<function f_classif at 0x7f4977e411e0>'"], "options": ["", ["f_classif", "mutual_info_classif", "chi2"]], "descrs": ["    Percent of features to keep.\n", "    Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues) or a single array with scores.\n    Default is f_classif (see below \"See also\"). The default function only\n    works with classification tasks.\n    .. versionadded:: 0.18\n"]}, "variance threshold": {"docs_name": "sklearn.feature_selection.VarianceThreshold", "preset_params": ["default"], "param_names": ["threshold"], "default_param_types": ["float"], "default_params": ["0.0"], "options": [""], "descrs": ["    Features with a training-set variance lower than this threshold will\n    be removed. The default is to keep all features with non-zero variance,\n    i.e. remove the features that have the same value in all samples.\n"]}}, "regression": {"rfe": {"docs_name": "sklearn.feature_selection.RFE", "preset_params": ["base rfe", "rfe num feats dist"], "param_names": ["estimator", "n_features_to_select", "step", "verbose"], "default_param_types": ["str", "NoneType", "int", "int"], "default_params": ["\"<class 'inspect._empty'>\"", "None", "1", "0"], "options": ["", "", "", ""], "descrs": ["    A supervised learning estimator with a ``fit`` method that provides\n    information about feature importance either through a ``coef_``\n    attribute or through a ``feature_importances_`` attribute.\n", "   The number of features to select. If `None`, half of the features are selected. If a floating point number between 0 and 1, that percentage of the total features will be selected.", "    If greater than or equal to 1, then ``step`` corresponds to the\n    (integer) number of features to remove at each iteration.\n    If within (0.0, 1.0), then ``step`` corresponds to the percentage\n    (rounded down) of features to remove at each iteration.\n", "    Controls verbosity of output.\n"]}, "selector": {"docs_name": "ABCD_ML.extensions.Feat_Selectors.FeatureSelector", "preset_params": ["random", "searchable"], "param_names": ["mask"], "default_param_types": ["str"], "default_params": ["'sets as random features'"], "options": [["'sets as random features'", "'sets as hyperparameters'"]], "descrs": ["    - 'sets as random features': Use random features.\n    - 'sets as hyperparameters': Each feature is set as a             hyperparameter, such that the parameter search can             tune if each feature is included or not.\n"]}, "univariate selection r": {"docs_name": "sklearn.feature_selection.SelectPercentile", "preset_params": ["base univar fs regression", "univar fs regression dist"], "param_names": ["percentile", "score_func"], "default_param_types": ["int", "str"], "default_params": ["10", "'<function f_classif at 0x7f4977e411e0>'"], "options": ["", ["f_regression", "mutual_info_regression", "chi2"]], "descrs": ["    Percent of features to keep.\n", "    Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues) or a single array with scores.\n    Default is f_classif (see below \"See also\"). The default function only\n    works with classification tasks.\n    .. versionadded:: 0.18\n"]}, "variance threshold": {"docs_name": "sklearn.feature_selection.VarianceThreshold", "preset_params": ["default"], "param_names": ["threshold"], "default_param_types": ["float"], "default_params": ["0.0"], "options": [""], "descrs": ["    Features with a training-set variance lower than this threshold will\n    be removed. The default is to keep all features with non-zero variance,\n    i.e. remove the features that have the same value in all samples.\n"]}}, "categorical": {"rfe": {"docs_name": "sklearn.feature_selection.RFE", "preset_params": ["base rfe", "rfe num feats dist"], "param_names": ["estimator", "n_features_to_select", "step", "verbose"], "default_param_types": ["str", "NoneType", "int", "int"], "default_params": ["\"<class 'inspect._empty'>\"", "None", "1", "0"], "options": ["", "", "", ""], "descrs": ["    A supervised learning estimator with a ``fit`` method that provides\n    information about feature importance either through a ``coef_``\n    attribute or through a ``feature_importances_`` attribute.\n", "   The number of features to select. If `None`, half of the features are selected. If a floating point number between 0 and 1, that percentage of the total features will be selected.", "    If greater than or equal to 1, then ``step`` corresponds to the\n    (integer) number of features to remove at each iteration.\n    If within (0.0, 1.0), then ``step`` corresponds to the percentage\n    (rounded down) of features to remove at each iteration.\n", "    Controls verbosity of output.\n"]}, "selector": {"docs_name": "ABCD_ML.extensions.Feat_Selectors.FeatureSelector", "preset_params": ["random", "searchable"], "param_names": ["mask"], "default_param_types": ["str"], "default_params": ["'sets as random features'"], "options": [["'sets as random features'", "'sets as hyperparameters'"]], "descrs": ["    - 'sets as random features': Use random features.\n    - 'sets as hyperparameters': Each feature is set as a             hyperparameter, such that the parameter search can             tune if each feature is included or not.\n"]}, "univariate selection c": {"docs_name": "sklearn.feature_selection.SelectPercentile", "preset_params": ["base univar fs classifier", "univar fs classifier dist"], "param_names": ["percentile", "score_func"], "default_param_types": ["int", "str"], "default_params": ["10", "'<function f_classif at 0x7f4977e411e0>'"], "options": ["", ["f_classif", "mutual_info_classif", "chi2"]], "descrs": ["    Percent of features to keep.\n", "    Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues) or a single array with scores.\n    Default is f_classif (see below \"See also\"). The default function only\n    works with classification tasks.\n    .. versionadded:: 0.18\n"]}, "variance threshold": {"docs_name": "sklearn.feature_selection.VarianceThreshold", "preset_params": ["default"], "param_names": ["threshold"], "default_param_types": ["float"], "default_params": ["0.0"], "options": [""], "descrs": ["    Features with a training-set variance lower than this threshold will\n    be removed. The default is to keep all features with non-zero variance,\n    i.e. remove the features that have the same value in all samples.\n"]}}}, "ensembles": {"binary": {"adaboost": {"docs_name": "sklearn.ensemble.AdaBoostClassifier", "preset_params": ["default"], "param_names": ["algorithm", "base_estimator", "learning_rate", "n_estimators", "random_state"], "default_param_types": ["str", "NoneType", "float", "int", "NoneType"], "default_params": ["'SAMME.R'", "None", "1.0", "50", "None"], "options": [["'SAMME'", "'SAMME.R'"], "", "", "", ""], "descrs": ["    If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n    ``base_estimator`` must support calculation of class probabilities.\n    If 'SAMME' then use the SAMME discrete boosting algorithm.\n    The SAMME.R algorithm typically converges faster than SAMME,\n    achieving a lower test error with fewer boosting iterations.\n", "    The base estimator from which the boosted ensemble is built.\n    Support for sample weighting is required, as well as proper\n    ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n    the base estimator is ``DecisionTreeClassifier(max_depth=1)``.\n", "    Learning rate shrinks the contribution of each classifier by\n    ``learning_rate``. There is a trade-off between ``learning_rate`` and\n    ``n_estimators``.\n", "    The maximum number of estimators at which boosting is terminated.\n    In case of perfect fit, the learning procedure is stopped early.\n", "    Controls the random seed given at each `base_estimator` at each\n    boosting iteration.\n    Thus, it is only used when `base_estimator` exposes a `random_state`.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n"]}, "aposteriori": {"docs_name": "deslib.dcs.a_posteriori.APosteriori", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "diff_thresh", "k", "knn_classifier", "pool_classifiers", "random_state", "safe_k", "selection_method", "with_IH"], "default_param_types": ["bool", "float", "float", "float", "int", "str", "NoneType", "NoneType", "NoneType", "str", "bool"], "default_params": ["False", "0.5", "0.3", "0.1", "7", "'knn'", "None", "None", "None", "'diff'", "False"], "options": [["True", "False"], "", "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Threshold to measure the difference between the competence level of the\n    base classifiers for the random and diff selection schemes. If the\n    difference is lower than the threshold, their performance are\n    considered equivalent.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "    The generated_pool of classifiers trained for the corresponding\n    classification problem. Each base classifiers should support the method\n    \"predict\" and \"predict_proba\". If None, then the pool of classifiers is\n    a bagging classifier.\n", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Determines which method is used to select the base classifier after\n    the competences are estimated.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "apriori": {"docs_name": "deslib.dcs.a_priori.APriori", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "diff_thresh", "k", "knn_classifier", "pool_classifiers", "random_state", "safe_k", "selection_method", "with_IH"], "default_param_types": ["bool", "float", "float", "float", "int", "str", "NoneType", "NoneType", "NoneType", "str", "bool"], "default_params": ["False", "0.5", "0.3", "0.1", "7", "'knn'", "None", "None", "None", "'diff'", "False"], "options": [["True", "False"], "", "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n", "    Threshold to measure the difference between the competence level of the\n    base classifiers for the random and diff selection schemes. If the\n    difference is lower than the threshold, their performance are\n    considered equivalent.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "    The generated_pool of classifiers trained for the corresponding\n    classification problem. Each base classifiers should support the method\n    \"predict\" and \"predict_proba\". If None, then the pool of classifiers is\n    a bagging classifier.\n", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Determines which method is used to select the base classifier after\n    the competences are estimated.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "bagging": {"docs_name": "sklearn.ensemble.BaggingClassifier", "preset_params": ["default"], "param_names": ["base_estimator", "bootstrap", "bootstrap_features", "max_features", "max_samples", "n_estimators", "n_jobs", "oob_score", "random_state", "verbose", "warm_start"], "default_param_types": ["NoneType", "bool", "bool", "float", "float", "int", "NoneType", "bool", "NoneType", "int", "bool"], "default_params": ["None", "True", "False", "1.0", "1.0", "10", "None", "False", "None", "0", "False"], "options": ["", ["True", "False"], ["True", "False"], "", "", "", "", ["True", "False"], "", "", ["True", "False"]], "descrs": ["    The base estimator to fit on random subsets of the dataset.\n    If None, then the base estimator is a decision tree.\n", "    Whether samples are drawn with replacement. If False, sampling\n    without replacement is performed.\n", "    Whether features are drawn with replacement.\n", "    The number of features to draw from X to train each base estimator (\n    without replacement by default, see `bootstrap_features` for more\n    details).\n    - If int, then draw `max_features` features.\n    - If float, then draw `max_features * X.shape[1]` features.\n", "    The number of samples to draw from X to train each base estimator (with\n    replacement by default, see `bootstrap` for more details).\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples.\n", "    The number of base estimators in the ensemble.\n", "    The number of jobs to run in parallel for both :meth:`fit` and\n    :meth:`predict`. ``None`` means 1 unless in a\n    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n    processors. See :term:`Glossary <n_jobs>` for more details.\n", "    Whether to use out-of-bag samples to estimate\n    the generalization error.\n", "    Controls the random resampling of the original dataset\n    (sample wise and feature wise).\n    If the base estimator accepts a `random_state` attribute, a different\n    seed is generated for each instance in the ensemble.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    Controls the verbosity when fitting and predicting.\n", "    When set to True, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit\n    a whole new ensemble. See :term:`the Glossary <warm_start>`.\n    .. versionadded:: 0.17\n       *warm_start* constructor parameter.\n"]}, "balanced bagging": {"docs_name": "imblearn.ensemble.BalancedBaggingClassifier", "preset_params": ["default"], "param_names": ["base_estimator", "bootstrap", "bootstrap_features", "max_features", "max_samples", "n_estimators", "n_jobs", "oob_score", "random_state", "replacement", "sampling_strategy", "verbose", "warm_start"], "default_param_types": ["NoneType", "bool", "bool", "float", "float", "int", "NoneType", "bool", "NoneType", "bool", "str", "int", "bool"], "default_params": ["None", "True", "False", "1.0", "1.0", "10", "None", "False", "None", "False", "'auto'", "0", "False"], "options": ["", ["True", "False"], ["True", "False"], "", "", "", "", ["True", "False"], "", ["True", "False"], "", "", ["True", "False"]], "descrs": ["    The base estimator to fit on random subsets of the dataset.\n    If None, then the base estimator is a decision tree.\n", "    Whether samples are drawn with replacement.\n", "    Whether features are drawn with replacement.\n", "    The number of features to draw from X to train each base estimator.\n    - If int, then draw ``max_features`` features.\n    - If float, then draw ``max_features * X.shape[1]`` features.\n", "    The number of samples to draw from X to train each base estimator.\n    - If int, then draw ``max_samples`` samples.\n    - If float, then draw ``max_samples * X.shape[0]`` samples.\n", "    The number of base estimators in the ensemble.\n", "    Number of CPU cores used during the cross-validation loop.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See\n    `Glossary <https://scikit-learn.org/stable/glossary.html#term-n-jobs>`_\n    for more details.\n", "    Whether to use out-of-bag samples to estimate\n    the generalization error.\n", "    Control the randomization of the algorithm.\n    - If int, ``random_state`` is the seed used by the random number\n      generator;\n    - If ``RandomState`` instance, random_state is the random number\n      generator;\n    - If ``None``, the random number generator is the ``RandomState``\n      instance used by ``np.random``.\n", "    Whether or not to sample randomly with replacement or not.\n", "    Sampling information to sample the data set.\n    - When ``float``, it corresponds to the desired ratio of the number of\n      samples in the minority class over the number of samples in the\n      majority class after resampling. Therefore, the ratio is expressed as\n      :math:`\\alpha_{us} = N_{m} / N_{rM}` where :math:`N_{m}` is the\n      number of samples in the minority class and\n      :math:`N_{rM}` is the number of samples in the majority class\n      after resampling.\n      .. warning::\n         ``float`` is only available for **binary** classification. An\n         error is raised for multi-class classification.\n    - When ``str``, specify the class targeted by the resampling. The\n      number of samples in the different classes will be equalized.\n      Possible choices are:\n        ``'majority'``: resample only the majority class;\n        ``'not minority'``: resample all classes but the minority class;\n        ``'not majority'``: resample all classes but the majority class;\n        ``'all'``: resample all classes;\n        ``'auto'``: equivalent to ``'not minority'``.\n    - When ``dict``, the keys correspond to the targeted classes. The\n      values correspond to the desired number of samples for each targeted\n      class.\n    - When callable, function taking ``y`` and returns a ``dict``. The keys\n      correspond to the targeted classes. The values correspond to the\n      desired number of samples for each class.\n", "    Controls the verbosity of the building process.\n", "    When set to True, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit\n    a whole new ensemble.\n"]}, "des clustering": {"docs_name": "deslib.des.des_clustering.DESClustering", "preset_params": ["default"], "param_names": ["DSEL_perc", "IH_rate", "clustering", "metric", "more_diverse", "n_clusters", "pct_accuracy", "pct_diversity", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["float", "float", "NoneType", "str", "bool", "int", "float", "float", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["0.5", "0.3", "None", "'DF'", "True", "5", "0.5", "0.33", "None", "None", "None", "False"], "options": ["", "", "", "", ["True", "False"], "", "", "", "", "", "", ["True", "False"]], "descrs": ["    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "", "    The clustering model used to estimate the region of competence.\n    If None, a KMeans with K = 5 is used.\n", "    Metric used to estimate the diversity of the base classifiers. Can be\n    either the double fault (df), Q-statistics (Q), or error correlation.\n", "               Whether we select the most or the least diverse classifiers\n               to add to the pre-selected ensemble\n", "", "               Percentage of base classifiers selected based on accuracy\n", "                Percentage of base classifiers selected based n diversity\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "", ""]}, "des knn": {"docs_name": "deslib.des.des_knn.DESKNN", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "k", "knn_classifier", "metric", "more_diverse", "pct_accuracy", "pct_diversity", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["bool", "float", "float", "int", "str", "str", "bool", "float", "float", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["False", "0.5", "0.3", "7", "'knn'", "'DF'", "True", "0.5", "0.3", "None", "None", "None", "False"], "options": [["True", "False"], "", "", "", ["'knn'", "'faiss'", "None"], "", ["True", "False"], "", "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "    Metric used to estimate the diversity of the base classifiers. Can be\n    either the double fault (df), Q-statistics (Q), or error correlation.\n", "    Whether we select the most or the least diverse classifiers to add\n    to the pre-selected ensemble\n", "               Percentage of base classifiers selected based on accuracy\n", "                Percentage of base classifiers selected based n diversity\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "deskl": {"docs_name": "deslib.des.probabilistic.DESKL", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "k", "knn_classifier", "mode", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["bool", "float", "float", "NoneType", "str", "str", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["False", "0.5", "0.3", "None", "'knn'", "'selection'", "None", "None", "None", "False"], "options": [["True", "False"], "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "       Whether the technique will perform dynamic selection,\n       dynamic weighting or an hybrid approach for classification.\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "desmi": {"docs_name": "deslib.des.des_mi.DESMI", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "alpha", "k", "knn_classifier", "pct_accuracy", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["bool", "float", "float", "float", "int", "str", "float", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["False", "0.5", "0.3", "0.9", "7", "'knn'", "0.4", "None", "None", "None", "False"], "options": [["True", "False"], "", "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "        Scaling coefficient to regulate the weight value\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "desp": {"docs_name": "deslib.des.des_p.DESP", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "k", "knn_classifier", "mode", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["bool", "float", "float", "int", "str", "str", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["False", "0.5", "0.3", "7", "'knn'", "'selection'", "None", "None", "None", "False"], "options": [["True", "False"], "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "       Whether the technique will perform dynamic selection,\n       dynamic weighting or an hybrid approach for classification.\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "exponential": {"docs_name": "deslib.des.probabilistic.Exponential", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "k", "knn_classifier", "mode", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["bool", "float", "float", "NoneType", "str", "str", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["False", "0.5", "0.3", "None", "'knn'", "'selection'", "None", "None", "None", "False"], "options": [["True", "False"], "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "       Whether the technique will perform dynamic selection,\n       dynamic weighting or an hybrid approach for classification.\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "knop": {"docs_name": "deslib.des.knop.KNOP", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "k", "knn_classifier", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["bool", "float", "float", "int", "str", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["False", "0.5", "0.3", "7", "'knn'", "None", "None", "None", "False"], "options": [["True", "False"], "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "knorae": {"docs_name": "deslib.des.knora_e.KNORAE", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "k", "knn_classifier", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["bool", "float", "float", "int", "str", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["False", "0.5", "0.3", "7", "'knn'", "None", "None", "None", "False"], "options": [["True", "False"], "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "knrau": {"docs_name": "deslib.des.knora_u.KNORAU", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "k", "knn_classifier", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["bool", "float", "float", "int", "str", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["False", "0.5", "0.3", "7", "'knn'", "None", "None", "None", "False"], "options": [["True", "False"], "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "lca": {"docs_name": "deslib.dcs.lca.LCA", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "diff_thresh", "k", "knn_classifier", "pool_classifiers", "random_state", "safe_k", "selection_method", "with_IH"], "default_param_types": ["bool", "float", "float", "float", "int", "str", "NoneType", "NoneType", "NoneType", "str", "bool"], "default_params": ["False", "0.5", "0.3", "0.1", "7", "'knn'", "None", "None", "None", "'best'", "False"], "options": [["True", "False"], "", "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Threshold to measure the difference between the competence level of the\n    base classifiers for the random and diff selection schemes. If the\n    difference is lower than the threshold, their performance are\n    considered equivalent.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "    The generated_pool of classifiers trained for the corresponding\n    classification problem. Each base classifiers should support the method\n    \"predict\". If None, then the pool of classifiers is a bagging\n    classifier.\n", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Determines which method is used to select the base classifier after\n    the competences are estimated.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "logarithmic": {"docs_name": "deslib.des.probabilistic.Logarithmic", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "k", "knn_classifier", "mode", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["bool", "float", "float", "NoneType", "str", "str", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["False", "0.5", "0.3", "None", "'knn'", "'selection'", "None", "None", "None", "False"], "options": [["True", "False"], "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "       Whether the technique will perform dynamic selection,\n       dynamic weighting or an hybrid approach for classification.\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "mcb": {"docs_name": "deslib.dcs.mcb.MCB", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "diff_thresh", "k", "knn_classifier", "pool_classifiers", "random_state", "safe_k", "selection_method", "similarity_threshold", "with_IH"], "default_param_types": ["bool", "float", "float", "float", "int", "str", "NoneType", "NoneType", "NoneType", "str", "float", "bool"], "default_params": ["False", "0.5", "0.3", "0.1", "7", "'knn'", "None", "None", "None", "'diff'", "0.7", "False"], "options": [["True", "False"], "", "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Threshold to measure the difference between the competence level of the\n    base classifiers for the random and diff selection schemes. If the\n    difference is lower than the threshold, their performance are\n    considered equivalent.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "    The generated_pool of classifiers trained for the corresponding\n    classification problem. Each base classifiers should support the method\n    \"predict\". If None, then the pool of classifiers is a bagging\n    classifier.\n", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Determines which method is used to select the base classifier after\n    the competences are estimated.\n", "", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "metades": {"docs_name": "deslib.des.meta_des.METADES", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "Hc", "IH_rate", "Kp", "k", "knn_classifier", "meta_classifier", "mode", "pool_classifiers", "random_state", "safe_k", "selection_threshold", "with_IH"], "default_param_types": ["bool", "float", "float", "float", "int", "int", "str", "NoneType", "str", "NoneType", "NoneType", "NoneType", "float", "bool"], "default_params": ["False", "0.5", "1.0", "0.3", "5", "7", "'knn'", "None", "'selection'", "None", "None", "None", "0.5", "False"], "options": [["True", "False"], "", "", "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "     Sample selection threshold.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "     Number of output profiles used to estimate the competence of the\n     base classifiers.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "                    Classifier model used for the meta-classifier. If None,\n                    a Multinomial naive Bayes classifier is used.\n", "    Determines the mode of META-des that is used\n    (selection, weighting or hybrid).\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Threshold used to select the base classifier. Only the base classifiers\n    with competence level higher than the selection_threshold are selected\n    to compose the ensemble.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "min dif": {"docs_name": "deslib.des.probabilistic.MinimumDifference", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "k", "knn_classifier", "mode", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["bool", "float", "float", "NoneType", "str", "str", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["False", "0.5", "0.3", "None", "'knn'", "'selection'", "None", "None", "None", "False"], "options": [["True", "False"], "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "       Whether the technique will perform dynamic selection,\n       dynamic weighting or an hybrid approach for classification.\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "mla": {"docs_name": "deslib.dcs.mla.MLA", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "diff_thresh", "k", "knn_classifier", "pool_classifiers", "random_state", "safe_k", "selection_method", "with_IH"], "default_param_types": ["bool", "float", "float", "float", "int", "str", "NoneType", "NoneType", "NoneType", "str", "bool"], "default_params": ["False", "0.5", "0.3", "0.1", "7", "'knn'", "None", "None", "None", "'best'", "False"], "options": [["True", "False"], "", "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Threshold to measure the difference between the competence level of the\n    base classifiers for the random and diff selection schemes. If the\n    difference is lower than the threshold, their performance are\n    considered equivalent.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "    The generated_pool of classifiers trained for the corresponding\n    classification problem. Each base classifiers should support the method\n    \"predict\". If None, then the pool of classifiers is a bagging\n    classifier.\n", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Determines which method is used to select the base classifier after\n    the competences are estimated.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "ola": {"docs_name": "deslib.dcs.ola.OLA", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "diff_thresh", "k", "knn_classifier", "pool_classifiers", "random_state", "safe_k", "selection_method", "with_IH"], "default_param_types": ["bool", "float", "float", "float", "int", "str", "NoneType", "NoneType", "NoneType", "str", "bool"], "default_params": ["False", "0.5", "0.3", "0.1", "7", "'knn'", "None", "None", "None", "'best'", "False"], "options": [["True", "False"], "", "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Threshold to measure the difference between the competence level of the\n    base classifiers for the random and diff selection schemes. If the\n    difference is lower than the threshold, their performance are\n    considered equivalent.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "    The generated_pool of classifiers trained for the corresponding\n    classification problem. Each base classifiers should support the method\n    \"predict\". If None, then the pool of classifiers is a bagging\n    classifier.\n", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Determines which method is used to select the base classifier after\n    the competences are estimated.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "rank": {"docs_name": "deslib.dcs.rank.Rank", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "diff_thresh", "k", "knn_classifier", "pool_classifiers", "random_state", "safe_k", "selection_method", "with_IH"], "default_param_types": ["bool", "float", "float", "float", "int", "str", "NoneType", "NoneType", "NoneType", "str", "bool"], "default_params": ["False", "0.5", "0.3", "0.1", "7", "'knn'", "None", "None", "None", "'best'", "False"], "options": [["True", "False"], "", "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Threshold to measure the difference between the competence level of the\n    base classifiers for the random and diff selection schemes. If the\n    difference is lower than the threshold, their performance are\n    considered equivalent.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "    The generated_pool of classifiers trained for the corresponding\n    classification problem. Each base classifiers should support the method\n    \"predict\". If None, then the pool of classifiers is a bagging\n    classifier.\n", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Determines which method is used to select the base classifier after\n    the competences are estimated.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "rrc": {"docs_name": "deslib.des.probabilistic.RRC", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "k", "knn_classifier", "mode", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["bool", "float", "float", "NoneType", "str", "str", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["False", "0.5", "0.3", "None", "'knn'", "'selection'", "None", "None", "None", "False"], "options": [["True", "False"], "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "       Whether the technique will perform dynamic selection,\n       dynamic weighting or an hybrid approach for classification.\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "single best": {"docs_name": "deslib.static.single_best.SingleBest", "preset_params": ["default"], "param_names": ["pool_classifiers", "random_state"], "default_param_types": ["NoneType", "NoneType"], "default_params": ["None", "None"], "options": ["", ""], "descrs": ["    The generated_pool of classifiers trained for the corresponding\n    classification problem. Each base classifiers should support the method\n    \"predict\". If None, then the pool of classifiers is a bagging\n    classifier.\n", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n"]}, "stacked": {"docs_name": "deslib.static.stacked.StackedClassifier", "preset_params": ["default"], "param_names": ["meta_classifier", "pool_classifiers", "random_state"], "default_param_types": ["NoneType", "NoneType", "NoneType"], "default_params": ["None", "None", "None"], "options": ["", "", ""], "descrs": ["    Classifier model used to aggregate the output of the base classifiers.\n    If None, a :class:`LogisticRegression` classifier is used.\n", "    The generated_pool of classifiers trained for the corresponding\n    classification problem. Each base classifiers should support the method\n    \"predict\". If None, then the pool of classifiers is a bagging\n    classifier.\n", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n"]}, "stacking": {"docs_name": "sklearn.ensemble.StackingClassifier", "preset_params": ["default"], "param_names": ["cv", "estimators", "final_estimator", "n_jobs", "passthrough", "stack_method", "verbose"], "default_param_types": ["NoneType", "str", "NoneType", "NoneType", "bool", "str", "int"], "default_params": ["None", "\"<class 'inspect._empty'>\"", "None", "None", "False", "'auto'", "0"], "options": ["", "", "", "", ["True", "False"], ["'auto'", "'predict_proba'", "'decision_function'", "'predict'"], ""], "descrs": ["    Determines the cross-validation splitting strategy used in\n    `cross_val_predict` to train `final_estimator`. Possible inputs for\n    cv are:\n    * None, to use the default 5-fold cross validation,\n    * integer, to specify the number of folds in a (Stratified) KFold,\n    * An object to be used as a cross-validation generator,\n    * An iterable yielding train, test splits.\n    For integer/None inputs, if the estimator is a classifier and y is\n    either binary or multiclass, `StratifiedKFold` is used. In all other\n    cases, `KFold` is used.\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n    .. note::\n       A larger number of split will provide no benefits if the number\n       of training samples is large enough. Indeed, the training time\n       will increase. ``cv`` is not used for model evaluation but for\n       prediction.\n", "    Base estimators which will be stacked together. Each element of the\n    list is defined as a tuple of string (i.e. name) and an estimator\n    instance. An estimator can be set to 'drop' using `set_params`.\n", "    A classifier which will be used to combine the base estimators.\n    The default classifier is a `LogisticRegression`.\n", "    The number of jobs to run in parallel all `estimators` `fit`.\n    `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n    using all processors. See Glossary for more details.\n", "    When False, only the predictions of estimators will be used as\n    training data for `final_estimator`. When True, the\n    `final_estimator` is trained on the predictions as well as the\n    original training data.\n", "    Methods called for each base estimator. It can be:\n    * if 'auto', it will try to invoke, for each estimator,\n      `'predict_proba'`, `'decision_function'` or `'predict'` in that\n      order.\n    * otherwise, one of `'predict_proba'`, `'decision_function'` or\n      `'predict'`. If the method is not implemented by the estimator, it\n      will raise an error.\n", "    Verbosity level.\n"]}, "voting": {"docs_name": "sklearn.ensemble.VotingClassifier", "preset_params": ["default"], "param_names": ["estimators", "flatten_transform", "n_jobs", "verbose", "voting", "weights"], "default_param_types": ["str", "bool", "NoneType", "bool", "str", "NoneType"], "default_params": ["\"<class 'inspect._empty'>\"", "True", "None", "False", "'hard'", "None"], "options": ["", ["True", "False"], "", ["True", "False"], ["'hard'", "'soft'"], ""], "descrs": ["    Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones\n    of those original estimators that will be stored in the class attribute\n    ``self.estimators_``. An estimator can be set to ``'drop'``\n    using ``set_params``.\n    .. versionchanged:: 0.21\n        ``'drop'`` is accepted.\n    .. deprecated:: 0.22\n       Using ``None`` to drop an estimator is deprecated in 0.22 and\n       support will be dropped in 0.24. Use the string ``'drop'`` instead.\n", "    Affects shape of transform output only when voting='soft'\n    If voting='soft' and flatten_transform=True, transform method returns\n    matrix with shape (n_samples, n_classifiers * n_classes). If\n    flatten_transform=False, it returns\n    (n_classifiers, n_samples, n_classes).\n", "    The number of jobs to run in parallel for ``fit``.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n    .. versionadded:: 0.18\n", "    If True, the time elapsed while fitting will be printed as it\n    is completed.\n", "    If 'hard', uses predicted class labels for majority rule voting.\n    Else if 'soft', predicts the class label based on the argmax of\n    the sums of the predicted probabilities, which is recommended for\n    an ensemble of well-calibrated classifiers.\n", "    Sequence of weights (`float` or `int`) to weight the occurrences of\n    predicted class labels (`hard` voting) or class probabilities\n    before averaging (`soft` voting). Uses uniform weights if `None`.\n"]}}, "regression": {"adaboost": {"docs_name": "sklearn.ensemble.AdaBoostRegressor", "preset_params": ["default"], "param_names": ["base_estimator", "learning_rate", "loss", "n_estimators", "random_state"], "default_param_types": ["NoneType", "float", "str", "int", "NoneType"], "default_params": ["None", "1.0", "'linear'", "50", "None"], "options": ["", "", ["'linear'", "'square'", "'exponential'"], "", ""], "descrs": ["    The base estimator from which the boosted ensemble is built.\n    If ``None``, then the base estimator is\n    ``DecisionTreeRegressor(max_depth=3)``.\n", "    Learning rate shrinks the contribution of each regressor by\n    ``learning_rate``. There is a trade-off between ``learning_rate`` and\n    ``n_estimators``.\n", "    The loss function to use when updating the weights after each\n    boosting iteration.\n", "    The maximum number of estimators at which boosting is terminated.\n    In case of perfect fit, the learning procedure is stopped early.\n", "    Controls the random seed given at each `base_estimator` at each\n    boosting iteration.\n    Thus, it is only used when `base_estimator` exposes a `random_state`.\n    In addition, it controls the bootstrap of the weights used to train the\n    `base_estimator` at each boosting iteration.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n"]}, "bagging": {"docs_name": "sklearn.ensemble.BaggingRegressor", "preset_params": ["default"], "param_names": ["base_estimator", "bootstrap", "bootstrap_features", "max_features", "max_samples", "n_estimators", "n_jobs", "oob_score", "random_state", "verbose", "warm_start"], "default_param_types": ["NoneType", "bool", "bool", "float", "float", "int", "NoneType", "bool", "NoneType", "int", "bool"], "default_params": ["None", "True", "False", "1.0", "1.0", "10", "None", "False", "None", "0", "False"], "options": ["", ["True", "False"], ["True", "False"], "", "", "", "", ["True", "False"], "", "", ["True", "False"]], "descrs": ["    The base estimator to fit on random subsets of the dataset.\n    If None, then the base estimator is a decision tree.\n", "    Whether samples are drawn with replacement. If False, sampling\n    without replacement is performed.\n", "    Whether features are drawn with replacement.\n", "    The number of features to draw from X to train each base estimator (\n    without replacement by default, see `bootstrap_features` for more\n    details).\n    - If int, then draw `max_features` features.\n    - If float, then draw `max_features * X.shape[1]` features.\n", "    The number of samples to draw from X to train each base estimator (with\n    replacement by default, see `bootstrap` for more details).\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples.\n", "    The number of base estimators in the ensemble.\n", "    The number of jobs to run in parallel for both :meth:`fit` and\n    :meth:`predict`. ``None`` means 1 unless in a\n    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n    processors. See :term:`Glossary <n_jobs>` for more details.\n", "    Whether to use out-of-bag samples to estimate\n    the generalization error.\n", "    Controls the random resampling of the original dataset\n    (sample wise and feature wise).\n    If the base estimator accepts a `random_state` attribute, a different\n    seed is generated for each instance in the ensemble.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    Controls the verbosity when fitting and predicting.\n", "    When set to True, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit\n    a whole new ensemble. See :term:`the Glossary <warm_start>`.\n"]}, "stacking": {"docs_name": "sklearn.ensemble.StackingRegressor", "preset_params": ["default"], "param_names": ["cv", "estimators", "final_estimator", "n_jobs", "passthrough", "verbose"], "default_param_types": ["NoneType", "str", "NoneType", "NoneType", "bool", "int"], "default_params": ["None", "\"<class 'inspect._empty'>\"", "None", "None", "False", "0"], "options": ["", "", "", "", ["True", "False"], ""], "descrs": ["    Determines the cross-validation splitting strategy used in\n    `cross_val_predict` to train `final_estimator`. Possible inputs for\n    cv are:\n    * None, to use the default 5-fold cross validation,\n    * integer, to specify the number of folds in a (Stratified) KFold,\n    * An object to be used as a cross-validation generator,\n    * An iterable yielding train, test splits.\n    For integer/None inputs, if the estimator is a classifier and y is\n    either binary or multiclass, `StratifiedKFold` is used. In all other\n    cases, `KFold` is used.\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n    .. note::\n       A larger number of split will provide no benefits if the number\n       of training samples is large enough. Indeed, the training time\n       will increase. ``cv`` is not used for model evaluation but for\n       prediction.\n", "    Base estimators which will be stacked together. Each element of the\n    list is defined as a tuple of string (i.e. name) and an estimator\n    instance. An estimator can be set to 'drop' using `set_params`.\n", "    A regressor which will be used to combine the base estimators.\n    The default regressor is a `RidgeCV`.\n", "    The number of jobs to run in parallel for `fit` of all `estimators`.\n    `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n    using all processors. See Glossary for more details.\n", "    When False, only the predictions of estimators will be used as\n    training data for `final_estimator`. When True, the\n    `final_estimator` is trained on the predictions as well as the\n    original training data.\n", "    Verbosity level.\n"]}, "voting": {"docs_name": "sklearn.ensemble.VotingRegressor", "preset_params": ["default"], "param_names": ["estimators", "n_jobs", "verbose", "weights"], "default_param_types": ["str", "NoneType", "bool", "NoneType"], "default_params": ["\"<class 'inspect._empty'>\"", "None", "False", "None"], "options": ["", "", ["True", "False"], ""], "descrs": ["    Invoking the ``fit`` method on the ``VotingRegressor`` will fit clones\n    of those original estimators that will be stored in the class attribute\n    ``self.estimators_``. An estimator can be set to ``'drop'`` using\n    ``set_params``.\n    .. versionchanged:: 0.21\n        ``'drop'`` is accepted.\n    .. deprecated:: 0.22\n       Using ``None`` to drop an estimator is deprecated in 0.22 and\n       support will be dropped in 0.24. Use the string ``'drop'`` instead.\n", "    The number of jobs to run in parallel for ``fit``.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n", "    If True, the time elapsed while fitting will be printed as it\n    is completed.\n", "    Sequence of weights (`float` or `int`) to weight the occurrences of\n    predicted values before averaging. Uses uniform weights if `None`.\n"]}}, "categorical": {"adaboost": {"docs_name": "sklearn.ensemble.AdaBoostClassifier", "preset_params": ["default"], "param_names": ["algorithm", "base_estimator", "learning_rate", "n_estimators", "random_state"], "default_param_types": ["str", "NoneType", "float", "int", "NoneType"], "default_params": ["'SAMME.R'", "None", "1.0", "50", "None"], "options": [["'SAMME'", "'SAMME.R'"], "", "", "", ""], "descrs": ["    If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n    ``base_estimator`` must support calculation of class probabilities.\n    If 'SAMME' then use the SAMME discrete boosting algorithm.\n    The SAMME.R algorithm typically converges faster than SAMME,\n    achieving a lower test error with fewer boosting iterations.\n", "    The base estimator from which the boosted ensemble is built.\n    Support for sample weighting is required, as well as proper\n    ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n    the base estimator is ``DecisionTreeClassifier(max_depth=1)``.\n", "    Learning rate shrinks the contribution of each classifier by\n    ``learning_rate``. There is a trade-off between ``learning_rate`` and\n    ``n_estimators``.\n", "    The maximum number of estimators at which boosting is terminated.\n    In case of perfect fit, the learning procedure is stopped early.\n", "    Controls the random seed given at each `base_estimator` at each\n    boosting iteration.\n    Thus, it is only used when `base_estimator` exposes a `random_state`.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n"]}, "aposteriori": {"docs_name": "deslib.dcs.a_posteriori.APosteriori", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "diff_thresh", "k", "knn_classifier", "pool_classifiers", "random_state", "safe_k", "selection_method", "with_IH"], "default_param_types": ["bool", "float", "float", "float", "int", "str", "NoneType", "NoneType", "NoneType", "str", "bool"], "default_params": ["False", "0.5", "0.3", "0.1", "7", "'knn'", "None", "None", "None", "'diff'", "False"], "options": [["True", "False"], "", "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Threshold to measure the difference between the competence level of the\n    base classifiers for the random and diff selection schemes. If the\n    difference is lower than the threshold, their performance are\n    considered equivalent.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "    The generated_pool of classifiers trained for the corresponding\n    classification problem. Each base classifiers should support the method\n    \"predict\" and \"predict_proba\". If None, then the pool of classifiers is\n    a bagging classifier.\n", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Determines which method is used to select the base classifier after\n    the competences are estimated.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "apriori": {"docs_name": "deslib.dcs.a_priori.APriori", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "diff_thresh", "k", "knn_classifier", "pool_classifiers", "random_state", "safe_k", "selection_method", "with_IH"], "default_param_types": ["bool", "float", "float", "float", "int", "str", "NoneType", "NoneType", "NoneType", "str", "bool"], "default_params": ["False", "0.5", "0.3", "0.1", "7", "'knn'", "None", "None", "None", "'diff'", "False"], "options": [["True", "False"], "", "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n", "    Threshold to measure the difference between the competence level of the\n    base classifiers for the random and diff selection schemes. If the\n    difference is lower than the threshold, their performance are\n    considered equivalent.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "    The generated_pool of classifiers trained for the corresponding\n    classification problem. Each base classifiers should support the method\n    \"predict\" and \"predict_proba\". If None, then the pool of classifiers is\n    a bagging classifier.\n", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Determines which method is used to select the base classifier after\n    the competences are estimated.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "bagging": {"docs_name": "sklearn.ensemble.BaggingClassifier", "preset_params": ["default"], "param_names": ["base_estimator", "bootstrap", "bootstrap_features", "max_features", "max_samples", "n_estimators", "n_jobs", "oob_score", "random_state", "verbose", "warm_start"], "default_param_types": ["NoneType", "bool", "bool", "float", "float", "int", "NoneType", "bool", "NoneType", "int", "bool"], "default_params": ["None", "True", "False", "1.0", "1.0", "10", "None", "False", "None", "0", "False"], "options": ["", ["True", "False"], ["True", "False"], "", "", "", "", ["True", "False"], "", "", ["True", "False"]], "descrs": ["    The base estimator to fit on random subsets of the dataset.\n    If None, then the base estimator is a decision tree.\n", "    Whether samples are drawn with replacement. If False, sampling\n    without replacement is performed.\n", "    Whether features are drawn with replacement.\n", "    The number of features to draw from X to train each base estimator (\n    without replacement by default, see `bootstrap_features` for more\n    details).\n    - If int, then draw `max_features` features.\n    - If float, then draw `max_features * X.shape[1]` features.\n", "    The number of samples to draw from X to train each base estimator (with\n    replacement by default, see `bootstrap` for more details).\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples.\n", "    The number of base estimators in the ensemble.\n", "    The number of jobs to run in parallel for both :meth:`fit` and\n    :meth:`predict`. ``None`` means 1 unless in a\n    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n    processors. See :term:`Glossary <n_jobs>` for more details.\n", "    Whether to use out-of-bag samples to estimate\n    the generalization error.\n", "    Controls the random resampling of the original dataset\n    (sample wise and feature wise).\n    If the base estimator accepts a `random_state` attribute, a different\n    seed is generated for each instance in the ensemble.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n", "    Controls the verbosity when fitting and predicting.\n", "    When set to True, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit\n    a whole new ensemble. See :term:`the Glossary <warm_start>`.\n    .. versionadded:: 0.17\n       *warm_start* constructor parameter.\n"]}, "balanced bagging": {"docs_name": "imblearn.ensemble.BalancedBaggingClassifier", "preset_params": ["default"], "param_names": ["base_estimator", "bootstrap", "bootstrap_features", "max_features", "max_samples", "n_estimators", "n_jobs", "oob_score", "random_state", "replacement", "sampling_strategy", "verbose", "warm_start"], "default_param_types": ["NoneType", "bool", "bool", "float", "float", "int", "NoneType", "bool", "NoneType", "bool", "str", "int", "bool"], "default_params": ["None", "True", "False", "1.0", "1.0", "10", "None", "False", "None", "False", "'auto'", "0", "False"], "options": ["", ["True", "False"], ["True", "False"], "", "", "", "", ["True", "False"], "", ["True", "False"], "", "", ["True", "False"]], "descrs": ["    The base estimator to fit on random subsets of the dataset.\n    If None, then the base estimator is a decision tree.\n", "    Whether samples are drawn with replacement.\n", "    Whether features are drawn with replacement.\n", "    The number of features to draw from X to train each base estimator.\n    - If int, then draw ``max_features`` features.\n    - If float, then draw ``max_features * X.shape[1]`` features.\n", "    The number of samples to draw from X to train each base estimator.\n    - If int, then draw ``max_samples`` samples.\n    - If float, then draw ``max_samples * X.shape[0]`` samples.\n", "    The number of base estimators in the ensemble.\n", "    Number of CPU cores used during the cross-validation loop.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See\n    `Glossary <https://scikit-learn.org/stable/glossary.html#term-n-jobs>`_\n    for more details.\n", "    Whether to use out-of-bag samples to estimate\n    the generalization error.\n", "    Control the randomization of the algorithm.\n    - If int, ``random_state`` is the seed used by the random number\n      generator;\n    - If ``RandomState`` instance, random_state is the random number\n      generator;\n    - If ``None``, the random number generator is the ``RandomState``\n      instance used by ``np.random``.\n", "    Whether or not to sample randomly with replacement or not.\n", "    Sampling information to sample the data set.\n    - When ``float``, it corresponds to the desired ratio of the number of\n      samples in the minority class over the number of samples in the\n      majority class after resampling. Therefore, the ratio is expressed as\n      :math:`\\alpha_{us} = N_{m} / N_{rM}` where :math:`N_{m}` is the\n      number of samples in the minority class and\n      :math:`N_{rM}` is the number of samples in the majority class\n      after resampling.\n      .. warning::\n         ``float`` is only available for **binary** classification. An\n         error is raised for multi-class classification.\n    - When ``str``, specify the class targeted by the resampling. The\n      number of samples in the different classes will be equalized.\n      Possible choices are:\n        ``'majority'``: resample only the majority class;\n        ``'not minority'``: resample all classes but the minority class;\n        ``'not majority'``: resample all classes but the majority class;\n        ``'all'``: resample all classes;\n        ``'auto'``: equivalent to ``'not minority'``.\n    - When ``dict``, the keys correspond to the targeted classes. The\n      values correspond to the desired number of samples for each targeted\n      class.\n    - When callable, function taking ``y`` and returns a ``dict``. The keys\n      correspond to the targeted classes. The values correspond to the\n      desired number of samples for each class.\n", "    Controls the verbosity of the building process.\n", "    When set to True, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit\n    a whole new ensemble.\n"]}, "des clustering": {"docs_name": "deslib.des.des_clustering.DESClustering", "preset_params": ["default"], "param_names": ["DSEL_perc", "IH_rate", "clustering", "metric", "more_diverse", "n_clusters", "pct_accuracy", "pct_diversity", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["float", "float", "NoneType", "str", "bool", "int", "float", "float", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["0.5", "0.3", "None", "'DF'", "True", "5", "0.5", "0.33", "None", "None", "None", "False"], "options": ["", "", "", "", ["True", "False"], "", "", "", "", "", "", ["True", "False"]], "descrs": ["    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "", "    The clustering model used to estimate the region of competence.\n    If None, a KMeans with K = 5 is used.\n", "    Metric used to estimate the diversity of the base classifiers. Can be\n    either the double fault (df), Q-statistics (Q), or error correlation.\n", "               Whether we select the most or the least diverse classifiers\n               to add to the pre-selected ensemble\n", "", "               Percentage of base classifiers selected based on accuracy\n", "                Percentage of base classifiers selected based n diversity\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "", ""]}, "des knn": {"docs_name": "deslib.des.des_knn.DESKNN", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "k", "knn_classifier", "metric", "more_diverse", "pct_accuracy", "pct_diversity", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["bool", "float", "float", "int", "str", "str", "bool", "float", "float", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["False", "0.5", "0.3", "7", "'knn'", "'DF'", "True", "0.5", "0.3", "None", "None", "None", "False"], "options": [["True", "False"], "", "", "", ["'knn'", "'faiss'", "None"], "", ["True", "False"], "", "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "    Metric used to estimate the diversity of the base classifiers. Can be\n    either the double fault (df), Q-statistics (Q), or error correlation.\n", "    Whether we select the most or the least diverse classifiers to add\n    to the pre-selected ensemble\n", "               Percentage of base classifiers selected based on accuracy\n", "                Percentage of base classifiers selected based n diversity\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "deskl": {"docs_name": "deslib.des.probabilistic.DESKL", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "k", "knn_classifier", "mode", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["bool", "float", "float", "NoneType", "str", "str", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["False", "0.5", "0.3", "None", "'knn'", "'selection'", "None", "None", "None", "False"], "options": [["True", "False"], "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "       Whether the technique will perform dynamic selection,\n       dynamic weighting or an hybrid approach for classification.\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "desmi": {"docs_name": "deslib.des.des_mi.DESMI", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "alpha", "k", "knn_classifier", "pct_accuracy", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["bool", "float", "float", "float", "int", "str", "float", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["False", "0.5", "0.3", "0.9", "7", "'knn'", "0.4", "None", "None", "None", "False"], "options": [["True", "False"], "", "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "        Scaling coefficient to regulate the weight value\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "desp": {"docs_name": "deslib.des.des_p.DESP", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "k", "knn_classifier", "mode", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["bool", "float", "float", "int", "str", "str", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["False", "0.5", "0.3", "7", "'knn'", "'selection'", "None", "None", "None", "False"], "options": [["True", "False"], "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "       Whether the technique will perform dynamic selection,\n       dynamic weighting or an hybrid approach for classification.\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "exponential": {"docs_name": "deslib.des.probabilistic.Exponential", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "k", "knn_classifier", "mode", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["bool", "float", "float", "NoneType", "str", "str", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["False", "0.5", "0.3", "None", "'knn'", "'selection'", "None", "None", "None", "False"], "options": [["True", "False"], "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "       Whether the technique will perform dynamic selection,\n       dynamic weighting or an hybrid approach for classification.\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "knop": {"docs_name": "deslib.des.knop.KNOP", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "k", "knn_classifier", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["bool", "float", "float", "int", "str", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["False", "0.5", "0.3", "7", "'knn'", "None", "None", "None", "False"], "options": [["True", "False"], "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "knorae": {"docs_name": "deslib.des.knora_e.KNORAE", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "k", "knn_classifier", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["bool", "float", "float", "int", "str", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["False", "0.5", "0.3", "7", "'knn'", "None", "None", "None", "False"], "options": [["True", "False"], "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "knrau": {"docs_name": "deslib.des.knora_u.KNORAU", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "k", "knn_classifier", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["bool", "float", "float", "int", "str", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["False", "0.5", "0.3", "7", "'knn'", "None", "None", "None", "False"], "options": [["True", "False"], "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "lca": {"docs_name": "deslib.dcs.lca.LCA", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "diff_thresh", "k", "knn_classifier", "pool_classifiers", "random_state", "safe_k", "selection_method", "with_IH"], "default_param_types": ["bool", "float", "float", "float", "int", "str", "NoneType", "NoneType", "NoneType", "str", "bool"], "default_params": ["False", "0.5", "0.3", "0.1", "7", "'knn'", "None", "None", "None", "'best'", "False"], "options": [["True", "False"], "", "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Threshold to measure the difference between the competence level of the\n    base classifiers for the random and diff selection schemes. If the\n    difference is lower than the threshold, their performance are\n    considered equivalent.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "    The generated_pool of classifiers trained for the corresponding\n    classification problem. Each base classifiers should support the method\n    \"predict\". If None, then the pool of classifiers is a bagging\n    classifier.\n", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Determines which method is used to select the base classifier after\n    the competences are estimated.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "logarithmic": {"docs_name": "deslib.des.probabilistic.Logarithmic", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "k", "knn_classifier", "mode", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["bool", "float", "float", "NoneType", "str", "str", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["False", "0.5", "0.3", "None", "'knn'", "'selection'", "None", "None", "None", "False"], "options": [["True", "False"], "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "       Whether the technique will perform dynamic selection,\n       dynamic weighting or an hybrid approach for classification.\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "mcb": {"docs_name": "deslib.dcs.mcb.MCB", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "diff_thresh", "k", "knn_classifier", "pool_classifiers", "random_state", "safe_k", "selection_method", "similarity_threshold", "with_IH"], "default_param_types": ["bool", "float", "float", "float", "int", "str", "NoneType", "NoneType", "NoneType", "str", "float", "bool"], "default_params": ["False", "0.5", "0.3", "0.1", "7", "'knn'", "None", "None", "None", "'diff'", "0.7", "False"], "options": [["True", "False"], "", "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Threshold to measure the difference between the competence level of the\n    base classifiers for the random and diff selection schemes. If the\n    difference is lower than the threshold, their performance are\n    considered equivalent.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "    The generated_pool of classifiers trained for the corresponding\n    classification problem. Each base classifiers should support the method\n    \"predict\". If None, then the pool of classifiers is a bagging\n    classifier.\n", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Determines which method is used to select the base classifier after\n    the competences are estimated.\n", "", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "metades": {"docs_name": "deslib.des.meta_des.METADES", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "Hc", "IH_rate", "Kp", "k", "knn_classifier", "meta_classifier", "mode", "pool_classifiers", "random_state", "safe_k", "selection_threshold", "with_IH"], "default_param_types": ["bool", "float", "float", "float", "int", "int", "str", "NoneType", "str", "NoneType", "NoneType", "NoneType", "float", "bool"], "default_params": ["False", "0.5", "1.0", "0.3", "5", "7", "'knn'", "None", "'selection'", "None", "None", "None", "0.5", "False"], "options": [["True", "False"], "", "", "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "     Sample selection threshold.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "     Number of output profiles used to estimate the competence of the\n     base classifiers.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "                    Classifier model used for the meta-classifier. If None,\n                    a Multinomial naive Bayes classifier is used.\n", "    Determines the mode of META-des that is used\n    (selection, weighting or hybrid).\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Threshold used to select the base classifier. Only the base classifiers\n    with competence level higher than the selection_threshold are selected\n    to compose the ensemble.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "min dif": {"docs_name": "deslib.des.probabilistic.MinimumDifference", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "k", "knn_classifier", "mode", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["bool", "float", "float", "NoneType", "str", "str", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["False", "0.5", "0.3", "None", "'knn'", "'selection'", "None", "None", "None", "False"], "options": [["True", "False"], "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "       Whether the technique will perform dynamic selection,\n       dynamic weighting or an hybrid approach for classification.\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "mla": {"docs_name": "deslib.dcs.mla.MLA", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "diff_thresh", "k", "knn_classifier", "pool_classifiers", "random_state", "safe_k", "selection_method", "with_IH"], "default_param_types": ["bool", "float", "float", "float", "int", "str", "NoneType", "NoneType", "NoneType", "str", "bool"], "default_params": ["False", "0.5", "0.3", "0.1", "7", "'knn'", "None", "None", "None", "'best'", "False"], "options": [["True", "False"], "", "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Threshold to measure the difference between the competence level of the\n    base classifiers for the random and diff selection schemes. If the\n    difference is lower than the threshold, their performance are\n    considered equivalent.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "    The generated_pool of classifiers trained for the corresponding\n    classification problem. Each base classifiers should support the method\n    \"predict\". If None, then the pool of classifiers is a bagging\n    classifier.\n", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Determines which method is used to select the base classifier after\n    the competences are estimated.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "ola": {"docs_name": "deslib.dcs.ola.OLA", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "diff_thresh", "k", "knn_classifier", "pool_classifiers", "random_state", "safe_k", "selection_method", "with_IH"], "default_param_types": ["bool", "float", "float", "float", "int", "str", "NoneType", "NoneType", "NoneType", "str", "bool"], "default_params": ["False", "0.5", "0.3", "0.1", "7", "'knn'", "None", "None", "None", "'best'", "False"], "options": [["True", "False"], "", "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Threshold to measure the difference between the competence level of the\n    base classifiers for the random and diff selection schemes. If the\n    difference is lower than the threshold, their performance are\n    considered equivalent.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "    The generated_pool of classifiers trained for the corresponding\n    classification problem. Each base classifiers should support the method\n    \"predict\". If None, then the pool of classifiers is a bagging\n    classifier.\n", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Determines which method is used to select the base classifier after\n    the competences are estimated.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "rank": {"docs_name": "deslib.dcs.rank.Rank", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "diff_thresh", "k", "knn_classifier", "pool_classifiers", "random_state", "safe_k", "selection_method", "with_IH"], "default_param_types": ["bool", "float", "float", "float", "int", "str", "NoneType", "NoneType", "NoneType", "str", "bool"], "default_params": ["False", "0.5", "0.3", "0.1", "7", "'knn'", "None", "None", "None", "'best'", "False"], "options": [["True", "False"], "", "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Threshold to measure the difference between the competence level of the\n    base classifiers for the random and diff selection schemes. If the\n    difference is lower than the threshold, their performance are\n    considered equivalent.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "    The generated_pool of classifiers trained for the corresponding\n    classification problem. Each base classifiers should support the method\n    \"predict\". If None, then the pool of classifiers is a bagging\n    classifier.\n", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Determines which method is used to select the base classifier after\n    the competences are estimated.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "rrc": {"docs_name": "deslib.des.probabilistic.RRC", "preset_params": ["default"], "param_names": ["DFP", "DSEL_perc", "IH_rate", "k", "knn_classifier", "mode", "pool_classifiers", "random_state", "safe_k", "with_IH"], "default_param_types": ["bool", "float", "float", "NoneType", "str", "str", "NoneType", "NoneType", "NoneType", "bool"], "default_params": ["False", "0.5", "0.3", "None", "'knn'", "'selection'", "None", "None", "None", "False"], "options": [["True", "False"], "", "", "", ["'knn'", "'faiss'", "None"], "", "", "", "", ["True", "False"]], "descrs": ["    Determines if the dynamic frienemy pruning is applied.\n", "    Percentage of the input data used to fit DSEL.\n    Note: This parameter is only used if the pool of classifier is None or\n    unfitted.\n", "    Hardness threshold. If the hardness level of the competence region is\n    lower than the IH_rate the KNN classifier is used. Otherwise, the DS\n    algorithm is used for classification.\n", "    Number of neighbors used to estimate the competence of the base\n    classifiers.\n", "     The algorithm used to estimate the region of competence:\n     - 'knn' will use :class:`KNeighborsClassifier` from sklearn\n     - 'faiss' will use Facebook's Faiss similarity search through the\n       class :class:`FaissKNNClassifier`\n     - None, will use sklearn :class:`KNeighborsClassifier`.\n", "       Whether the technique will perform dynamic selection,\n       dynamic weighting or an hybrid approach for classification.\n", "", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n", "    The size of the indecision region.\n", "    Whether the hardness level of the region of competence is used to\n    decide between using the DS algorithm or the KNN for classification of\n    a given query sample.\n"]}, "single best": {"docs_name": "deslib.static.single_best.SingleBest", "preset_params": ["default"], "param_names": ["pool_classifiers", "random_state"], "default_param_types": ["NoneType", "NoneType"], "default_params": ["None", "None"], "options": ["", ""], "descrs": ["    The generated_pool of classifiers trained for the corresponding\n    classification problem. Each base classifiers should support the method\n    \"predict\". If None, then the pool of classifiers is a bagging\n    classifier.\n", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n"]}, "stacked": {"docs_name": "deslib.static.stacked.StackedClassifier", "preset_params": ["default"], "param_names": ["meta_classifier", "pool_classifiers", "random_state"], "default_param_types": ["NoneType", "NoneType", "NoneType"], "default_params": ["None", "None", "None"], "options": ["", "", ""], "descrs": ["    Classifier model used to aggregate the output of the base classifiers.\n    If None, a :class:`LogisticRegression` classifier is used.\n", "    The generated_pool of classifiers trained for the corresponding\n    classification problem. Each base classifiers should support the method\n    \"predict\". If None, then the pool of classifiers is a bagging\n    classifier.\n", "    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`.\n"]}, "stacking": {"docs_name": "sklearn.ensemble.StackingClassifier", "preset_params": ["default"], "param_names": ["cv", "estimators", "final_estimator", "n_jobs", "passthrough", "stack_method", "verbose"], "default_param_types": ["NoneType", "str", "NoneType", "NoneType", "bool", "str", "int"], "default_params": ["None", "\"<class 'inspect._empty'>\"", "None", "None", "False", "'auto'", "0"], "options": ["", "", "", "", ["True", "False"], ["'auto'", "'predict_proba'", "'decision_function'", "'predict'"], ""], "descrs": ["    Determines the cross-validation splitting strategy used in\n    `cross_val_predict` to train `final_estimator`. Possible inputs for\n    cv are:\n    * None, to use the default 5-fold cross validation,\n    * integer, to specify the number of folds in a (Stratified) KFold,\n    * An object to be used as a cross-validation generator,\n    * An iterable yielding train, test splits.\n    For integer/None inputs, if the estimator is a classifier and y is\n    either binary or multiclass, `StratifiedKFold` is used. In all other\n    cases, `KFold` is used.\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n    .. note::\n       A larger number of split will provide no benefits if the number\n       of training samples is large enough. Indeed, the training time\n       will increase. ``cv`` is not used for model evaluation but for\n       prediction.\n", "    Base estimators which will be stacked together. Each element of the\n    list is defined as a tuple of string (i.e. name) and an estimator\n    instance. An estimator can be set to 'drop' using `set_params`.\n", "    A classifier which will be used to combine the base estimators.\n    The default classifier is a `LogisticRegression`.\n", "    The number of jobs to run in parallel all `estimators` `fit`.\n    `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n    using all processors. See Glossary for more details.\n", "    When False, only the predictions of estimators will be used as\n    training data for `final_estimator`. When True, the\n    `final_estimator` is trained on the predictions as well as the\n    original training data.\n", "    Methods called for each base estimator. It can be:\n    * if 'auto', it will try to invoke, for each estimator,\n      `'predict_proba'`, `'decision_function'` or `'predict'` in that\n      order.\n    * otherwise, one of `'predict_proba'`, `'decision_function'` or\n      `'predict'`. If the method is not implemented by the estimator, it\n      will raise an error.\n", "    Verbosity level.\n"]}, "voting": {"docs_name": "sklearn.ensemble.VotingClassifier", "preset_params": ["default"], "param_names": ["estimators", "flatten_transform", "n_jobs", "verbose", "voting", "weights"], "default_param_types": ["str", "bool", "NoneType", "bool", "str", "NoneType"], "default_params": ["\"<class 'inspect._empty'>\"", "True", "None", "False", "'hard'", "None"], "options": ["", ["True", "False"], "", ["True", "False"], ["'hard'", "'soft'"], ""], "descrs": ["    Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones\n    of those original estimators that will be stored in the class attribute\n    ``self.estimators_``. An estimator can be set to ``'drop'``\n    using ``set_params``.\n    .. versionchanged:: 0.21\n        ``'drop'`` is accepted.\n    .. deprecated:: 0.22\n       Using ``None`` to drop an estimator is deprecated in 0.22 and\n       support will be dropped in 0.24. Use the string ``'drop'`` instead.\n", "    Affects shape of transform output only when voting='soft'\n    If voting='soft' and flatten_transform=True, transform method returns\n    matrix with shape (n_samples, n_classifiers * n_classes). If\n    flatten_transform=False, it returns\n    (n_classifiers, n_samples, n_classes).\n", "    The number of jobs to run in parallel for ``fit``.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n    .. versionadded:: 0.18\n", "    If True, the time elapsed while fitting will be printed as it\n    is completed.\n", "    If 'hard', uses predicted class labels for majority rule voting.\n    Else if 'soft', predicts the class label based on the argmax of\n    the sums of the predicted probabilities, which is recommended for\n    an ensemble of well-calibrated classifiers.\n", "    Sequence of weights (`float` or `int`) to weight the occurrences of\n    predicted class labels (`hard` voting) or class probabilities\n    before averaging (`soft` voting). Uses uniform weights if `None`.\n"]}}}, "parameter_search": [null, "ASCMA2PDEthird", "ASCMADEQRthird", "ASCMADEthird", "AlmostRotationInvariantDE", "BO", "CM", "CMA", "CMandAS", "CMandAS2", "CMandAS3", "CauchyLHSSearch", "CauchyOnePlusOne", "CauchyScrHammersleySearch", "Cobyla", "DE", "DiagonalCMA", "DiscreteOnePlusOne", "DoubleFastGADiscreteOnePlusOne", "EDA", "ES", "FCMA", "HaltonSearch", "HaltonSearchPlusMiddlePoint", "HammersleySearch", "HammersleySearchPlusMiddlePoint", "LHSSearch", "LargeHaltonSearch", "LhsDE", "MEDA", "MPCEDA", "MetaRecentering", "MixES", "MultiCMA", "MultiScaleCMA", "MutDE", "NGO", "NaiveIsoEMNA", "NaiveTBPSA", "NelderMead", "NoisyBandit", "NoisyDE", "NoisyDiscreteOnePlusOne", "NoisyOnePlusOne", "ORandomSearch", "OScrHammersleySearch", "OnePlusOne", "OptimisticDiscreteOnePlusOne", "OptimisticNoisyOnePlusOne", "PBIL", "PCEDA", "PSO", "ParaPortfolio", "Portfolio", "Powell", "QORandomSearch", "QOScrHammersleySearch", "QrDE", "RCobyla", "RPowell", "RSQP", "RandomSearch", "RandomSearchPlusMiddlePoint", "RealSpacePSO", "RecES", "RecMixES", "RecMutDE", "RecombiningPortfolioOptimisticNoisyDiscreteOnePlusOne", "RotationInvariantDE", "SPSA", "SQP", "SQPCMA", "ScrHaltonSearch", "ScrHaltonSearchPlusMiddlePoint", "ScrHammersleySearch", "ScrHammersleySearchPlusMiddlePoint", "Shiva", "SplitOptimizer", "TBPSA", "TripleCMA", "TwoPointsDE", "cGA", "chainCMAPowell"], "metrics": {"binary": {"accuracy": {"docs_name": "sklearn.metrics.accuracy_score"}, "roc_auc": {"docs_name": "sklearn.metrics.roc_auc_score"}, "roc_auc_ovr": {"docs_name": "sklearn.metrics.roc_auc_score"}, "roc_auc_ovo": {"docs_name": "sklearn.metrics.roc_auc_score"}, "roc_auc_ovr_weighted": {"docs_name": "sklearn.metrics.roc_auc_score"}, "roc_auc_ovo_weighted": {"docs_name": "sklearn.metrics.roc_auc_score"}, "balanced_accuracy": {"docs_name": "sklearn.metrics.balanced_accuracy_score"}, "average_precision": {"docs_name": "sklearn.metrics.average_precision_score"}, "neg_log_loss": {"docs_name": "sklearn.metrics.log_loss"}, "neg_brier_score": {"docs_name": "sklearn.metrics.brier_score_loss"}, "precision": {"docs_name": "sklearn.metrics.precision_score"}, "precision_macro": {"docs_name": "sklearn.metrics.precision_score"}, "precision_micro": {"docs_name": "sklearn.metrics.precision_score"}, "precision_samples": {"docs_name": "sklearn.metrics.precision_score"}, "precision_weighted": {"docs_name": "sklearn.metrics.precision_score"}, "recall": {"docs_name": "sklearn.metrics.recall_score"}, "recall_macro": {"docs_name": "sklearn.metrics.recall_score"}, "recall_micro": {"docs_name": "sklearn.metrics.recall_score"}, "recall_samples": {"docs_name": "sklearn.metrics.recall_score"}, "recall_weighted": {"docs_name": "sklearn.metrics.recall_score"}, "f1": {"docs_name": "sklearn.metrics.f1_score"}, "f1_macro": {"docs_name": "sklearn.metrics.f1_score"}, "f1_micro": {"docs_name": "sklearn.metrics.f1_score"}, "f1_samples": {"docs_name": "sklearn.metrics.f1_score"}, "f1_weighted": {"docs_name": "sklearn.metrics.f1_score"}, "jaccard": {"docs_name": "sklearn.metrics.jaccard_score"}, "jaccard_macro": {"docs_name": "sklearn.metrics.jaccard_score"}, "jaccard_micro": {"docs_name": "sklearn.metrics.jaccard_score"}, "jaccard_samples": {"docs_name": "sklearn.metrics.jaccard_score"}, "jaccard_weighted": {"docs_name": "sklearn.metrics.jaccard_score"}, "neg_hamming": {"docs_name": "sklearn.metrics.hamming_loss"}, "matthews": {"docs_name": "sklearn.metrics.matthews_corrcoef"}}, "regression": {"explained_variance": {"docs_name": "sklearn.metrics.explained_variance_score"}, "r2": {"docs_name": "sklearn.metrics.r2_score"}, "max_error": {"docs_name": "sklearn.metrics.max_error"}, "neg_median_absolute_error": {"docs_name": "sklearn.metrics.median_absolute_error"}, "neg_mean_absolute_error": {"docs_name": "sklearn.metrics.mean_absolute_error"}, "neg_mean_squared_error": {"docs_name": "sklearn.metrics.mean_squared_error"}, "neg_mean_squared_log_error": {"docs_name": "sklearn.metrics.mean_squared_log_error"}, "neg_root_mean_squared_error": {"docs_name": "sklearn.metrics.mean_squared_error"}, "neg_mean_poisson_deviance": {"docs_name": "sklearn.metrics.mean_poisson_deviance"}, "neg_mean_gamma_deviance": {"docs_name": "sklearn.metrics.mean_gamma_deviance"}}, "categorical": {"accuracy": {"docs_name": "sklearn.metrics.accuracy_score"}, "roc_auc": {"docs_name": "sklearn.metrics.roc_auc_score"}, "roc_auc_ovr": {"docs_name": "sklearn.metrics.roc_auc_score"}, "roc_auc_ovo": {"docs_name": "sklearn.metrics.roc_auc_score"}, "roc_auc_ovr_weighted": {"docs_name": "sklearn.metrics.roc_auc_score"}, "roc_auc_ovo_weighted": {"docs_name": "sklearn.metrics.roc_auc_score"}, "balanced_accuracy": {"docs_name": "sklearn.metrics.balanced_accuracy_score"}, "average_precision": {"docs_name": "sklearn.metrics.average_precision_score"}, "neg_log_loss": {"docs_name": "sklearn.metrics.log_loss"}, "neg_brier_score": {"docs_name": "sklearn.metrics.brier_score_loss"}, "precision": {"docs_name": "sklearn.metrics.precision_score"}, "precision_macro": {"docs_name": "sklearn.metrics.precision_score"}, "precision_micro": {"docs_name": "sklearn.metrics.precision_score"}, "precision_samples": {"docs_name": "sklearn.metrics.precision_score"}, "precision_weighted": {"docs_name": "sklearn.metrics.precision_score"}, "recall": {"docs_name": "sklearn.metrics.recall_score"}, "recall_macro": {"docs_name": "sklearn.metrics.recall_score"}, "recall_micro": {"docs_name": "sklearn.metrics.recall_score"}, "recall_samples": {"docs_name": "sklearn.metrics.recall_score"}, "recall_weighted": {"docs_name": "sklearn.metrics.recall_score"}, "f1": {"docs_name": "sklearn.metrics.f1_score"}, "f1_macro": {"docs_name": "sklearn.metrics.f1_score"}, "f1_micro": {"docs_name": "sklearn.metrics.f1_score"}, "f1_samples": {"docs_name": "sklearn.metrics.f1_score"}, "f1_weighted": {"docs_name": "sklearn.metrics.f1_score"}, "jaccard": {"docs_name": "sklearn.metrics.jaccard_score"}, "jaccard_macro": {"docs_name": "sklearn.metrics.jaccard_score"}, "jaccard_micro": {"docs_name": "sklearn.metrics.jaccard_score"}, "jaccard_samples": {"docs_name": "sklearn.metrics.jaccard_score"}, "jaccard_weighted": {"docs_name": "sklearn.metrics.jaccard_score"}, "neg_hamming": {"docs_name": "sklearn.metrics.hamming_loss"}, "matthews": {"docs_name": "sklearn.metrics.matthews_corrcoef"}}}}